0	"Traditional <e1>information retrieval techniques</e1> use a histogram of <e2>keywords</e2> as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance"
USAGE(e2,e1)

1	"Traditional information retrieval techniques use a histogram of keywords as the document representation but <e1>oral communication</e1> may offer additional <e2>indices</e2> such as the time and place of the rejoinder and the attendance"
USAGE(e1,e2)

2	"Several extensions of this basic idea are being discussed and/or evaluated: Similar to activities one can define subsets of larger database and detect those automatically which is shown on a large <e1>database</e1> of <e2>TV shows</e2> "
PART_WHOLE(e2,e1)

3	" To support engaging human users in robust, mixed-initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems , the DARPA Communicator program [1] is funding the development of a <e1>distributed message-passing infrastructure</e1> for <e2>dialogue systems</e2> which all Communicator participants are using"
MODEL-FEATURE(e1,e2)

4	"The <e1>CCLINC Korean-to-English translation system</e1> consists of two <e2>core modules</e2> , language understanding and generation modules mediated by a language neutral meaning representation called a semantic frame "
PART_WHOLE(e2,e1)

5	"The key features of the system include: (i) Robust efficient <e1>parsing</e1> of <e2>Korean</e2> (a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments )"
USAGE(e1,e2)

6	"The key features of the system include: (i) Robust efficient parsing of Korean (a <e1>verb final language</e1> with <e2>overt case markers</e2> , relatively free word order , and frequent omissions of arguments )"
MODEL-FEATURE(e2,e1)

7	"(ii) High quality <e1>translation</e1> via <e2>word sense disambiguation</e2> and accurate word order generation of the target language "
USAGE(e2,e1)

8	" The purpose of this research is to test the efficacy of applying <e1>automated evaluation techniques</e1> , originally devised for the evaluation of human language learners , to the <e2>output</e2> of machine translation &amp;&amp;lpar&amp;&amp;MT&amp;&amp;rpar&amp;&amp; systems "
USAGE(e1,e2)

9	"This, the first experiment in a series of experiments, looks at the <e1>intelligibility</e1> of <e2>MT output</e2> "
MODEL-FEATURE(e1,e2)

10	"We integrate a <e1>spoken language understanding system</e1> with <e2>intelligent mobile agents</e2> that mediate between users and information sources "
PART_WHOLE(e2,e1)

11	"We find that simple <e1>interpolation methods</e1> , like log-linear and linear interpolation , improve the <e2>performance</e2> but fall short of the performance of an oracle "
RESULT(e1,e2)

12	"The oracle knows the reference word string and selects the <e1>word string</e1> with the best <e2>performance</e2> (typically, word or semantic error rate ) from a list of word strings , where each word string has been obtained by using a different LM "
RESULT(e1,e2)

13	"The oracle knows the reference word string and selects the word string with the best performance (typically, word or semantic error rate ) from a list of word strings , where each <e1>word string</e1> has been obtained by using a different <e2>LM</e2> "
RESULT(e2,e1)

14	"Actually, the oracle acts like a <e1>dynamic combiner</e1> with hard decisions using the <e2>reference</e2> "
USAGE(e2,e1)

15	"We provide experimental results that clearly show the need for a <e1>dynamic language model combination</e1> to improve the <e2>performance</e2> further "
RESULT(e1,e2)

16	"The method amounts to tagging <e1>LMs</e1> with <e2>confidence measures</e2> and picking the best hypothesis corresponding to the LM with the best confidence "
MODEL-FEATURE(e2,e1)

17	"The method amounts to tagging LMs with confidence measures and picking the best hypothesis corresponding to the <e1>LM</e1> with the best <e2>confidence</e2> "
MODEL-FEATURE(e2,e1)

18	" This paper proposes a practical approach employing n-gram models and <e1>error-correction rules</e1> for <e2>Thai key prediction</e2> and Thai-English language identification "
USAGE(e1,e2)

19	"The paper also proposes rule-reduction algorithm applying <e1>mutual information</e1> to reduce the <e2>error-correction rules</e2> "
USAGE(e1,e2)

20	"Our algorithm reported more than 99% <e1>accuracy</e1> in both <e2>language identification</e2> and key prediction "
RESULT(e2,e1)

21	"First, a very simple, randomized sentence-plan-generator &amp;&amp;lpar&amp;&amp;SPG&amp;&amp;rpar&amp;&amp; generates a potentially large list of possible <e1>sentence plans</e1> for a given <e2>text-plan input</e2> "
MODEL-FEATURE(e1,e2)

22	"The SPR uses <e1>ranking rules</e1> automatically learned from <e2>training data</e2> "
USAGE(e2,e1)

23	"We show that the trained SPR learns to select a <e1>sentence plan</e1> whose rating on average is only 5% worse than the <e2>top human-ranked sentence plan</e2> "
COMPARE(e1,e2)

24	" In this paper, we compare the relative effects of segment order , segmentation and segment contiguity on the <e1>retrieval performance</e1> of a <e2>translation memory system</e2> "
RESULT(e2,e1)

25	"We take a selection of both <e1>bag-of-words and segment order-sensitive string comparison methods</e1> , and run each over both <e2>character- and word-segmented data</e2> , in combination with a range of local segment contiguity models (in the form of N-grams )"
USAGE(e1,e2)

26	"Over two distinct datasets , we find that <e1>indexing</e1> according to simple <e2>character bigrams</e2> produces a retrieval accuracy superior to any of the tested word N-gram models "
USAGE(e2,e1)

27	"Further,in their optimum configuration , <e1>bag-of-words methods</e1> are shown to be equivalent to <e2>segment order-sensitive methods</e2> in terms of retrieval accuracy , but much faster"
COMPARE(e1,e2)

28	" The theoretical study of the <e1>range concatenation grammar [RCG] formalism</e1> has revealed many attractive properties which may be used in <e2>NLP</e2> "
USAGE(e1,e2)

29	"In particular, <e1>range concatenation languages [RCL]</e1> can be parsed in <e2>polynomial time</e2> and many classical grammatical formalisms can be translated into equivalent RCGs without increasing their worst-case parsing time complexity "
MODEL-FEATURE(e2,e1)

30	"For example, after translation into an equivalent RCG , any <e1>tree adjoining grammar</e1> can be parsed in <e2>O&amp;&amp;lpar&amp;&amp;n6&amp;&amp;rpar&amp;&amp; time</e2> "
MODEL-FEATURE(e2,e1)

31	"In this paper, we study a <e1>parsing technique</e1> whose purpose is to improve the practical efficiency of <e2>RCL parsers</e2> "
USAGE(e1,e2)

32	"The non-deterministic parsing choices of the main parser for a language L are directed by a <e1>guide</e1> which uses the <e2>shared derivation forest</e2> output by a prior RCL parser for a suitable superset of L "
USAGE(e2,e1)

33	"The non-deterministic parsing choices of the main parser for a language L are directed by a guide which uses the shared derivation forest output by a prior <e1>RCL parser</e1> for a suitable <e2>superset of L</e2> "
USAGE(e1,e2)

34	" While <e1>paraphrasing</e1> is critical both for <e2>interpretation and generation of natural language</e2> , current systems use manual or semi-automatic methods to collect paraphrases "
USAGE(e1,e2)

35	"We present an <e1>unsupervised learning algorithm</e1> for <e2>identification of paraphrases</e2> from a corpus of multiple English translations of the same source text "
USAGE(e1,e2)

36	" This paper presents a <e1>formal analysis</e1> for a large class of words called <e2>alternative markers</e2> , which includes other &amp;&amp;lpar&amp;&amp;than&amp;&amp;rpar&amp;&amp; , such &amp;&amp;lpar&amp;&amp;as&amp;&amp;rpar&amp;&amp; , and besides "
TOPIC(e1,e2)

37	"I show that the <e1>performance</e1> of a search engine can be improved dramatically by incorporating an approximation of the <e2>formal analysis</e2> that is compatible with the search engine 's operational semantics "
RESULT(e2,e1)

38	"The value of this approach is that as the <e1>operational semantics</e1> of <e2>natural language applications</e2> improve, even larger improvements are possible"
PART_WHOLE(e1,e2)

39	"Our logical definition leads to a neat relation to categorial grammar , (yielding a treatment of Montague semantics ), a parsing-as-deduction in a resource sensitive logic , and a <e1>learning algorithm</e1> from <e2>structured data</e2> (based on a typing-algorithm and type-unification )"
USAGE(e2,e1)

40	" Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the <e1>quality</e1> of <e2>utterances</e2> produced with trainable components can compete with hand-crafted template-based or rule-based approaches "
MODEL-FEATURE(e1,e2)

41	" Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with <e1>trainable components</e1> can compete with <e2>hand-crafted template-based or rule-based approaches</e2> "
COMPARE(e1,e2)

42	"In this paper We experimentally evaluate a <e1>trainable sentence planner</e1> for a <e2>spoken dialogue system</e2> by eliciting subjective human judgments "
USAGE(e1,e2)

43	"We show that the <e1>trainable sentence planner</e1> performs better than the <e2>rule-based systems</e2> and the baselines , and as well as the hand-crafted system "
COMPARE(e1,e2)

44	" We describe a set of supervised machine learning experiments centering on the construction of <e1>statistical models</e1> of <e2>WH-questions</e2> "
MODEL-FEATURE(e1,e2)

45	"These models , which are built from <e1>shallow linguistic features</e1> of <e2>questions</e2> , are employed to predict target variables which represent a user's informational goals "
MODEL-FEATURE(e1,e2)

46	"We report on different aspects of the <e1>predictive performance</e1> of our <e2>models</e2> , including the influence of various training and testing factors on predictive performance , and examine the relationships among the target variables"
RESULT(e2,e1)

47	"We report on different aspects of the predictive performance of our models , including the influence of various <e1>training and testing factors</e1> on <e2>predictive performance</e2> , and examine the relationships among the target variables"
RESULT(e1,e2)

48	" This paper describes a method for <e1>utterance classification</e1> that does not require <e2>manual transcription</e2> of training data "
USAGE(e2,e1)

49	"The method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional <e1>word-trigram recognition</e1> requiring <e2>manual transcription</e2> "
USAGE(e2,e1)

50	"In our method, <e1>unsupervised training</e1> is first used to train a <e2>phone n-gram model</e2> for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier "
USAGE(e1,e2)

51	"In our method, unsupervised training is first used to train a phone n-gram model for a particular domain ; the <e1>output</e1> of recognition with this model is then passed to a <e2>phone-string classifier</e2> "
USAGE(e2,e1)

52	" Motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a <e1>multi-strategy and multi-source approach to question answering</e1> which is based on combining the results from different <e2>answering agents</e2> searching for answers in multiple corpora "
USAGE(e2,e1)

53	" Motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for <e1>answers</e1> in multiple <e2>corpora</e2> "
PART_WHOLE(e1,e2)

54	"The <e1>answering agents</e1> adopt fundamentally different strategies, one utilizing primarily <e2>knowledge-based mechanisms</e2> and the other adopting statistical techniques "
USAGE(e2,e1)

55	"Experiments evaluating the effectiveness of our <e1>answer resolution algorithm</e1> show a 35.0% relative improvement over our <e2>baseline system</e2> in the number of questions correctly answered , and a 32.8% improvement according to the average precision metric "
COMPARE(e1,e2)

56	"We apply our system to the task of <e1>scoring</e1> alternative <e2>speech recognition hypotheses &amp;&amp;lpar&amp;&amp;SRH&amp;&amp;rpar&amp;&amp;</e2> in terms of their semantic coherence "
USAGE(e1,e2)

57	"An evaluation of our system against the annotated data shows that, it successfully classifies 73.2% in a <e1>German corpus</e1> of 2.284 <e2>SRHs</e2> as either coherent or incoherent (given a baseline of 54.55%)"
PART_WHOLE(e2,e1)

58	"Within our framework, we carry out a large number of experiments to understand better and explain why <e1>phrase-based models</e1> outperform <e2>word-based models</e2> "
COMPARE(e1,e2)

59	"Our empirical results, which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means: <e1>heuristic learning</e1> of phrase translations from <e2>word-based alignments</e2> and lexical weighting of phrase translations "
USAGE(e2,e1)

60	"Our empirical results, which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and <e1>lexical weighting</e1> of <e2>phrase translations</e2> "
MODEL-FEATURE(e1,e2)

61	"Surprisingly, learning phrases longer than three words and learning <e1>phrases</e1> from <e2>high-accuracy word-level alignment models</e2> does not have a strong impact on performance"
PART_WHOLE(e1,e2)

62	"The <e1>model</e1> is designed for use in <e2>error correction</e2> , with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks "
USAGE(e1,e2)

63	"The model is designed for use in error correction , with a focus on <e1>post-processing</e1> the <e2>output</e2> of black-box OCR systems in order to make it more useful for NLP tasks "
USAGE(e1,e2)

64	"We present an implementation of the <e1>model</e1> based on <e2>finite-state models</e2> , demonstrate the model 's ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text "
USAGE(e2,e1)

65	"We present an implementation of the model based on finite-state models , demonstrate the <e1>model</e1> 's ability to significantly reduce <e2>character and word error rate</e2> , and provide evaluation results involving automatic extraction of translation lexicons from printed text "
RESULT(e1,e2)

66	"We present an implementation of the model based on finite-state models , demonstrate the model 's ability to significantly reduce character and word error rate , and provide evaluation results involving <e1>automatic extraction</e1> of translation lexicons from <e2>printed text</e2> "
USAGE(e1,e2)

67	" We present an application of <e1>ambiguity packing and stochastic disambiguation techniques</e1> for <e2>Lexical-Functional Grammars &amp;&amp;lpar&amp;&amp;LFG&amp;&amp;rpar&amp;&amp;</e2> to the domain of sentence condensation "
USAGE(e1,e2)

68	"Our system incorporates a <e1>linguistic parser/generator</e1> for <e2>LFG</e2> , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection "
USAGE(e1,e2)

69	"Our system incorporates a linguistic parser/generator for LFG , a <e1>transfer component</e1> for <e2>parse reduction</e2> operating on packed parse forests , and a maximum-entropy model for stochastic output selection "
USAGE(e1,e2)

70	"Our system incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a <e1>maximum-entropy model</e1> for <e2>stochastic output selection</e2> "
USAGE(e2,e1)

71	"An <e1>experimental evaluation</e1> of <e2>summarization</e2> quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings "
TOPIC(e1,e2)

72	"Overall summarization quality of the proposed system is state-of-the-art, with guaranteed <e1>grammaticality</e1> of the <e2>system output</e2> due to the use of a constraint-based parser/generator "
MODEL-FEATURE(e1,e2)

73	" We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation , (ii) broad use of lexical features , including jointly conditioning on multiple consecutive words , (iii) effective use of <e1>priors</e1> in <e2>conditional loglinear models</e2> , and (iv) fine-grained modeling of unknown word features "
USAGE(e1,e2)

74	"Using these ideas together, the resulting <e1>tagger</e1> gives a 97.24% <e2>accuracy</e2> on the Penn Treebank WSJ , an error reduction of 4.4% on the best previous single automatically learned tagging result"
RESULT(e1,e2)

75	" Sources of <e1>training data</e1> suitable for <e2>language modeling</e2> of conversational speech are limited"
USAGE(e1,e2)

76	"In this paper, we show how training data can be supplemented with <e1>text</e1> from the <e2>web</e2> filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger performance gains from the data by using class-dependent interpolation of N-grams "
PART_WHOLE(e1,e2)

77	" In order to boost the <e1>translation quality</e1> of <e2>EBMT</e2> based on a small-sized bilingual corpus , we use an out-of-domain bilingual corpus and, in addition, the language model of an in-domain monolingual corpus "
RESULT(e2,e1)

78	" We describe a simple unsupervised technique for learning morphology by identifying <e1>hubs</e1> in an <e2>automaton</e2> "
PART_WHOLE(e1,e2)

79	" We present a <e1>syntax-based constraint</e1> for <e2>word alignment</e2> , known as the cohesion constraint "
USAGE(e1,e2)

80	" A novel <e1>bootstrapping approach</e1> to <e2>Named Entity &amp;&amp;lpar&amp;&amp;NE&amp;&amp;rpar&amp;&amp; tagging</e2> using concept-based seeds and successive learners is presented"
USAGE(e1,e2)

81	"This approach only requires a few common noun or pronoun <e1>seeds</e1> that correspond to the <e2>concept</e2> for the targeted NE , e.g. he/she/man/woman for PERSON NE "
MODEL-FEATURE(e1,e2)

82	"First, <e1>decision list</e1> is used to learn the <e2>parsing-based NE rules</e2> "
USAGE(e2,e1)

83	"Then, a <e1>Hidden Markov Model</e1> is trained on a <e2>corpus</e2> automatically tagged by the first learner "
USAGE(e1,e2)

84	" In this paper, we describe a <e1>phrase-based unigram model</e1> for <e2>statistical machine translation</e2> that uses a much simpler set of model parameters than similar phrase-based models "
USAGE(e1,e2)

85	"During training , the <e1>blocks</e1> are learned from <e2>source interval projections</e2> using an underlying word alignment "
PART_WHOLE(e1,e2)

86	"We show experimental results on <e1>block selection criteria</e1> based on <e2>unigram</e2> counts and phrase length"
USAGE(e2,e1)

87	" In this paper, we propose a novel <e1>Cooperative Model</e1> for <e2>natural language understanding</e2> in a dialogue system "
USAGE(e1,e2)

88	" The <e1>JAVELIN system</e1> integrates a flexible, <e2>planning-based architecture</e2> with a variety of language processing modules to provide an open-domain question answering capability on free text "
PART_WHOLE(e2,e1)

89	"The demonstration will focus on how <e1>JAVELIN</e1> processes <e2>questions</e2> and retrieves the most likely answer candidates from the given text corpus "
USAGE(e1,e2)

90	"The demonstration will focus on how JAVELIN processes questions and retrieves the most likely <e1>answer candidates</e1> from the given <e2>text corpus</e2> "
PART_WHOLE(e1,e2)

91	"The operation of the system will be explained in depth through browsing the <e1>repository</e1> of <e2>data objects</e2> created by the system during each question answering session "
PART_WHOLE(e2,e1)

92	" In this paper we present a novel, customizable : <e1>IE paradigm</e1> that takes advantage of <e2>predicate-argument structures</e2> "
USAGE(e2,e1)

93	" This paper proposes the <e1>Hierarchical Directed Acyclic Graph &amp;&amp;lpar&amp;&amp;HDAG&amp;&amp;rpar&amp;&amp; Kernel</e1> for <e2>structured natural language data</e2> "
USAGE(e1,e2)

94	"The HDAG Kernel directly accepts several levels of both chunks and their relations , and then efficiently computes the weighed sum of the number of common <e1>attribute sequences</e1> of the <e2>HDAGs</e2> "
MODEL-FEATURE(e1,e2)

95	"The results of the experiments demonstrate that the <e1>HDAG Kernel</e1> is superior to other <e2>kernel functions</e2> and baseline methods "
COMPARE(e1,e2)

96	" Previous research has demonstrated the utility of clustering in inducing <e1>semantic verb classes</e1> from undisambiguated <e2>corpus data</e2> "
MODEL-FEATURE(e1,e2)

97	"We describe a new approach which involves clustering <e1>subcategorization frame &amp;&amp;lpar&amp;&amp;SCF&amp;&amp;rpar&amp;&amp;</e1> distributions using the <e2>Information Bottleneck</e2> and nearest neighbour methods"
USAGE(e2,e1)

98	"A novel evaluation scheme is proposed which accounts for the effect of <e1>polysemy</e1> on the <e2>clusters</e2> , offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data "
MODEL-FEATURE(e1,e2)

99	" We apply a <e1>decision tree based approach</e1> to <e2>pronoun resolution</e2> in spoken dialogue "
USAGE(e1,e2)

100	"Our system deals with <e1>pronouns</e1> with <e2>NP- and non-NP-antecedents</e2> "
MODEL-FEATURE(e2,e1)

101	"We present a set of features designed for <e1>pronoun resolution</e1> in <e2>spoken dialogue</e2> and determine the most promising features "
USAGE(e1,e2)

102	" Link detection has been regarded as a core technology for the <e1>Topic Detection and Tracking tasks</e1> of <e2>new event detection</e2> "
PART_WHOLE(e1,e2)

103	" This paper concerns the <e1>discourse understanding process</e1> in <e2>spoken dialogue systems</e2> "
USAGE(e1,e2)

104	"This process enables the system to understand user utterances based on the <e1>context</e1> of a <e2>dialogue</e2> "
MODEL-FEATURE(e1,e2)

105	"Since multiple <e1>candidates</e1> for the <e2>understanding</e2> result can be obtained for a user utterance due to the ambiguity of speech understanding , it is not appropriate to decide on a single understanding result after each user utterance "
USAGE(e1,e2)

106	"Since multiple candidates for the understanding result can be obtained for a user utterance due to the <e1>ambiguity</e1> of <e2>speech understanding</e2> , it is not appropriate to decide on a single understanding result after each user utterance "
MODEL-FEATURE(e1,e2)

107	"By holding multiple <e1>candidates</e1> for <e2>understanding</e2> results and resolving the ambiguity as the dialogue progresses, the discourse understanding accuracy can be improved"
USAGE(e1,e2)

108	"This paper proposes a method for resolving this ambiguity based on <e1>statistical information</e1> obtained from <e2>dialogue corpora</e2> "
MODEL-FEATURE(e1,e2)

109	"Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple <e1>candidates</e1> for <e2>understanding</e2> results is effective"
USAGE(e1,e2)

110	" We address appropriate <e1>user modeling</e1> in order to generate <e2>cooperative responses</e2> to each user in spoken dialogue systems "
USAGE(e1,e2)

111	"Moreover, the <e1>models</e1> are automatically derived by <e2>decision tree learning</e2> using real dialogue data collected by the system"
MODEL-FEATURE(e2,e1)

112	"<e1>Dialogue strategies</e1> based on the <e2>user modeling</e2> are implemented in Kyoto city bus information system that has been developed at our laboratory"
USAGE(e2,e1)

113	" This paper presents an <e1>unsupervised learning approach</e1> to building a <e2>non-English &amp;&amp;lpar&amp;&amp;Arabic&amp;&amp;rpar&amp;&amp; stemmer</e2> "
USAGE(e1,e2)

114	"The <e1>stemming model</e1> is based on <e2>statistical machine translation</e2> and it uses an English stemmer and a small (10K sentences) parallel corpus as its sole training resources "
USAGE(e2,e1)

115	"Examples and results will be given for Arabic , but the approach is applicable to any <e1>language</e1> that needs <e2>affix removal</e2> "
MODEL-FEATURE(e2,e1)

116	"Our <e1>resource-frugal approach</e1> results in 87.5% <e2>agreement</e2> with a state of the art, proprietary Arabic stemmer built using rules , affix lists , and human annotated text , in addition to an unsupervised component "
RESULT(e1,e2)

117	"<e1>Task-based evaluation</e1> using <e2>Arabic information retrieval</e2> indicates an improvement of 22-38% in average precision over unstemmed text , and 96% of the performance of the proprietary stemmer above"
USAGE(e2,e1)

118	"Our method is seeded by a small <e1>manually segmented Arabic corpus</e1> and uses it to bootstrap an <e2>unsupervised algorithm</e2> to build the Arabic word segmenter from a large unsegmented Arabic corpus "
USAGE(e1,e2)

119	"Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the <e1>Arabic word segmenter</e1> from a large <e2>unsegmented Arabic corpus</e2> "
USAGE(e2,e1)

120	"The algorithm uses a trigram language model to determine the most probable <e1>morpheme sequence</e1> for a given <e2>input</e2> "
MODEL-FEATURE(e1,e2)

121	"The <e1>language model</e1> is initially estimated from a small <e2>manually segmented corpus</e2> of about 110,000 words "
MODEL-FEATURE(e1,e2)

122	"To improve the segmentation accuracy , we use an unsupervised algorithm for automatically acquiring new <e1>stems</e1> from a 155 million word <e2>unsegmented corpus</e2> , and re-estimate the model parameters with the expanded vocabulary and training corpus "
PART_WHOLE(e1,e2)

123	"To improve the segmentation accuracy , we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus , and re-estimate the <e1>model parameters</e1> with the expanded <e2>vocabulary</e2> and training corpus "
USAGE(e1,e2)

124	"The resulting <e1>Arabic word segmentation system</e1> achieves around 97% <e2>exact match accuracy</e2> on a test corpus containing 28,449 word tokens "
RESULT(e1,e2)

125	"The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a <e1>test corpus</e1> containing 28,449 <e2>word tokens</e2> "
PART_WHOLE(e2,e1)

126	"We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small <e1>manually segmented corpus</e1> of the <e2>language</e2> of interest"
MODEL-FEATURE(e1,e2)

127	"  A central problem of <e1>word sense disambiguation &amp;&amp;lpar&amp;&amp;WSD&amp;&amp;rpar&amp;&amp;</e1> is the lack of <e2>manually sense-tagged data</e2> required for supervised learning "
USAGE(e2,e1)

128	"In this paper, we evaluate an approach to automatically acquire <e1>sense-tagged training data</e1> from <e2>English-Chinese parallel corpora</e2> , which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task "
MODEL-FEATURE(e1,e2)

129	"Our analysis also highlights the importance of the issue of <e1>domain dependence</e1> in evaluating <e2>WSD programs</e2> "
MODEL-FEATURE(e1,e2)

130	" We describe the ongoing construction of a large, <e1>semantically annotated corpus</e1> resource as reliable basis for the large-scale <e2>acquisition of word-semantic information</e2> , e.g. the construction of domain-independent lexica "
USAGE(e1,e2)

131	"On this basis, we discuss the problems of vagueness and <e1>ambiguity</e1> in <e2>semantic annotation</e2> "
MODEL-FEATURE(e1,e2)

132	"We analyzed eye gaze , head nods and <e1>attentional focus</e1> in the context of a <e2>direction-giving task</e2> "
MODEL-FEATURE(e1,e2)

133	"Based on these results, we present an <e1>ECA</e1> that uses <e2>verbal and nonverbal grounding acts</e2> to update dialogue state "
USAGE(e2,e1)

134	"We demonstrate that an approximation of <e1>HPSG</e1> produces a more effective CFG filter than that of <e2>LTAG</e2> "
COMPARE(e1,e2)

135	"We report experiments conducted on a multilingual corpus to estimate the number of <e1>analogies</e1> among the <e2>sentences</e2> that it contains"
MODEL-FEATURE(e1,e2)

136	" <e1>CriterionSM Online Essay Evaluation Service</e1> includes a capability that labels sentences in student <e2>writing</e2> with essay-based discourse elements (e.g., thesis statements )"
USAGE(e1,e2)

137	"We describe a new system that enhances Criterion 's capability, by evaluating multiple aspects of <e1>coherence</e1> in <e2>essays</e2> "
MODEL-FEATURE(e1,e2)

138	"This system identifies <e1>features</e1> of <e2>sentences</e2> based on semantic similarity measures and discourse structure "
MODEL-FEATURE(e1,e2)

139	"A <e1>support vector machine</e1> uses these <e2>features</e2> to capture breakdowns in coherence due to relatedness to the essay question and relatedness between discourse elements "
USAGE(e2,e1)

140	"<e1>Intra-sentential quality</e1> is evaluated with <e2>rule-based heuristics</e2> "
TOPIC(e2,e1)

141	" In this paper, we use the <e1>information redundancy</e1> in <e2>multilingual input</e2> to correct errors in machine translation and thus improve the quality of multilingual summaries "
MODEL-FEATURE(e1,e2)

142	"We consider the case of multi-document summarization , where the input <e1>documents</e1> are in <e2>Arabic</e2> , and the output summary is in English "
MODEL-FEATURE(e2,e1)

143	"We consider the case of multi-document summarization , where the input documents are in Arabic , and the output <e1>summary</e1> is in <e2>English</e2> "
MODEL-FEATURE(e2,e1)

144	"Further, the use of multiple machine translation systems provides yet more redundancy , yielding different ways to realize that <e1>information</e1> in <e2>English</e2> "
MODEL-FEATURE(e2,e1)

145	" This paper presents a <e1>maximum entropy word alignment algorithm</e1> for Arabic-English based on <e2>supervised training data</e2> "
USAGE(e2,e1)

146	"We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of <e1>supervised and unsupervised methods</e1> yields superior <e2>performance</e2> "
RESULT(e1,e2)

147	"The <e1>probabilistic model</e1> used in the <e2>alignment</e2> directly models the link decisions "
USAGE(e1,e2)

148	" This paper presents a <e1>phrase-based statistical machine translation method</e1> , based on <e2>non-contiguous phrases</e2> , i.e. phrases with gaps"
USAGE(e2,e1)

149	"A method for producing such <e1>phrases</e1> from a <e2>word-aligned corpora</e2> is proposed"
PART_WHOLE(e1,e2)

150	"A <e1>statistical translation model</e1> is also presented that deals such <e2>phrases</e2> , as well as a training method based on the maximization of translation accuracy , as measured with the NIST evaluation metric "
USAGE(e1,e2)

151	"A statistical translation model is also presented that deals such phrases , as well as a <e1>training method</e1> based on the maximization of <e2>translation accuracy</e2> , as measured with the NIST evaluation metric "
USAGE(e2,e1)

152	"<e1>Translations</e1> are produced by means of a <e2>beam-search decoder</e2> "
USAGE(e2,e1)

153	" Following recent developments in the <e1>automatic evaluation</e1> of <e2>machine translation</e2> and document summarization , we present a similar approach, implemented in a measure called POURPRE , for automatically evaluating answers to definition questions "
USAGE(e1,e2)

154	"Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that <e1>rankings</e1> produced by our metric correlate highly with <e2>official rankings</e2> , and that POURPRE outperforms direct application of existing metrics"
COMPARE(e1,e2)

155	" We describe a method for identifying systematic <e1>patterns</e1> in <e2>translation data</e2> using part-of-speech tag sequences "
PART_WHOLE(e1,e2)

156	"We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems , and demonstrate how our application can be used by developers to explore <e1>patterns</e1> in <e2>machine translation output</e2> "
PART_WHOLE(e1,e2)

157	"At the same time, the recent improvements in the BLEU scores of statistical machine translation &amp;&amp;lpar&amp;&amp;SMT&amp;&amp;rpar&amp;&amp; suggests that SMT models are good at predicting the right <e1>translation</e1> of the <e2>words</e2> in source language sentences "
MODEL-FEATURE(e1,e2)

158	"Surprisingly however, the WSD <e1>accuracy</e1> of <e2>SMT models</e2> has never been evaluated and compared with that of the dedicated WSD models "
RESULT(e2,e1)

159	"This tends to support the view that despite recent speculative claims to the contrary, current <e1>SMT models</e1> do have limitations in comparison with dedicated <e2>WSD models</e2> , and that SMT should benefit from the better predictions made by the WSD models "
COMPARE(e1,e2)

160	"Over the last few years dramatic improvements have been made, and a number of comparative evaluations have shown, that <e1>SMT</e1> gives competitive results to <e2>rule-based translation systems</e2> , requiring significantly less development time"
COMPARE(e1,e2)

161	" In this paper we present our recent work on harvesting <e1>English-Chinese bitexts</e1> of the laws of Hong Kong from the <e2>Web</e2> and aligning them to the subparagraph level via utilizing the numbering system in the legal text hierarchy "
PART_WHOLE(e1,e2)

162	"This piece of work has also laid a foundation for exploring and harvesting <e1>English-Chinese bitexts</e1> in a larger volume from the <e2>Web</e2> "
PART_WHOLE(e1,e2)

163	" The task of <e1>machine translation &amp;&amp;lpar&amp;&amp;MT&amp;&amp;rpar&amp;&amp; evaluation</e1> is closely related to the task of <e2>sentence-level semantic equivalence classification</e2> "
COMPARE(e1,e2)

164	"This paper investigates the utility of applying standard <e1>MT evaluation methods &amp;&amp;lpar&amp;&amp;BLEU, NIST, WER and PER&amp;&amp;rpar&amp;&amp;</e1> to building <e2>classifiers</e2> to predict semantic equivalence and entailment "
USAGE(e1,e2)

165	"We also introduce a novel <e1>classification method</e1> based on <e2>PER</e2> which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence "
USAGE(e2,e1)

166	"We also introduce a novel classification method based on PER which leverages <e1>part of speech information</e1> of the <e2>words</e2> contributing to the word matches and non-matches in the sentence "
MODEL-FEATURE(e1,e2)

167	"Our results show that <e1>MT evaluation techniques</e1> are able to produce useful <e2>features</e2> for paraphrase classification and to a lesser extent entailment "
RESULT(e1,e2)

168	"Our <e1>technique</e1> gives a substantial improvement in <e2>paraphrase classification accuracy</e2> over all of the other models used in the experiments"
RESULT(e1,e2)

169	" We propose a method that automatically generates <e1>paraphrase</e1> sets from <e2>seed sentences</e2> to be used as reference sets in objective machine translation evaluation measures like BLEU and NIST "
PART_WHOLE(e1,e2)

170	"We measured the quality of the paraphrases produced in an experiment, i.e., (i) their grammaticality : at least 99% correct sentences ; (ii) their equivalence in meaning : at least 96% correct paraphrases either by meaning equivalence or entailment ; and, (iii) the amount of internal <e1>lexical and syntactical variation</e1> in a set of <e2>paraphrases</e2> : slightly superior to that of hand-produced sets "
MODEL-FEATURE(e1,e2)

171	"The paraphrase sets produced by this method thus seem adequate as <e1>reference sets</e1> to be used for <e2>MT evaluation</e2> "
USAGE(e1,e2)

172	" This paper proposes an <e1>annotating scheme</e1> that encodes <e2>honorifics</e2> (respectful words)"
MODEL-FEATURE(e1,e2)

173	"This <e1>referential information</e1> is vital for resolving zero pronouns and improving <e2>machine translation outputs</e2> "
RESULT(e1,e2)

174	"Annotating honorifics is a complex task that involves identifying a predicate with honorifics , assigning <e1>ranks</e1> to <e2>referents</e2> of the predicate , calibrating the ranks , and connecting referents with their predicates "
MODEL-FEATURE(e1,e2)

175	"The base parser produces a set of <e1>candidate parses</e1> for each input <e2>sentence</e2> , with associated probabilities that define an initial ranking of these parses "
MODEL-FEATURE(e1,e2)

176	"A second model then attempts to improve upon this initial ranking , using additional <e1>features</e1> of the <e2>tree</e2> as evidence"
MODEL-FEATURE(e1,e2)

177	"The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a <e1>generative model</e1> which takes these <e2>features</e2> into account "
USAGE(e2,e1)

178	"We introduce a new method for the reranking task , based on the <e1>boosting approach</e1> to <e2>ranking problems</e2> described in Freund et al"
USAGE(e1,e2)

179	"We apply the boosting method to <e1>parsing</e1> the <e2>Wall Street Journal treebank</e2> "
USAGE(e1,e2)

180	"The new <e1>model</e1> achieved 89.75% <e2>F-measure</e2> , a 13% relative decrease in F-measure error over the baseline model's score of 88.2%"
RESULT(e1,e2)

181	"The article also introduces a new algorithm for the boosting approach which takes advantage of the <e1>sparsity of the feature space</e1> in the <e2>parsing data</e2> "
MODEL-FEATURE(e1,e2)

182	"We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on <e1>feature selection methods</e1> within <e2>log-linear &amp;&amp;lpar&amp;&amp;maximum-entropy&amp;&amp;rpar&amp;&amp; models</e2> "
PART_WHOLE(e1,e2)

183	" <SectionTitle /> We present a novel method for <e1>discovering parallel sentences</e1> in <e2>comparable, non-parallel corpora</e2> "
USAGE(e1,e2)

184	"Using this approach, we extract <e1>parallel data</e1> from large <e2>Chinese, Arabic, and English non-parallel newspaper corpora</e2> "
PART_WHOLE(e1,e2)

185	"We also show that a good-quality <e1>MT system</e1> can be built from scratch by starting with a very small <e2>parallel corpus</e2> (100,000 words ) and exploiting a large non-parallel corpus "
USAGE(e2,e1)

186	"We detail the computational complexity and average retrieval times for looking up <e1>phrase translations</e1> in our <e2>suffix array-based data structure</e2> "
PART_WHOLE(e1,e2)

187	" We describe a novel approach to statistical machine translation that combines <e1>syntactic information</e1> in the <e2>source language</e2> with recent advances in phrasal translation "
MODEL-FEATURE(e1,e2)

188	"We align a parallel corpus , project the <e1>source dependency parse</e1> onto the target <e2>sentence</e2> , extract dependency treelet translation pairs , and train a tree-based ordering model "
MODEL-FEATURE(e1,e2)

189	"Using a state-of-the-art <e1>Chinese word sense disambiguation model</e1> to choose <e2>translation candidates</e2> for a typical IBM statistical MT system , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone"
USAGE(e1,e2)

190	"Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that <e1>word sense disambiguation</e1> does not yield significantly better <e2>translation quality</e2> than the statistical machine translation system alone"
RESULT(e1,e2)

191	" Syntax-based statistical machine translation &amp;&amp;lpar&amp;&amp;MT&amp;&amp;rpar&amp;&amp; aims at applying <e1>statistical models</e1> to <e2>structured data</e2> "
USAGE(e1,e2)

192	"In this paper, we present a <e1>syntax-based statistical machine translation system</e1> based on a <e2>probabilistic synchronous dependency insertion grammar</e2> "
USAGE(e2,e1)

193	"We first introduce our approach to inducing such a <e1>grammar</e1> from <e2>parallel corpora</e2> "
PART_WHOLE(e1,e2)

194	"Second, we describe the <e1>graphical model</e1> for the <e2>machine translation task</e2> , which can also be viewed as a stochastic tree-to-tree transducer "
USAGE(e1,e2)

195	"The result shows that our system outperforms the <e1>baseline system</e1> based on the <e2>IBM models</e2> in both translation speed and quality "
USAGE(e2,e1)

196	" In this paper, we present a novel training method for a <e1>localized phrase-based prediction model</e1> for <e2>statistical machine translation &amp;&amp;lpar&amp;&amp;SMT&amp;&amp;rpar&amp;&amp;</e2> "
USAGE(e1,e2)

197	"We use a maximum likelihood criterion to train a <e1>log-linear block bigram model</e1> which uses <e2>real-valued features</e2> (e.g. a language model score ) as well as binary features based on the block identities themselves, e.g. block bigram features"
USAGE(e2,e1)

198	" Previous work has used <e1>monolingual parallel corpora</e1> to extract and generate <e2>paraphrases</e2> "
PART_WHOLE(e2,e1)

199	"We define a paraphrase probability that allows <e1>paraphrases</e1> extracted from a <e2>bilingual parallel corpus</e2> to be ranked using translation probabilities , and show how it can be refined to take contextual information into account"
PART_WHOLE(e1,e2)

200	"We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with <e1>paraphrases</e1> extracted from <e2>automatic alignments</e2> "
PART_WHOLE(e1,e2)

201	" We present a <e1>Czech-English statistical machine translation system</e1> which performs <e2>tree-to-tree translation</e2> of dependency structures "
USAGE(e1,e2)

202	"We also refer to an evaluation method and plan to compare our <e1>system's output</e1> with a <e2>benchmark system</e2> "
COMPARE(e1,e2)

203	"The combination with a <e1>two-step clustering process</e1> using <e2>sentence co-occurrences</e2> as features allows for accurate results"
USAGE(e2,e1)

204	" We present results on <e1>addressee identification</e1> in <e2>four-participants face-to-face meetings</e2> using Bayesian Network and Naive Bayes classifiers "
USAGE(e1,e2)

205	"The <e1>classifiers</e1> show little <e2>gain</e2> from information about meeting context "
RESULT(e1,e2)

206	" Most state-of-the-art <e1>evaluation measures</e1> for <e2>machine translation</e2> assign high costs to movements of word blocks"
USAGE(e1,e2)

207	"In this paper, we will present a new <e1>evaluation measure</e1> which explicitly models <e2>block reordering</e2> as an edit operation "
USAGE(e1,e2)

208	"Furthermore, we will show how some <e1>evaluation measures</e1> can be improved by the introduction of <e2>word-dependent substitution costs</e2> "
RESULT(e2,e1)

209	"The correlation of the new <e1>measure</e1> with <e2>human judgment</e2> has been investigated systematically on two different language pairs "
COMPARE(e1,e2)

210	"Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between <e1>automatic evaluation measures</e1> and <e2>human judgment</e2> "
COMPARE(e1,e2)

211	" In this paper, we investigate the problem of automatically predicting <e1>segment boundaries</e1> in <e2>spoken multiparty dialogue</e2> "
PART_WHOLE(e1,e2)

212	"We then explore the impact on performance of using <e1>ASR output</e1> as opposed to <e2>human transcription</e2> "
COMPARE(e1,e2)

213	"Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks: (1) for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results, (2) for <e1>predicting top-level boundaries</e1> , the <e2>machine learning approach</e2> that combines lexical-cohesion and conversational features performs best, and (3) conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task"
USAGE(e2,e1)

214	"We also find that the <e1>transcription errors</e1> inevitable in <e2>ASR output</e2> have a negative impact on models that combine lexical-cohesion and conversational features , but do not change the general preference of approach for the two tasks"
MODEL-FEATURE(e1,e2)

215	" <e1>Combination methods</e1> are an effective way of improving <e2>system performance</e2> "
RESULT(e1,e2)

216	"Our <e1>combination methods</e1> rely on <e2>predominant senses</e2> which are derived automatically from raw text "
USAGE(e2,e1)

217	" We present an efficient algorithm for the redundancy elimination problem : Given an <e1>underspecified semantic representation &amp;&amp;lpar&amp;&amp;USR&amp;&amp;rpar&amp;&amp;</e1> of a <e2>scope ambiguity</e2> , compute an USR with fewer mutually equivalent readings "
MODEL-FEATURE(e1,e2)

218	" In this paper, we describe the research using <e1>machine learning techniques</e1> to build a <e2>comma checker</e2> to be integrated in a grammar checker for Basque "
USAGE(e1,e2)

219	" In this paper, we describe the research using machine learning techniques to build a comma checker to be integrated in a <e1>grammar checker</e1> for <e2>Basque</e2> "
USAGE(e1,e2)

220	"After several experiments, and trained with a little <e1>corpus</e1> of 100,000 <e2>words</e2> , the system guesses correctly not placing commas with a precision of 96% and a recall of 98%"
PART_WHOLE(e2,e1)

221	"Finally, we have shown that these results can be improved using a bigger and a more homogeneous corpus to train, that is, a bigger <e1>corpus</e1> written by one unique <e2>author</e2> "
MODEL-FEATURE(e2,e1)

222	" This paper presents an <e1>unsupervised learning approach</e1> to disambiguate various relations between named entities by use of various <e2>lexical and syntactic features</e2> from the contexts "
USAGE(e2,e1)

223	"Experiment results on ACE corpora show that this <e1>spectral clustering based approach</e1> outperforms the other <e2>clustering methods</e2> "
COMPARE(e1,e2)

224	" This paper proposes a novel method of building <e1>polarity-tagged corpus</e1> from <e2>HTML documents</e2> "
PART_WHOLE(e1,e2)

225	"In our experiment, the method could construct a <e1>corpus</e1> consisting of 126,610 <e2>sentences</e2> "
PART_WHOLE(e2,e1)

226	"   In this paper we show how two standard outputs from information extraction &amp;&amp;lpar&amp;&amp;IE&amp;&amp;rpar&amp;&amp; systems - named entity annotations and <e1>scenario templates</e1> - can be used to enhance access to text collections via a standard <e2>text browser</e2> "
USAGE(e1,e2)

227	"We describe how this information is used in a <e1>prototype system</e1> designed to support information workers ' access to a pharmaceutical news archive as part of their <e2>industry watch</e2> function"
USAGE(e1,e2)

228	"We also report results of a preliminary, qualitative user evaluation of the system, which while broadly positive indicates further work needs to be done on the <e1>interface</e1> to make users aware of the increased potential of <e2>IE-enhanced text browsers</e2> "
PART_WHOLE(e1,e2)

229	"   Recent advances in <e1>Automatic Speech Recognition technology</e1> have put the goal of naturally sounding <e2>dialog systems</e2> within reach"
USAGE(e1,e2)

230	"The issue of <e1>system response</e1> to users has been extensively studied by the <e2>natural language generation community</e2> , though rarely in the context of dialog systems "
TOPIC(e2,e1)

231	"We show how research in <e1>generation</e1> can be adapted to <e2>dialog systems</e2> , and how the high cost of hand-crafting knowledge-based generation systems can be overcome by employing machine learning techniques "
USAGE(e1,e2)

232	"We show how research in generation can be adapted to dialog systems , and how the high cost of hand-crafting <e1>knowledge-based generation systems</e1> can be overcome by employing <e2>machine learning techniques</e2> "
USAGE(e2,e1)

233	"   The <e1>TAP-XL Automated Analyst's Assistant</e1> is an application designed to help an English -speaking analyst write a topical report , culling information from a large inflow of <e2>multilingual, multimedia data</e2> "
USAGE(e2,e1)

234	"   This paper investigates some <e1>computational problems</e1> associated with <e2>probabilistic translation models</e2> that have recently been adopted in the literature on machine translation "
MODEL-FEATURE(e1,e2)

235	"These <e1>models</e1> can be viewed as pairs of <e2>probabilistic context-free grammars</e2> working in a 'synchronous' way"
MODEL-FEATURE(e2,e1)

236	"Two <e1>hardness</e1> results for the class <e2>NP</e2> are reported, along with an exponential time lower-bound for certain classes of algorithms that are currently used in the literature"
MODEL-FEATURE(e2,e1)

237	"   Automatic <e1>evaluation metrics</e1> for <e2>Machine Translation &amp;&amp;lpar&amp;&amp;MT&amp;&amp;rpar&amp;&amp; systems</e2> , such as BLEU or NIST , are now well established"
USAGE(e1,e2)

238	"Yet, they are scarcely used for the assessment of language pairs like English-Chinese or <e1>English-Japanese</e1> , because of the <e2>word segmentation problem</e2> "
MODEL-FEATURE(e2,e1)

239	"This study establishes the equivalence between the standard use of <e1>BLEU</e1> in <e2>word n-grams</e2> and its application at the character level"
USAGE(e1,e2)

240	"The use of <e1>BLEU</e1> at the <e2>character</e2> level eliminates the word segmentation problem : it makes it possible to directly compare commercial systems outputting unsegmented texts with, for instance, statistical MT systems which usually segment their outputs "
USAGE(e1,e2)

241	"The method allows a user to explore a model of syntax-based statistical machine translation &amp;&amp;lpar&amp;&amp;MT&amp;&amp;rpar&amp;&amp; , to understand the <e1>model</e1> 's strengths and weaknesses, and to compare it to other <e2>MT systems</e2> "
COMPARE(e1,e2)

242	"Using this <e1>visualization method</e1> , we can find and address conceptual and practical problems in an <e2>MT system</e2> "
USAGE(e1,e2)

243	"   In this paper we study a set of problems that are of considerable importance to <e1>Statistical Machine Translation &amp;&amp;lpar&amp;&amp;SMT&amp;&amp;rpar&amp;&amp;</e1> but which have not been addressed satisfactorily by the <e2>SMT research community</e2> "
TOPIC(e2,e1)

244	"Over the last decade, a variety of SMT algorithms have been built and empirically tested whereas little is known about the <e1>computational complexity</e1> of some of the fundamental problems of <e2>SMT</e2> "
MODEL-FEATURE(e1,e2)

245	"Since it is unlikely that there exists a <e1>polynomial time solution</e1> for any of these <e2>hard problems</e2> (unless P = NP and P#P = P ), our results highlight and justify the need for developing polynomial time approximations for these computations"
USAGE(e1,e2)

246	"We investigate that claim by adopting a simple <e1>MT-based paraphrasing technique</e1> and evaluating <e2>QA system</e2> performance on paraphrased questions "
USAGE(e1,e2)

247	"   There are several approaches that model information extraction as a <e1>token classification task</e1> , using various <e2>tagging strategies</e2> to combine multiple tokens "
USAGE(e2,e1)

248	"<e1>InfoMagnets</e1> aims at making <e2>exploratory corpus analysis</e2> accessible to researchers who are not experts in text mining "
USAGE(e1,e2)

249	"As evidence of its usefulness and usability, it has been used successfully in a research context to uncover relationships between language and <e1>behavioral patterns</e1> in two distinct domains: <e2>tutorial dialogue </e2>(Kumar et al., submitted) and on-line communities (Arguello et al., 2006)"
PART_WHOLE(e1,e2)

250	"As an <e1>educational tool</e1> , it has been used as part of a unit on <e2>protocol analysis</e2> in an Educational Research Methods course "
USAGE(e1,e2)

251	"The <e1>polarization</e1> of the objects of the <e2>elementary structures</e2> controls the saturation of the final structure "
USAGE(e1,e2)

252	"The polarization of the objects of the elementary structures controls the <e1>saturation</e1> of the final <e2>structure</e2> "
MODEL-FEATURE(e1,e2)

253	"   This paper examines what kind of <e1>similarity</e1> between words can be represented by what kind of <e2>word vectors</e2> in the vector space model "
MODEL-FEATURE(e2,e1)

254	"Through two experiments, three methods for constructing word vectors , i.e., <e1>LSA-based, cooccurrence-based and dictionary-based methods</e1> , were compared in terms of the ability to represent two kinds of <e2>similarity</e2> , i.e., taxonomic similarity and associative similarity "
USAGE(e1,e2)

255	"The result of the comparison was that the <e1>dictionary-based word vectors</e1> better reflect <e2>taxonomic similarity</e2> , while the LSA-based and the cooccurrence-based word vectors better reflect associative similarity "
USAGE(e1,e2)

256	"The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity , while the <e1>LSA-based and the cooccurrence-based word vectors</e1> better reflect <e2>associative similarity</e2> "
USAGE(e1,e2)

257	"In this paper, <e1>events</e1> are defined as <e2>event terms</e2> and associated event elements "
MODEL-FEATURE(e2,e1)

258	"With relevant approach, we identify important contents by PageRank algorithm on the <e1>event map</e1> constructed from <e2>documents</e2> "
USAGE(e2,e1)

259	"   This paper describes FERRET , an <e1>interactive question-answering &amp;&amp;lpar&amp;&amp;Q/A&amp;&amp;rpar&amp;&amp; system</e1> designed to address the challenges of integrating <e2>automatic Q/A</e2> applications into real-world environments"
USAGE(e1,e2)

260	"FERRET utilizes a novel approach to <e1>Q/A</e1> known as <e2>predictive questioning</e2> which attempts to identify the questions (and answers ) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario"
USAGE(e2,e1)

261	"   This paper introduces a method for computational analysis of move structures in <e1>abstracts</e1> of <e2>research articles</e2> "
PART_WHOLE(e1,e2)

262	"In our approach, <e1>sentences</e1> in a given <e2>abstract</e2> are analyzed and labeled with a specific move in light of various rhetorical functions "
PART_WHOLE(e1,e2)

263	"The method involves automatically gathering a large number of <e1>abstracts</e1> from the <e2>Web</e2> and building a language model of abstract moves "
PART_WHOLE(e1,e2)

264	"The method involves automatically gathering a large number of abstracts from the Web and building a <e1>language model</e1> of <e2>abstract moves</e2> "
MODEL-FEATURE(e1,e2)

265	"We also present a prototype concordancer , CARE , which exploits the <e1>move-tagged abstracts</e1> for <e2>digital learning</e2> "
USAGE(e1,e2)

266	"   The LOGON MT demonstrator assembles independently valuable <e1>general-purpose NLP components</e1> into a <e2>machine translation pipeline</e2> that capitalizes on output quality "
PART_WHOLE(e1,e2)

267	"In this format, developed by the LNR research group at The University of California at San Diego, <e1>verbs</e1> are represented as interconnected sets of <e2>subpredicates</e2> "
MODEL-FEATURE(e2,e1)

268	"These subpredicates may be thought of as the almost inevitable inferences that a listener makes when a <e1>verb</e1> is used in a <e2>sentence</e2> "
PART_WHOLE(e1,e2)

269	"They confer a meaning structure on the <e1>sentence</e1> in which the <e2>verb</e2> is used"
PART_WHOLE(e2,e1)

270	"   The paper outlines a <e1>computational theory</e1> of <e2>human plausible reasoning</e2> constructed from analysis of people's answers to everyday questions"
MODEL-FEATURE(e1,e2)

271	"Like logic , the <e1>theory</e1> is expressed in a <e2>content-independent formalism</e2> "
MODEL-FEATURE(e2,e1)

272	"Unlike <e1>logic</e1> , the <e2>theory</e2> specifies how different information in memory affects the certainty of the conclusions drawn"
COMPARE(e1,e2)

273	"The <e1>theory</e1> consists of a <e2>dimensionalized space</e2> of different inference types and their certainty conditions , including a variety of meta-inference types where the inference depends on the person's knowledge about his own knowledge"
PART_WHOLE(e2,e1)

274	"The theory consists of a dimensionalized space of different <e1>inference types</e1> and their <e2>certainty conditions</e2> , including a variety of meta-inference types where the inference depends on the person's knowledge about his own knowledge"
MODEL-FEATURE(e2,e1)

275	"The theory consists of a dimensionalized space of different inference types and their certainty conditions , including a variety of <e1>meta-inference types</e1> where the <e2>inference</e2> depends on the person's knowledge about his own knowledge"
MODEL-FEATURE(e1,e2)

276	"<e1>Path-based inference rules</e1> may be written using a <e2>binary relational calculus notation</e2> "
USAGE(e2,e1)

277	"<e1>Node-based inference</e1> allows a structure of nodes to be inferred from the existence of an instance of a pattern of <e2>node structures</e2> "
USAGE(e2,e1)

278	"<e1>Node-based inference rules</e1> can be constructed in a semantic network using a variant of a <e2>predicate calculus notation</e2> "
USAGE(e2,e1)

279	"<e1>Path-based inference</e1> is more efficient, while <e2>node-based inference</e2> is more general"
COMPARE(e1,e2)

280	"Applications of path-based inference rules to the representation of the extensional equivalence of intensional concepts , and to the explication of <e1>inheritance</e1> in <e2>hierarchies</e2> are sketched"
MODEL-FEATURE(e1,e2)

281	"By using commands or <e1>rules</e1> which are defined to facilitate the construction of format expected or some <e2>mathematical expressions</e2> , elaborate and pretty documents can be successfully obtained"
USAGE(e1,e2)

282	"   An attempt has been made to use an <e1>Augmented Transition Network</e1> as a procedural <e2>dialog model</e2> "
USAGE(e1,e2)

283	"The development of such a model appears to be important in several respects: as a device to represent and to use different <e1>dialog schemata</e1> proposed in empirical <e2>conversation analysis</e2> ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs "
TOPIC(e2,e1)

284	"A standard ATN should be further developed in order to account for the <e1>verbal interactions</e1> of <e2>task-oriented dialogs</e2> "
PART_WHOLE(e1,e2)

285	"   Interpreting <e1>metaphors</e1> is an integral and inescapable process in <e2>human understanding of natural language</e2> "
PART_WHOLE(e1,e2)

286	"This paper discusses a <e1>method of analyzing metaphors</e1> based on the existence of a small number of <e2>generalized metaphor mappings</e2> "
USAGE(e2,e1)

287	"Each <e1>generalized metaphor</e1> contains a <e2>recognition network</e2> , a basic mapping , additional transfer mappings , and an implicit intention component "
PART_WHOLE(e2,e1)

288	"   Current natural language interfaces have concentrated largely on determining the literal <e1>meaning</e1> of <e2>input</e2> from their users "
MODEL-FEATURE(e1,e2)

289	"While such <e1>decoding</e1> is an essential underpinning, much recent work suggests that natural language interfaces will never appear cooperative or graceful unless they also incorporate numerous <e2>non-literal aspects of communication</e2> , such as robust communication procedures "
USAGE(e2,e1)

290	"This paper defends that view, but claims that direct imitation of human performance is not the best way to implement many of these non-literal aspects of communication ; that the new technology of powerful <e1>personal computers</e1> with integral <e2>graphics displays</e2> offers techniques superior to those of humans for these aspects, while still satisfying human communication needs "
PART_WHOLE(e2,e1)

291	"The paper proposes <e1>interfaces</e1> based on a judicious mixture of these techniques and the still valuable methods of more traditional <e2>natural language interfaces</e2>"
COMPARE(e1,e2)

292	"We go, on to describe FlexP , a <e1>bottom-up pattern-matching parser</e1> that we have designed and implemented to provide these flexibilities for <e2>restricted natural language</e2> input to a limited-domain computer system"
USAGE(e1,e2)

293	"   This paper proposes a series of modifications to the <e1>left corner parsing algorithm</e1> for <e2>context-free grammars</e2> "
USAGE(e1,e2)

294	"It is argued that the resulting algorithm is both efficient and flexible and is, therefore, a good choice for the <e1>parser</e1> used in a <e2>natural language interface</e2> "
USAGE(e1,e2)

295	"The system is implemented entirely in Prolog , a <e1>programming language</e1> based on <e2>logic</e2> "
USAGE(e2,e1)

296	"With the aid of a logic-based grammar formalism called extraposition grammars , <e1>Chat-80</e1> translates <e2>English questions</e2> into the Prolog subset of logic "
USAGE(e1,e2)

297	"With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the <e1>Prolog</e1> <e2>subset of logic</e2> "
PART_WHOLE(e2,e1)

298	"The resulting logical expression is then transformed by a planning algorithm into efficient Prolog , cf. <e1>query optimisation</e1> in a <e2>relational database</e2> "
USAGE(e1,e2)

299	"However, a great deal of natural language texts e.g., memos , rough drafts , <e1>conversation transcripts</e1> etc., have features that differ significantly from <e2>neat texts</e2> , posing special problems for readers, such as misspelled words , missing words , poor syntactic construction , missing periods , etc"
COMPARE(e1,e2)

300	"Our solution to these problems is to make use of <e1>expectations</e1> , based both on knowledge of <e2>surface English</e2> and on world knowledge of the situation being described"
USAGE(e2,e1)

301	"These syntactic and semantic expectations can be used to figure out <e1>unknown words</e1> from <e2>context</e2> , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in missing words ( ellipsis ), and resolve referents ( anaphora )"
MODEL-FEATURE(e2,e1)

302	"These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible <e1>word-senses</e1> of <e2>words with multiple meanings</e2> ( ambiguity ), fill in missing words ( ellipsis ), and resolve referents ( anaphora )"
MODEL-FEATURE(e1,e2)

303	"These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in <e1>missing words</e1> ( <e2>ellipsis</e2> ), and resolve referents ( anaphora )"
PART_WHOLE(e1,e2)

304	"These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in missing words ( ellipsis ), and resolve <e1>referents</e1> ( <e2>anaphora</e2> )"
PART_WHOLE(e1,e2)

305	"This method of using <e1>expectations</e1> to aid the understanding of <e2>scruffy texts</e2> has been incorporated into a working computer program called NOMAD , which understands scruffy texts in the domain of Navy messages"
USAGE(e1,e2)

306	"   This abstract describes a <e1>natural language system</e1> which deals usefully with <e2>ungrammatical input</e2> and describes some actual and potential applications of it in computer aided second language learning "
USAGE(e1,e2)

307	"For <e1>English-Japanese machine translation</e1> , the <e2>syntax directed approach</e2> is effective where the Heuristic Parsing Model &amp;&amp;lpar&amp;&amp;HPM&amp;&amp;rpar&amp;&amp; and the Syntactic Role System play important roles"
USAGE(e2,e1)

308	"For <e1>Japanese-English translation</e1> , the <e2>semantics directed approach</e2> is powerful where the Conceptual Dependency Diagram &amp;&amp;lpar&amp;&amp;CDD&amp;&amp;rpar&amp;&amp; and the Augmented Case Marker System (which is a kind of Semantic Role System ) play essential roles"
USAGE(e1,e2)

309	"Some examples of the difference between <e1>Japanese sentence structure</e1> and <e2>English sentence structure</e2> , which is vital to machine translation are also discussed together with various interesting ambiguities "
COMPARE(e1,e2)

310	"In this approach, the definitions of the structure and <e1>surface representation</e1> of <e2>domain entities</e2> are grouped together"
MODEL-FEATURE(e1,e2)

311	"In addition, it facilitates fragmentary recognition and the use of <e1>multiple parsing strategies</e1> , and so is particularly useful for robust <e2>recognition of extra-grammatical input</e2> "
USAGE(e1,e2)

312	"Representative samples from an entity-oriented language definition are presented, along with a <e1>control structure</e1> for an <e2>entity-oriented parser</e2> , some parsing strategies that use the control structure , and worked examples of parses "
USAGE(e1,e2)

313	"Representative samples from an entity-oriented language definition are presented, along with a control structure for an entity-oriented parser , some <e1>parsing strategies</e1> that use the <e2>control structure</e2> , and worked examples of parses "
USAGE(e2,e1)

314	"A <e1>parser</e1> incorporating the <e2>control structure</e2> and the parsing strategies is currently under implementation "
PART_WHOLE(e2,e1)

315	"An idea which underlies the theory described in this paper is that a disposition may be viewed as a <e1>proposition</e1> with implicit <e2>fuzzy quantifiers</e2> which are approximations to all and always, e.g., almost all, almost always, most, frequently, etc"
PART_WHOLE(e2,e1)

316	"For example, birds can fly may be interpreted as the result of suppressing the <e1>fuzzy quantifier</e1> most in the <e2>proposition</e2> most birds can fly"
PART_WHOLE(e1,e2)

317	"Explicitation sets the stage for representing the <e1>meaning</e1> of a <e2>proposition</e2> through the use of test-score semantics (Zadeh, 1978, 1982)"
MODEL-FEATURE(e1,e2)

318	"In this approach to semantics , the <e1>meaning</e1> of a <e2>proposition</e2> , p, is represented as a procedure which tests, scores and aggregates the elastic constraints which are induced by p"
MODEL-FEATURE(e1,e2)

319	"The paper closes with a description of an approach to <e1>reasoning with dispositions</e1> which is based on the concept of a <e2>fuzzy syllogism</e2> "
USAGE(e2,e1)

320	"<e1>Syllogistic reasoning with dispositions</e1> has an important bearing on <e2>commonsense reasoning</e2> as well as on the management of uncertainty in expert systems "
RESULT(e1,e2)

321	"Syllogistic reasoning with dispositions has an important bearing on commonsense reasoning as well as on the <e1>management of uncertainty</e1> in <e2>expert systems</e2> "
PART_WHOLE(e1,e2)

322	"As a simple application of the techniques described in this paper, we formulate a definition of <e1>typicality</e1> -- a concept which plays an important role in <e2>human cognition</e2> and is of relevance to default reasoning "
RESULT(e1,e2)

323	"   This report describes Paul , a <e1>computer text generation system</e1> designed to create cohesive text through the use of <e2>lexical substitutions</e2> "
USAGE(e2,e1)

324	"Specifically, this system is designed to deterministically choose between <e1>pronominalization</e1> , <e2>superordinate substitution</e2> , and definite noun phrase reiteration "
COMPARE(e1,e2)

325	"The system identifies a strength of <e1>antecedence recovery</e1> for each of the <e2>lexical substitutions</e2> , and matches them against the strength of potential antecedence of each element in the text to select the proper substitutions for these elements"
MODEL-FEATURE(e1,e2)

326	"The system identifies a strength of antecedence recovery for each of the lexical substitutions , and matches them against the <e1>strength of potential antecedence</e1> of each element in the text to select the proper <e2>substitutions</e2> for these elements"
MODEL-FEATURE(e1,e2)

327	"   Determiners play an important role in conveying the <e1>meaning</e1> of an <e2>utterance</e2> , but they have often been disregarded, perhaps because it seemed more important to devise methods to grasp the global meaning  of a sentence , even if not in a precise way"
MODEL-FEATURE(e1,e2)

328	"   Determiners play an important role in conveying the meaning of an utterance , but they have often been disregarded, perhaps because it seemed more important to devise methods to grasp the <e1>global meaning </e1> of a <e2>sentence</e2> , even if not in a precise way"
MODEL-FEATURE(e1,e2)

329	"Another problem with <e1>determiners</e1> is their inherent <e2>ambiguity</e2> "
MODEL-FEATURE(e2,e1)

330	"In this paper we propose a logical formalism , which, among other things, is suitable for representing determiners without forcing a particular <e1>interpretation</e1> when their <e2>meaning</e2> is still not clear"
MODEL-FEATURE(e1,e2)

331	"   This paper describes a system ( <e1>RAREAS</e1> ) which synthesizes marine weather forecasts directly from <e2>formatted weather data</e2> "
USAGE(e1,e2)

332	"Such synthesis appears feasible in certain <e1>natural sublanguages</e1> with <e2>stereotyped text structure</e2> "
MODEL-FEATURE(e2,e1)

333	"<e1>RAREAS</e1> draws on several kinds of <e2>linguistic and non-linguistic knowledge</e2> and mirrors a forecaster's apparent tendency to ascribe less precise temporal adverbs to more remote meteorological events"
USAGE(e2,e1)

334	"   A method for <e1>error correction</e1> of <e2>ill-formed input</e2> is described that acquires dialogue patterns in typical usage and uses these patterns to predict new inputs"
USAGE(e1,e2)

335	"A <e1>dialogue acquisition and tracking algorithm</e1> is presented along with a description of its implementation in a <e2>voice interactive system</e2> "
PART_WHOLE(e1,e2)

336	"   In this paper we explore a new theory of discourse structure that stresses the role of purpose and <e1>processing</e1> in <e2>discourse</e2> "
RESULT(e1,e2)

337	"In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of <e1>utterances</e1> (called the <e2>linguistic structure</e2> ), a structure of purposes (called the intentional structure ), and the state of focus of attention (called the attentional state )"
MODEL-FEATURE(e2,e1)

338	"In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure ), a structure of <e1>purposes</e1> (called the <e2>intentional structure</e2> ), and the state of focus of attention (called the attentional state )"
MODEL-FEATURE(e2,e1)

339	"In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure ), a structure of purposes (called the intentional structure ), and the state of <e1>focus of attention</e1> (called the <e2>attentional state</e2> )"
MODEL-FEATURE(e2,e1)

340	"The linguistic structure consists of segments of the <e1>discourse</e1> into which the <e2>utterances</e2> naturally aggregate"
PART_WHOLE(e2,e1)

341	"The <e1>intentional structure</e1> captures the <e2>discourse-relevant purposes</e2> , expressed in each of the linguistic segments as well as relationships among them"
MODEL-FEATURE(e1,e2)

342	"The <e1>attentional state</e1> is an abstraction of the <e2>focus of attention</e2> of the participants as the discourse unfolds"
MODEL-FEATURE(e1,e2)

343	"The <e1>theory of attention, intention, and aggregation of utterances</e1> is illustrated in the paper with a number of example <e2>discourses</e2> "
MODEL-FEATURE(e1,e2)

344	"This theory provides a framework for describing the processing of <e1>utterances</e1> in a <e2>discourse</e2> "
PART_WHOLE(e1,e2)

345	"Discourse processing requires recognizing how the <e1>utterances</e1> of the <e2>discourse</e2> aggregate into segments , recognizing the intentions expressed in the discourse and the relationships among intentions , and tracking the discourse through the operation of the mechanisms associated with attentional state "
PART_WHOLE(e1,e2)

346	"Discourse processing requires recognizing how the utterances of the discourse aggregate into segments , recognizing the <e1>intentions</e1> expressed in the <e2>discourse</e2> and the relationships among intentions , and tracking the discourse through the operation of the mechanisms associated with attentional state "
PART_WHOLE(e1,e2)

347	"   The goal of this work is the enrichment of <e1>human-machine interactions</e1> in a <e2>natural language environment</e2> "
MODEL-FEATURE(e2,e1)

348	"Because a <e1>speaker</e1> and <e2>listener</e2> cannot be assured to have the same beliefs , contexts , perceptions , backgrounds , or goals , at each point in a conversation , difficulties and mistakes arise when a listener interprets a speaker's utterance "
COMPARE(e1,e2)

349	"   We examine the relationship between the two grammatical formalisms : <e1>Tree Adjoining Grammars</e1> and <e2>Head Grammars</e2> "
COMPARE(e1,e2)

350	"We briefly investigate the weak <e1>equivalence</e1> of the two <e2>formalisms</e2> "
MODEL-FEATURE(e1,e2)

351	"We then turn to a discussion comparing the <e1>linguistic expressiveness</e1> of the two <e2>formalisms</e2> "
MODEL-FEATURE(e1,e2)

352	"   Unification-based grammar formalisms use structures containing sets of <e1>features</e1> to describe <e2>linguistic objects</e2> "
MODEL-FEATURE(e1,e2)

353	"We have developed a model in which descriptions of <e1>feature structures</e1> can be regarded as <e2>logical formulas</e2> , and interpreted by sets of directed graphs which satisfy them"
MODEL-FEATURE(e1,e2)

354	"These graphs are, in fact, <e1>transition graphs</e1> for a special type of <e2>deterministic finite automaton</e2> "
PART_WHOLE(e1,e2)

355	"This <e1>semantics</e1> for <e2>feature structures</e2> extends the ideas of Pereira and Shieber [11], by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions "
MODEL-FEATURE(e1,e2)

356	"This semantics for feature structures extends the ideas of Pereira and Shieber [11], by providing an interpretation for values which are specified by disjunctions and <e1>path values</e1> embedded within <e2>disjunctions</e2> "
PART_WHOLE(e1,e2)

357	"Our interpretation differs from that of Pereira and Shieber by using a <e1>logical model</e1> in place of a <e2>denotational semantics</e2> "
COMPARE(e1,e2)

358	"This <e1>logical model</e1> yields a calculus of equivalences , which can be used to simplify <e2>formulas</e2> "
USAGE(e1,e2)

359	"Our model allows a careful examination of the <e1>computational complexity</e1> of <e2>unification</e2> "
MODEL-FEATURE(e1,e2)

360	"We have shown that the <e1>consistency problem</e1> for <e2>formulas</e2> with disjunctive values is NP-complete "
MODEL-FEATURE(e1,e2)

361	"Multimedia answers include videodisc images and heuristically-produced complete sentences in <e1>text</e1> or <e2>text-to-speech form</e2> "
COMPARE(e1,e2)

362	"Deictic reference and <e1>feedback</e1> about the <e2>discourse</e2> are enabled"
MODEL-FEATURE(e1,e2)

363	"   In this paper, we describe the <e1>pronominal anaphora resolution module</e1> of <e2>Lucy</e2> , a portable English understanding system "
PART_WHOLE(e1,e2)

364	"Thus we have implemented a <e1>blackboard-like architecture</e1> in which individual <e2>partial theories</e2> can be encoded as separate modules that can interact to propose candidate antecedents and to evaluate each other's proposals"
PART_WHOLE(e2,e1)

365	"   This paper discusses the application of <e1>Unification Categorial Grammar &amp;&amp;lpar&amp;&amp;UCG&amp;&amp;rpar&amp;&amp;</e1> to the framework of Isomorphic Grammars for <e2>Machine Translation</e2> pioneered by Landsbergen"
USAGE(e1,e2)

366	"The Isomorphic Grammars approach to MT involves developing the <e1>grammars</e1> of the <e2>Source and Target languages</e2> in parallel, in order to ensure that SL and TL expressions which stand in the translation relation have isomorphic derivations "
MODEL-FEATURE(e1,e2)

367	"The Isomorphic Grammars approach to MT involves developing the grammars of the Source and Target languages in parallel, in order to ensure that <e1>SL</e1> and <e2>TL</e2> expressions which stand in the translation relation have isomorphic derivations "
COMPARE(e1,e2)

368	"Semantic and other information may still be incorporated, but as constraints on the <e1>translation relation</e1> , not as levels of <e2>textual representation</e2> "
COMPARE(e1,e2)

369	"   This paper presents necessary and sufficient conditions for the use of <e1>demonstrative expressions</e1> in <e2>English</e2> and discusses implications for current discourse processing algorithms "
PART_WHOLE(e1,e2)

370	"We examine a broad range of <e1>texts</e1> to show how the distribution of <e2>demonstrative forms and functions</e2> is genre dependent "
PART_WHOLE(e2,e1)

371	"CCRs are <e1>Boolean conditions</e1> on the cooccurrence of <e2>categories</e2> in local trees which allow the statement of generalizations which cannot be captured in other current syntax formalisms "
MODEL-FEATURE(e1,e2)

372	"The use of CCRs leads to <e1>syntactic descriptions</e1> formulated entirely with <e2>restrictive statements</e2> "
MODEL-FEATURE(e2,e1)

373	"The paper shows how conventional algorithms for the analysis of <e1>context free languages</e1> can be adapted to the <e2>CCR formalism</e2> "
COMPARE(e1,e2)

374	"Special attention is given to the part of the parser that checks the fulfillment of <e1>logical well-formedness conditions</e1> on <e2>trees</e2> "
MODEL-FEATURE(e1,e2)

375	"By reappraising these insightful counterexamples, the inferential theory for natural language presuppositions described in Mercer 1987, 1988 gives a simple and straightforward explanation for the <e1>presuppositional nature</e1> of these <e2>sentences</e2> "
MODEL-FEATURE(e1,e2)

376	"   We have developed a <e1>computational model</e1> of the process of describing the layout of an apartment or house, a much-studied <e2>discourse task</e2> first characterized linguistically by Linde (1974)"
USAGE(e1,e2)

377	"The <e1>model</e1> is embodied in a program, <e2>APT</e2> , that can reproduce segments of actual tape-recorded descriptions, using organizational and discourse strategies derived through analysis of our corpus "
PART_WHOLE(e1,e2)

378	"So, for any place where the easily identifiable <e1>fragments</e1> occur in the <e2>sentence</e2> , the process will extend to both the left and the right of the islands , until possibly completely missing fragments are reached"
PART_WHOLE(e1,e2)

379	"This paper presents a new interactive disambiguation scheme based on the <e1>paraphrasing</e1> of a <e2>parser</e2> 's multiple output"
RESULT(e2,e1)

380	"Some examples of <e1>paraphrasing</e1> ambiguous <e2>sentences</e2> are presented"
MODEL-FEATURE(e1,e2)

381	"For one thing, learning methodology applicable in <e1>general domains</e1> does not readily lend itself in the <e2>linguistic domain</e2> "
COMPARE(e1,e2)

382	"For another, <e1>linguistic representation</e1> used by <e2>language processing systems</e2> is not geared to learning "
USAGE(e1,e2)

383	"We introduced a new linguistic representation , the <e1>Dynamic Hierarchical Phrasal Lexicon &amp;&amp;lpar&amp;&amp;DHPL&amp;&amp;rpar&amp;&amp;</e1> [Zernik88], to facilitate <e2>language acquisition</e2> "
USAGE(e1,e2)

384	"From this, a <e1>language learning model</e1> was implemented in the program RINA , which enhances its own <e2>lexical hierarchy</e2> by processing examples in context"
PART_WHOLE(e2,e1)

385	"We identified two tasks: First, how <e1>linguistic concepts</e1> are acquired from <e2>training examples</e2> and organized in a hierarchy ; this task was discussed in previous papers [Zernik87]"
MODEL-FEATURE(e1,e2)

386	"Second, we show in this paper how a <e1>lexical hierarchy</e1> is used in predicting new <e2>linguistic concepts</e2> "
USAGE(e1,e2)

387	"Thus, a program does not stall even in the presence of a <e1>lexical unknown</e1> , and a <e2>hypothesis</e2> can be produced for covering that lexical gap "
MODEL-FEATURE(e2,e1)

388	"   Although every <e1>natural language system</e1> needs a <e2>computational lexicon</e2> , each system puts different amounts and types of information into its lexicon according to its individual needs"
USAGE(e2,e1)

389	"This paper presents our experience in planning and building COMPLEX , a <e1>computational lexicon</e1> designed to be a repository of <e2>shared lexical information</e2> for use by Natural Language Processing &amp;&amp;lpar&amp;&amp;NLP&amp;&amp;rpar&amp;&amp; systems "
PART_WHOLE(e2,e1)

390	"We have drawn primarily on explicit and implicit information from <e1>machine-readable dictionaries &amp;&amp;lpar&amp;&amp;MRD's&amp;&amp;rpar&amp;&amp;</e1> to create a <e2>broad coverage lexicon</e2> "
USAGE(e1,e2)

391	"This paper explores the role of <e1>user modeling</e1> in such <e2>systems</e2> "
RESULT(e1,e2)

392	"The types of information that a <e1>user model</e1> may be required to keep about a <e2>user</e2> are then identified and discussed"
MODEL-FEATURE(e1,e2)

393	"Since acquiring the knowledge for a <e1>user model</e1> is a fundamental problem in <e2>user modeling</e2> , a section is devoted to this topic"
PART_WHOLE(e1,e2)

394	"   This article introduces a bidirectional grammar generation system called <e1>feature structure-directed generation</e1> , developed for a <e2>dialogue translation system</e2> "
USAGE(e1,e2)

395	"The system utilizes <e1>typed feature structures</e1> to control the <e2>top-down derivation</e2> in a declarative way"
USAGE(e1,e2)

396	"This <e1>generation system</e1> also uses <e2>disjunctive feature structures</e2> to reduce the number of copies of the derivation tree "
USAGE(e2,e1)

397	"The <e1>grammar</e1> for this <e2>generator</e2> is designed to properly generate the speaker's intention in a telephone dialogue "
USAGE(e1,e2)

398	"   This paper proposes document oriented preference sets&amp;&amp;lpar&amp;&amp;DoPS&amp;&amp;rpar&amp;&amp; for the disambiguation of the <e1>dependency structure</e1> of <e2>sentences</e2> "
MODEL-FEATURE(e1,e2)

399	"The <e1>DoPS system</e1> extracts preference knowledge from a <e2>target document</e2> or other documents automatically"
USAGE(e1,e2)

400	"Implementation and empirical results are described for the the analysis of <e1>dependency structures</e1> of <e2>Japanese patent claim sentences</e2> "
MODEL-FEATURE(e1,e2)

401	"   This paper describes the framework of a <e1>Korean phonological knowledge base system</e1> using the <e2>unification-based grammar formalism</e2> : Korean Phonology Structure Grammar &amp;&amp;lpar&amp;&amp;KPSG&amp;&amp;rpar&amp;&amp; "
USAGE(e2,e1)

402	"The approach of <e1>KPSG</e1> provides an explicit development model for constructing a computational <e2>phonological system</e2> : speech recognition and synthesis system "
USAGE(e1,e2)

403	"   The unique properties of tree-adjoining grammars &amp;&amp;lpar&amp;&amp;TAG&amp;&amp;rpar&amp;&amp; present a challenge for the application of TAGs beyond the limited confines of <e1>syntax</e1> , for instance, to the task of <e2>semantic interpretation</e2> or automatic translation of natural language "
COMPARE(e1,e2)

404	"The formalism's intended usage is to relate expressions of natural languages to their associated <e1>semantics</e1> represented in a <e2>logical form language</e2> , or to their translates in another natural language ; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper "
MODEL-FEATURE(e2,e1)

405	"The formalism's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their <e1>translates</e1> in another <e2>natural language</e2> ; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper "
MODEL-FEATURE(e1,e2)

406	"The formalism's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary, we intend it to allow <e1>TAGs</e1> to be used beyond their role in <e2>syntax proper</e2> "
USAGE(e1,e2)

407	"   This paper proposes that <e1>sentence analysis</e1> should be treated as <e2>defeasible reasoning</e2> , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility "
MODEL-FEATURE(e1,e2)

408	"   This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for <e1>Japanese sentence analyses</e1> using an <e2>argumentation system</e2> by Konolige, which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility "
USAGE(e2,e1)

409	"   This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a <e1>formalization</e1> of <e2>defeasible reasoning</e2> , that includes arguments and defeat rules that capture defeasibility "
MODEL-FEATURE(e1,e2)

410	"   This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning , that includes arguments and <e1>defeat rules</e1> that capture <e2>defeasibility</e2> "
MODEL-FEATURE(e1,e2)

411	"   <e1>Spelling-checkers</e1> have become an integral part of most <e2>text processing software</e2> "
PART_WHOLE(e1,e2)

412	"From different reasons among which the speed of processing prevails they are usually based on <e1>dictionaries of word forms</e1> instead of <e2>words</e2> "
COMPARE(e1,e2)

413	"This approach is sufficient for languages with little <e1>inflection</e1> such as <e2>English</e2> , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages "
MODEL-FEATURE(e1,e2)

414	"This approach is sufficient for languages with little inflection such as English , but fails for <e1>highly inflective languages</e1> such as <e2>Czech</e2> , Russian , Slovak or other Slavonic languages "
MODEL-FEATURE(e1,e2)

415	"The speed of the resulting program lies somewhere in the middle of the scale of existing <e1>spelling-checkers</e1> for <e2>English</e2> and the main dictionary fits into the standard 360K floppy , whereas the number of recognized word forms exceeds 6 million (for Czech )"
USAGE(e1,e2)

416	"The speed of the resulting program lies somewhere in the middle of the scale of existing spelling-checkers for English and the main dictionary fits into the standard 360K floppy , whereas the number of recognized <e1>word forms</e1> exceeds 6 million (for <e2>Czech</e2> )"
PART_WHOLE(e1,e2)

417	"To avoid <e1>grammar coverage problems</e1> we use a <e2>fully-connected first-order statistical class grammar</e2> "
USAGE(e2,e1)

418	"The speech-search algorithm is implemented on a <e1>board</e1> with a single <e2>Intel i860 chip</e2> , which provides a factor of 5 speed-up over a SUN 4 for straight C code "
MODEL-FEATURE(e2,e1)

419	"The board plugs directly into the <e1>VME bus</e1> of the <e2>SUN4</e2> , which controls the system and contains the natural language system and application back end "
PART_WHOLE(e1,e2)

420	"First, we present a new paradigm for <e1>speaker-independent &amp;&amp;lpar&amp;&amp;SI&amp;&amp;rpar&amp;&amp; training</e1> of hidden Markov models &amp;&amp;lpar&amp;&amp;HMM&amp;&amp;rpar&amp;&amp; , which uses a large amount of <e2>speech</e2> from a few speakers instead of the traditional practice of using a little speech from many speakers "
USAGE(e2,e1)

421	"In addition, combination of the training speakers is done by averaging the <e1>statistics</e1> of independently trained models rather than the usual pooling of all the <e2>speech data</e2> from many speakers prior to training "
COMPARE(e1,e2)

422	"With only 12 <e1>training speakers</e1> for SI recognition , we achieved a 7.5% <e2>word error rate</e2> on a standard grammar and test set from the DARPA Resource Management corpus "
RESULT(e1,e2)

423	"With only 12 training speakers for SI recognition , we achieved a 7.5% word error rate on a standard grammar and <e1>test set</e1> from the <e2>DARPA Resource Management corpus</e2> "
PART_WHOLE(e1,e2)

424	"Second, we show a significant improvement for <e1>speaker adaptation &amp;&amp;lpar&amp;&amp;SA&amp;&amp;rpar&amp;&amp;</e1> using the new <e2>SI corpus</e2> and a small amount of speech from the new (target) speaker "
USAGE(e2,e1)

425	"A <e1>probabilistic spectral mapping</e1> is estimated independently for each <e2>training &amp;&amp;lpar&amp;&amp;reference&amp;&amp;rpar&amp;&amp; speaker</e2> and the target speaker "
MODEL-FEATURE(e1,e2)

426	"Each <e1>reference model</e1> is transformed to the space of the target speaker and combined by <e2>averaging</e2> "
USAGE(e2,e1)

427	"Using only 40 <e1>utterances</e1> from the target speaker for <e2>adaptation</e2> , the error rate dropped to 4.1% --- a 45% reduction in error compared to the SI result"
USAGE(e1,e2)

428	"   This paper presents a specialized <e1>editor</e1> for a highly structured <e2>dictionary</e2> "
USAGE(e1,e2)

429	"The basic goal in building that <e1>editor</e1> was to provide an adequate tool to help <e2>lexicologists</e2> produce a valid and coherent dictionary on the basis of a linguistic theory "
USAGE(e1,e2)

430	"The basic goal in building that editor was to provide an adequate tool to help lexicologists produce a valid and coherent <e1>dictionary</e1> on the basis of a <e2>linguistic theory</e2> "
USAGE(e2,e1)

431	"If we want valuable lexicons and <e1>grammars</e1> to achieve complex <e2>natural language processing</e2> , we must provide very powerful tools to help create and ensure the validity of such complex linguistic databases "
USAGE(e1,e2)

432	"   The principle known as <e1>free indexation</e1> plays an important role in the determination of the <e2>referential properties of noun phrases</e2> in the principle-and-parameters language framework "
RESULT(e1,e2)

433	"We describe three techniques for making <e1>syntactic analysis</e1> more robust---an <e2>agenda-based scheduling parser</e2> , a recovery technique for failed parses , and a new technique called terminal substring parsing "
USAGE(e2,e1)

434	"For <e1>pragmatics processing</e1> , we describe how the method of <e2>abductive inference</e2> is inherently robust, in that an interpretation is always possible, so that in the absence of the required world knowledge , performance degrades gracefully"
USAGE(e2,e1)

435	"   We present an efficient algorithm for <e1>chart-based phrase structure parsing</e1> of <e2>natural language</e2> that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task"
USAGE(e1,e2)

436	"   We present an efficient algorithm for chart-based phrase structure parsing of natural language that is tailored to the problem of extracting specific information from <e1>unrestricted texts</e1> where many of the <e2>words</e2> are unknown and much of the text is irrelevant to the task"
PART_WHOLE(e2,e1)

437	"As each new edge is added to the chart , the algorithm checks only the topmost of the <e1>edges</e1> adjacent to it, rather than all such <e2>edges</e2> as in conventional treatments"
COMPARE(e1,e2)

438	"This is facilitated through the use of <e1>phrase boundary heuristics</e1> based on the placement of <e2>function words</e2> , and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words "
USAGE(e2,e1)

439	"This is facilitated through the use of phrase boundary heuristics based on the placement of function words , and by heuristic rules that permit certain kinds of <e1>phrases</e1> to be deduced despite the presence of <e2>unknown words</e2> "
PART_WHOLE(e2,e1)

440	"A further reduction in the search space is achieved by using <e1>semantic</e1> rather than <e2>syntactic categories</e2> on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced"
COMPARE(e1,e2)

441	"A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of <e1>ambiguity</e1> and thus the number of <e2>edges</e2> , since only edges with a valid semantic interpretation are ever introduced"
RESULT(e1,e2)

442	"A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only <e1>edges</e1> with a valid <e2>semantic</e2> interpretation are ever introduced"
MODEL-FEATURE(e2,e1)

443	"   In this paper discourse segments are defined and a method for <e1>discourse segmentation</e1> primarily based on <e2>abduction</e2> of temporal relations between segments is proposed"
USAGE(e2,e1)

444	"   In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of <e1>temporal relations</e1> between <e2>segments</e2> is proposed"
MODEL-FEATURE(e1,e2)

445	"   In this paper, a discrimination and robustness oriented <e1>adaptive learning procedure</e1> is proposed to deal with the task of <e2>syntactic ambiguity resolution</e2> "
USAGE(e1,e2)

446	"Owing to the problem of insufficient training data and <e1>approximation error</e1> introduced by the <e2>language model</e2> , traditional statistical approaches , which resolve ambiguities by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications"
RESULT(e2,e1)

447	"Owing to the problem of insufficient training data and approximation error introduced by the language model , traditional <e1>statistical approaches</e1> , which resolve ambiguities by indirectly and implicitly using <e2>maximum likelihood method</e2> , fail to achieve high performance in real applications"
USAGE(e2,e1)

448	"The <e1>accuracy rate</e1> of <e2>syntactic disambiguation</e2> is raised from 46.0% to 60.62% by using this novel approach"
RESULT(e2,e1)

449	"   <e1>Graph unification</e1> remains the most expensive part of <e2>unification-based grammar parsing</e2> "
PART_WHOLE(e1,e2)

450	"We propose a method of attaining such a design through a method of structure-sharing which avoids <e1>log&amp;&amp;lpar&amp;&amp;d&amp;&amp;rpar&amp;&amp; overheads</e1> often associated with <e2>structure-sharing of graphs</e2> without any use of costly dependency pointers "
MODEL-FEATURE(e1,e2)

451	"   The <e1>transfer phase</e1> in machine translation &amp;&amp;lpar&amp;&amp;MT&amp;&amp;rpar&amp;&amp; systems has been considered to be more complicated than <e2>analysis</e2> and generation , since it is inherently a conglomeration of individual lexical rules "
COMPARE(e1,e2)

452	"Currently some attempts are being made to use <e1>case-based reasoning</e1> in <e2>machine translation</e2> , that is, to make decisions on the basis of translation examples at appropriate pints in MT "
USAGE(e1,e2)

453	"This paper proposes a new type of transfer system , called a <e1>Similarity-driven Transfer System &amp;&amp;lpar&amp;&amp;SimTran&amp;&amp;rpar&amp;&amp;</e1> , for use in such <e2>case-based MT &amp;&amp;lpar&amp;&amp;CBMT&amp;&amp;rpar&amp;&amp;</e2> "
USAGE(e1,e2)

454	"When a very noisy portion is detected, the <e1>parser</e1> skips that portion using a fake <e2>non-terminal symbol</e2> "
USAGE(e2,e1)

455	"The unidentified portion is resolved by re-utterance of that portion which is parsed very efficiently by using the <e1>parse record</e1> of the first <e2>utterance</e2> "
MODEL-FEATURE(e1,e2)

456	"Detected <e1>unknown words</e1> can be incrementally incorporated into the <e2>dictionary</e2> after the interaction with the user "
PART_WHOLE(e1,e2)

457	"In this paper, a new mechanism, based on the concept of <e1>sublanguage</e1> , is proposed for identifying <e2>unknown words</e2> , especially personal names , in Chinese newspapers "
USAGE(e1,e2)

458	"In this paper, a new mechanism, based on the concept of sublanguage , is proposed for identifying unknown words , especially <e1>personal names</e1> , in <e2>Chinese newspapers</e2> "
PART_WHOLE(e1,e2)

459	"   This paper describes the understanding process of the <e1>spatial descriptions</e1> in <e2>Japanese</e2> "
PART_WHOLE(e1,e2)

460	"It is done by an experimental computer program SPRINT , which takes natural language texts and produces a <e1>model</e1> of the described <e2>world</e2> "
MODEL-FEATURE(e1,e2)

461	"To reconstruct the model , the authors extract the <e1>qualitative spatial constraints</e1> from the <e2>text</e2> , and represent them as the numerical constraints on the spatial attributes of the entities "
PART_WHOLE(e1,e2)

462	"To reconstruct the model , the authors extract the qualitative spatial constraints from the text , and represent them as the numerical constraints on the <e1>spatial attributes</e1> of the <e2>entities</e2> "
MODEL-FEATURE(e1,e2)

463	"The interpretation reflects the <e1>temporary belief</e1> about the <e2>world</e2> "
MODEL-FEATURE(e1,e2)

464	"   This paper describes a recently collected <e1>spoken language corpus</e1> for the <e2>ATIS &amp;&amp;lpar&amp;&amp;Air Travel Information System&amp;&amp;rpar&amp;&amp; domain</e2> "
MODEL-FEATURE(e2,e1)

465	"We summarize the motivation for this effort, the goals, the implementation of a multi-site data collection paradigm , and the accomplishments of MADCOW in monitoring the collection and distribution of 12,000 <e1>utterances</e1> of <e2>spontaneous speech</e2> from five sites for use in a multi-site common evaluation of speech, natural language and spoken language .   "
MODEL-FEATURE(e2,e1)

466	"   The paper provides an overview of the research conducted at LIMSI in the field of <e1>speech processing</e1> , but also in the related areas of <e2>Human-Machine Communication</e2> , including Natural Language Processing , Non Verbal and Multimodal Communication "
COMPARE(e1,e2)

467	"   This paper describes three relatively <e1>domain-independent capabilities</e1> recently added to the <e2>Paramax spoken language understanding system</e2> : non-monotonic reasoning , implicit reference resolution , and database query paraphrase "
MODEL-FEATURE(e1,e2)

468	"Finally, we briefly describe an experiment which we have done in extending the <e1>n-best speech/language integration architecture</e1> to improving OCR <e2>accuracy</e2> "
RESULT(e1,e2)

469	"   We describe a generative probabilistic model of natural language , which we call HBG , that takes advantage of detailed <e1>linguistic information</e1> to resolve <e2>ambiguity</e2> "
USAGE(e1,e2)

470	" <e1>HBG</e1> incorporates <e2>lexical, syntactic, semantic, and structural information</e2> from the parse tree into the disambiguation process in a novel way"
USAGE(e2,e1)

471	"We use a corpus of bracketed sentences , called a Treebank , in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct <e1>parse</e1> of a <e2>sentence</e2> "
MODEL-FEATURE(e1,e2)

472	"This stands in contrast to the usual approach of further <e1>grammar</e1> tailoring via the usual <e2>linguistic introspection</e2> in the hope of generating the correct parse "
USAGE(e2,e1)

473	"In head-to-head tests against one of the best existing robust probabilistic parsing models , which we call P-CFG, the <e1>HBG model</e1> significantly outperforms <e2>P-CFG</e2> , increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error"
COMPARE(e1,e2)

474	"The classical MLE reestimation algorithms , namely the forward-backward algorithm and the segmental k-means algorithm , are expanded and <e1>reestimation formulas</e1> are given for <e2>HMM with Gaussian mixture observation densities</e2> "
USAGE(e1,e2)

475	"Because of its adaptive nature, <e1>Bayesian learning</e1> serves as a unified approach for the following four <e2>speech recognition</e2> applications, namely parameter smoothing , speaker adaptation , speaker group modeling and corrective training "
USAGE(e1,e2)

476	"   It is well-known that there are polysemous words like <e1>sentence</e1> whose <e2>meaning</e2> or sense depends on the context of use"
MODEL-FEATURE(e2,e1)

477	"We have recently reported on two new <e1>word-sense disambiguation systems</e1> , one trained on <e2>bilingual material</e2> (the Canadian Hansards ) and the other trained on monolingual material ( Roget's Thesaurus and Grolier's Encyclopedia )"
USAGE(e2,e1)

478	"That is, if a polysemous word such as <e1>sentence</e1> appears two or more times in a <e2>well-written discourse</e2> , it is extremely likely that they will all share the same sense "
PART_WHOLE(e1,e2)

479	"This paper describes an experiment which confirmed this hypothesis and found that the tendency to share <e1>sense</e1> in the same <e2>discourse</e2> is extremely strong (98%)"
PART_WHOLE(e1,e2)

480	"This result can be used as an additional source of <e1>constraint</e1> for improving the performance of the <e2>word-sense disambiguation algorithm</e2> "
USAGE(e1,e2)

481	"In addition, it could also be used to help evaluate <e1>disambiguation algorithms</e1> that did not make use of the <e2>discourse constraint</e2> "
USAGE(e2,e1)

482	" In this paper, we explore correlation of <e1>dependency relation paths</e1> to rank candidate answers in <e2>answer extraction</e2>"
USAGE(e1,e2)

483	"Using the correlation measure, we compare <e1>dependency relations</e1> of a candidate answer and mapped <e2>question phrases</e2> in sentence with the corresponding relations in question"
MODEL-FEATURE(e1,e2)

484	"Different from previous studies, we propose an approximate phrase mapping algorithm and incorporate the <e1>mapping score</e1> into the <e2>correlation measure</e2>"
PART_WHOLE(e1,e2)

485	"This paper presents an automatic scheme for collecting statistics on <e1>cooccurrence patterns</e1> in a large <e2>corpus</e2>"
PART_WHOLE(e1,e2)

486	"To a large extent, these statistics reflect <e1>semantic constraints</e1> and thus are used to disambiguate <e2>anaphora references</e2> and syntactic ambiguities"
USAGE(e1,e2)

487	"An experiment was performed to resolve <e1>references</e1> of the <e2>pronoun "it"</e2> in sentences that were randomly selected from the corpus"
MODEL-FEATURE(e1,e2)

488	"An experiment was performed to resolve references of the pronoun "it" in <e1>sentences</e1> that were randomly selected from the <e2>corpus</e2>"
PART_WHOLE(e1,e2)

489	"The results of the experiment show that in most of the cases the <e1>cooccurrence statistics</e1> indeed reflect the semantic constraints and thus provide a basis for a useful <e2>disambiguation tool</e2>.     "
USAGE(e1,e2)

490	" We consider the problem of computing the Kullback-Leibler distance, also called the relative entropy, between a <e1>probabilistic context-free grammar</e1> and a <e2>probabilistic finite automaton</e2>"
COMPARE(e1,e2)

491	"We discuss several applications of the result to the problem of <e1>distributional approximation</e1> of probabilistic context-free grammars by means of <e2>probabilistic finite automata</e2>.     "
USAGE(e2,e1)

492	" Methods developed for <e1>spelling correction</e1> for <e2>languages</e2> like English (see the review by Kukich (Kukich, 1992)) are not readily applicable to agglutinative languages"
USAGE(e1,e2)

493	"This poster presents an approach to <e1>spelling correction</e1> in <e2>agglutinative languages</e2> that is based on two-level morphology and a dynamic-programming based search algorithm"
USAGE(e1,e2)

494	"After an overview of our approach, we present results from experiments with <e1>spelling correction</e1> in <e2>Turkish</e2>.     "
USAGE(e1,e2)

495	" The major objective of this program is to develop and demonstrate robust, high performance <e1>continuous speech recognition &amp;&amp;lpar&amp;&amp;CSR&amp;&amp;rpar&amp;&amp; techniques</e1> focussed on application in <e2>Spoken Language Systems &amp;&amp;lpar&amp;&amp;SLS&amp;&amp;rpar&amp;&amp;</e2> which will enhance the effectiveness of military and civilian computer-based systems"
USAGE(e1,e2)

496	"A key complementary objective is to define and develop applications of robust speech recognition and understanding systems, and to help catalyze the transition of <e1>spoken language technology</e1> into <e2>military and civilian systems</e2>, with particular focus on application of robust CSR to mobile military command and control"
USAGE(e1,e2)

497	"A key complementary objective is to define and develop applications of robust speech recognition and understanding systems, and to help catalyze the transition of spoken language technology into military and civilian systems, with particular focus on application of robust <e1>CSR</e1> to <e2>mobile military command and control</e2>"
USAGE(e1,e2)

498	"The research effort focusses on developing advanced acoustic modelling, rapid search, and recognition-time adaptation techniques for robust <e1>large-vocabulary CSR</e1>, and on applying these techniques to the new <e2>ARPA large-vocabulary CSR corpora</e2> and to military application tasks.     "
USAGE(e1,e2)

499	"<e1>Presentor</e1> offers intuitive and powerful <e2>declarative languages</e2> specifying the presentation at different levels: macro-planning, micro-planning, realization, and formatting"
PART_WHOLE(e2,e1)

500	"Recently considerable progress has been made by a number of groups involved in the <e1>DARPA Spoken Language Systems &amp;&amp;lpar&amp;&amp;SLS&amp;&amp;rpar&amp;&amp; program</e1> to agree on a methodology for comparative evaluation of <e2>SLS systems</e2>, and that methodology has been put into practice several times in comparative tests of several SLS systems"
TOPIC(e1,e2)

501	"These evaluations are probably the only <e1>NL evaluations</e1> other than the series of <e2>Message Understanding Conferences</e2> (Sundheim, 1989; Sundheim, 1991) to have been developed and used by a group of researchers at different sites, although several excellent workshops have been held to study some of these problems (Palmer et al., 1989; Neal et al., 1991)"
TOPIC(e2,e1)

502	"This paper describes a practical <e1>"black-box" methodology</e1> for automatic evaluation of <e2>question-answering NL systems</e2>"
USAGE(e1,e2)

503	" The <e1>psycholinguistic literature</e1> provides evidence for <e2>syntactic priming</e2>, i.e., the tendency to repeat structures"
TOPIC(e1,e2)

504	"This paper describes a method for incorporating <e1>priming</e1> into an <e2>incremental probabilistic parser</e2>"
USAGE(e1,e2)

505	"These models simulate the reading time advantage for <e1>parallel structures</e1> found in <e2>human data</e2>, and also yield a small increase in overall parsing accuracy.     "
PART_WHOLE(e1,e2)

506	" Instances of a <e1>word</e1> drawn from different domains may have different <e2>sense priors</e2> (the proportions of the different senses of a word)"
MODEL-FEATURE(e2,e1)

507	" Instances of a word drawn from different domains may have different sense priors (the proportions of the different <e1>senses</e1> of a <e2>word</e2>)"
MODEL-FEATURE(e1,e2)

508	"This in turn affects the accuracy of <e1>word sense disambiguation &amp;&amp;lpar&amp;&amp;WSD&amp;&amp;rpar&amp;&amp; systems</e1> trained and applied on different <e2>domains</e2>"
USAGE(e1,e2)

509	"This paper presents a method to estimate the <e1>sense priors</e1> of <e2>words</e2> drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations"
MODEL-FEATURE(e1,e2)

510	"This paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using <e1>well calibrated probabilities</e1> when performing these <e2>estimations</e2>"
USAGE(e1,e2)

511	"By using <e1>well calibrated probabilities</e1>, we are able to estimate the <e2>sense priors</e2> effectively to achieve significant improvements in WSD accuracy.     "
USAGE(e1,e2)

512	" How to obtain <e1>hierarchical relations</e1>(e.g. superordinate -hyponym relation, synonym relation) is one of the most important problems for <e2>thesaurus construction</e2>"
PART_WHOLE(e1,e2)

513	"A pilot system for extracting these <e1>relations</e1> automatically from an ordinary <e2>Japanese language dictionary</e2> (Shinmeikai Kokugojiten, published by Sansei-do, in machine readable form) is given"
PART_WHOLE(e1,e2)

514	"The features of the <e1>definition sentences</e1> in the <e2>dictionary</e2>, the mechanical extraction of the hierarchical relations and the estimation of the results are discussed.     "
PART_WHOLE(e1,e2)

515	" The interlingual approach to MT has been repeatedly advocated by researchers originally interested in <e1>natural language understanding</e1> who take <e2>machine translation</e2> to be one possible application"
USAGE(e1,e2)

516	"However, not only the <e1>ambiguity</e1> but also the vagueness which every <e2>natural language</e2> inevitably has leads this approach into essential difficulties"
MODEL-FEATURE(e1,e2)

517	"In contrast, our project, the Mu-project, adopts the <e1>transfer approach</e1> as the basic framework of <e2>MT</e2>"
USAGE(e1,e2)

518	"This paper describes the detailed construction of the <e1>transfer phase</e1> of our system from Japanese to English, and gives some examples of problems which seem difficult to treat in the <e2>interlingual approach</e2>"
COMPARE(e1,e2)

519	" Empirical experience and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers, boosting and <e1>SVMs</e1> are applied to <e2>language processing tasks</e2>, it is possible to achieve high accuracies, but eventually their performances all tend to plateau out at around the same point"
USAGE(e1,e2)

520	"To further improve performance, various <e1>error correction mechanisms</e1> have been developed, but in practice, most of them cannot be relied on to predictably improve performance on <e2>unseen data</e2>; indeed, depending upon the test set, they are as likely to degrade accuracy as to improve it"
USAGE(e1,e2)

521	" The study addresses the problem of automatic acquisition of <e1>entailment relations</e1> between <e2>verbs</e2>"
MODEL-FEATURE(e1,e2)

522	"While this task has much in common with paraphrases acquisition which aims to discover <e1>semantic equivalence</e1> between <e2>verbs</e2>, the main challenge of entailment acquisition is to capture asymmetric, or directional, relations"
MODEL-FEATURE(e1,e2)

523	"While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs, the main challenge of <e1>entailment acquisition</e1> is to capture <e2>asymmetric, or directional, relations</e2>"
TOPIC(e1,e2)

524	"Motivated by the intuition that it often underlies the <e1>local structure</e1> of <e2>coherent text</e2>, we develop a method that discovers verb entailment using evidence about discourse relations between clauses available in a parsed corpus"
MODEL-FEATURE(e1,e2)

525	"Motivated by the intuition that it often underlies the local structure of coherent text, we develop a method that discovers verb entailment using evidence about <e1>discourse relations</e1> between <e2>clauses</e2> available in a parsed corpus"
MODEL-FEATURE(e1,e2)

526	"In comparison with earlier work, the proposed method covers a much wider range of verb entailment types and learns the mapping between <e1>verbs</e1> with highly varied <e2>argument structures</e2>.     "
MODEL-FEATURE(e1,e2)

527	" This paper presents a new approach to statistical sentence generation in which alternative <e1>phrases</e1> are represented as packed sets of <e2>trees</e2>, or forests, and then ranked statistically to choose the best one"
MODEL-FEATURE(e1,e2)

528	"An efficient <e1>ranking algorithm</e1> is described, together with experimental results showing significant improvements over simple enumeration or a <e2>lattice-based approach</e2>.     "
COMPARE(e1,e2)

529	"This paper focuses on the automatic summarization and proposes two different models to extract <e1>sentences</e1> for <e2>summary generation</e2> under two tasks initiated by SUMMAC-1"
USAGE(e1,e2)

530	"For <e1>categorization task</e1>, <e2>positive feature vectors</e2> and negative feature vectors are used cooperatively to construct generic, indicative summaries"
USAGE(e2,e1)

531	"For adhoc task, a <e1>text model</e1> based on relationship between nouns and verbs is used to filter out irrelevant discourse segment, to rank relevant sentences, and to generate the <e2>user-directed summaries</e2>"
USAGE(e1,e2)

532	"A set of articles is represented by a set of word vectors, and the <e1>similarity</e1> between the <e2>vectors</e2> is then calculated"
MODEL-FEATURE(e1,e2)

533	"By applying some constraints on the chronological ordering of articles, an efficient <e1>threading algorithm</e1> that runs in <e2>0&amp;&amp;lpar&amp;&amp;n&amp;&amp;rpar&amp;&amp; time</e2> (where n is the number of articles) is obtained"
MODEL-FEATURE(e1,e2)

534	"The constructed graph is visualized with words that represent the <e1>topics</e1> of the <e2>threads</e2>, and words that represent new information in each article"
MODEL-FEATURE(e1,e2)

535	"The constructed graph is visualized with words that represent the topics of the threads, and <e1>words</e1> that represent new <e2>information</e2> in each article"
MODEL-FEATURE(e1,e2)

536	"In our approach, examples are annotated with the Structured String Tree Correspondence &amp;&amp;lpar&amp;&amp;SSTC&amp;&amp;rpar&amp;&amp; annotation schema where each <e1>SSTC</e1> describes a <e2>sentence</e2>, a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree"
MODEL-FEATURE(e1,e2)

537	"In our approach, examples are annotated with the Structured String Tree Correspondence &amp;&amp;lpar&amp;&amp;SSTC&amp;&amp;rpar&amp;&amp; annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between <e1>substrings</e1> in the <e2>sentence</e2> and subtrees in the representation tree"
PART_WHOLE(e1,e2)

538	"In our approach, examples are annotated with the Structured String Tree Correspondence &amp;&amp;lpar&amp;&amp;SSTC&amp;&amp;rpar&amp;&amp; annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrings in the sentence and <e1>subtrees</e1> in the <e2>representation tree</e2>"
PART_WHOLE(e1,e2)

539	"In the process of parsing, we first try to build <e1>subtrees</e1> for <e2>phrases</e2> in the input sentence which have been successfully found in the example-base - a bottom up approach"
MODEL-FEATURE(e1,e2)

540	" This paper proposes a Hidden Markov Model &amp;&amp;lpar&amp;&amp;HMM&amp;&amp;rpar&amp;&amp; and an <e1>HMM-based chunk tagger</e1>, from which a <e2>named entity &amp;&amp;lpar&amp;&amp;NE&amp;&amp;rpar&amp;&amp; recognition &amp;&amp;lpar&amp;&amp;NER&amp;&amp;rpar&amp;&amp; system</e2> is built to recognize and classify names, times and numerical quantities"
USAGE(e1,e2)

541	"Through the HMM, our system is able to apply and integrate four types of internal and external evidences : 1) simple deterministic internal feature of the <e1>words</e1>, such as <e2>capitalization</e2> and digitalization ; 2) internal semantic feature of important triggers ; 3) internal gazetteer feature; 4) external macro context feature"
MODEL-FEATURE(e2,e1)

542	"Evaluation of our <e1>system</e1> on MUC-6 and MUC-7 English NE tasks achieves <e2>F-measures</e2> of 96.6% and 94.1% respectively"
RESULT(e1,e2)

543	" Porting a <e1>Natural Language Processing &amp;&amp;lpar&amp;&amp;NLP&amp;&amp;rpar&amp;&amp; system</e1> to a <e2>new domain</e2> remains one of the bottlenecks in syntactic parsing, because of the amount of effort required to fix gaps in the lexicon, and to attune the existing grammar to the idiosyncracies of the new sublanguage"
USAGE(e2,e1)

544	" Porting a Natural Language Processing &amp;&amp;lpar&amp;&amp;NLP&amp;&amp;rpar&amp;&amp; system to a new domain remains one of the bottlenecks in syntactic parsing, because of the amount of effort required to fix gaps in the lexicon, and to attune the <e1>existing grammar</e1> to the idiosyncracies of the <e2>new sublanguage</e2>"
USAGE(e1,e2)

545	"This paper shows how the process of fitting a <e1>lexicalized grammar</e1> to a <e2>domain</e2> can be automated to a great extent by using a hybrid system that combines traditional knowledge-based techniques with a corpus-based approach.     "
USAGE(e1,e2)

546	"This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines <e1>traditional knowledge-based techniques</e1> with a <e2>corpus-based approach</e2>.     "
COMPARE(e1,e2)

547	" We propose a detection method for orthographic variants caused by <e1>transliteration</e1> in a large <e2>corpus</e2>"
PART_WHOLE(e1,e2)

548	"One is <e1>string similarity</e1> based on <e2>edit distance</e2>"
USAGE(e2,e1)

549	"The other is <e1>contextual similarity</e1> by a <e2>vector space model</e2>"
USAGE(e2,e1)

550	"However most of the works found in the literature have focused on identifying and understanding <e1>temporal expressions</e1> in <e2>newswire texts</e2>"
PART_WHOLE(e1,e2)

551	"In this paper we report our work on anchoring <e1>temporal expressions</e1> in a novel <e2>genre</e2>, emails"
MODEL-FEATURE(e2,e1)

552	"We have developed and evaluated a <e1>Temporal Expression Anchoror &amp;&amp;lpar&amp;&amp;TEA&amp;&amp;rpar&amp;&amp;</e1>, and the result shows that it performs significantly better than the <e2>baseline</e2>, and compares favorably with some of the closely related work.    "
COMPARE(e1,e2)

553	" The goal of this research is to develop a spoken language system that will demonstrate the usefulness of <e1>voice input</e1> for <e2>interactive problem solving</e2>"
USAGE(e1,e2)

554	"Combining speech recognition and <e1>natural language processing</e1> to achieve <e2>speech understanding</e2>, the system will be demonstrated in an application domain relevant to the DoD"
USAGE(e1,e2)

555	"The objective of this project is to develop a robust and high-performance speech recognition system using a <e1>segment-based approach</e1> to <e2>phonetic recognition</e2>"
USAGE(e1,e2)

556	"The recognition system will eventually be integrated with <e1>natural language processing</e1> to achieve <e2>spoken language understanding</e2>.     "
USAGE(e1,e2)

557	"The <e1>knowledge</e1> to be expressed in <e2>text</e2> is first divided into small propositional units, which are then composed into appropriate combinations and converted into text.KDS &amp;&amp;lpar&amp;&amp;Knowledge Delivery System&amp;&amp;rpar&amp;&amp;, which embodies this paradigm, has distinct parts devoted to creation of the propositional units, to organization of the text, to prevention of excess redundancy, to creation of combinations of units, to evaluation of these combinations as potential sentences, to selection of the best among competing combinations, and to creation of the final text"
PART_WHOLE(e1,e2)

558	"The Fragment-and-Compose paradigm and the <e1>computational methods</e1> of <e2>KDS</e2> are described.     "
USAGE(e1,e2)

559	" <e1>A deterministic parser</e1> is under development which represents a departure from <e2>traditional deterministic parsers</e2> in that it combines both symbolic and connectionist components"
COMPARE(e1,e2)

560	"The connectionist component is trained either from patterns derived from the <e1>rules</e1> of a <e2>deterministic grammar</e2>"
PART_WHOLE(e1,e2)

561	"The development and evolution of such a hybrid architecture has lead to a <e1>parser</e1> which is superior to any <e2>known deterministic parser</e2>"
COMPARE(e1,e2)

562	"Experiments are described and powerful <e1>training techniques</e1> are demonstrated that permit <e2>decision-making</e2> by the connectionist component in the parsing process"
USAGE(e1,e2)

563	"Experiments are described and powerful training techniques are demonstrated that permit decision-making by the <e1>connectionist component</e1> in the <e2>parsing process</e2>"
PART_WHOLE(e1,e2)

564	"This approach has permitted some simplifications to the <e1>rules</e1> of other <e2>deterministic parsers</e2>, including the elimination of rule packets and priorities"
PART_WHOLE(e1,e2)

565	"Data are presented which show how a <e1>connectionist &amp;&amp;lpar&amp;&amp;neural&amp;&amp;rpar&amp;&amp; network</e1> trained with linguistic rules can parse both <e2>expected &amp;&amp;lpar&amp;&amp;grammatical&amp;&amp;rpar&amp;&amp; sentences</e2> as well as some novel (ungrammatical or lexically ambiguous) sentences.     "
USAGE(e1,e2)

566	" The paper proposes and empirically motivates an integration of <e1>supervised learning</e1> with unsupervised learning to deal with human biases in <e2>summarization</e2>"
USAGE(e1,e2)

567	"The <e1>corpus</e1> of human created extracts is created from a <e2>newspaper corpus</e2> and used as a test set"
PART_WHOLE(e2,e1)

568	" In this paper, we describe a search procedure for <e1>statistical machine translation &amp;&amp;lpar&amp;&amp;MT&amp;&amp;rpar&amp;&amp;</e1> based on <e2>dynamic programming &amp;&amp;lpar&amp;&amp;DP&amp;&amp;rpar&amp;&amp;</e2>"
USAGE(e2,e1)

569	"The experimental tests are carried out on the <e1>Verbmobil task</e1> (German-English, 8000-word vocabulary), which is a <e2>limited-domain spoken-language task</e2>.     "
MODEL-FEATURE(e2,e1)

570	" This paper addresses the issue of <e1>word-sense ambiguity</e1> in extraction from <e2>machine-readable resources</e2> for the construction of large-scale knowledge sources"
PART_WHOLE(e1,e2)

571	"These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that <e1>verb semantics</e1> and <e2>syntactic behavior</e2> are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses"
COMPARE(e1,e2)

572	"These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving <e1>semantic information</e1> from <e2>syntactic cues</e2> if we first divide the syntactic cues into distinct groupings that correlate with different word senses"
USAGE(e2,e1)

573	"These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the <e1>syntactic cues</e1> into distinct groupings that correlate with different <e2>word senses</e2>"
COMPARE(e1,e2)

574	"The objective is a generic system of tools, including a core English lexicon, grammar, and concept representations, for building <e1>natural language processing &amp;&amp;lpar&amp;&amp;NLP&amp;&amp;rpar&amp;&amp; systems</e1> for <e2>text understanding</e2>"
USAGE(e1,e2)

575	"<e1>PAKTUS</e1> supports the adaptation of the generic core to a variety of domains: <e2>JINTACCS messages</e2>, RAINFORM messages, news reports about a specific type of event, such as financial transfers or terrorist acts, etc., by acquiring sublanguage and domain-specific grammar, words, conceptual mappings, and discourse patterns"
USAGE(e1,e2)

576	" The multiplicative fragment of <e1>linear logic</e1> has found a number of applications in <e2>computational linguistics</e2>: in the "glue language" approach to LFG semantics, and in the formulation and parsing of various categorial grammars"
USAGE(e1,e2)

577	" The multiplicative fragment of linear logic has found a number of applications in computational linguistics: in the <e1>"glue language"</e1> approach to <e2>LFG semantics</e2>, and in the formulation and parsing of various categorial grammars"
USAGE(e1,e2)

578	" We describe an implementation of data-driven selection of emphatic facial displays for an <e1>embodied conversational agent</e1> in a <e2>dialogue system</e2>"
PART_WHOLE(e1,e2)

579	" We present the first application of the head-driven statistical parsing model of Collins (1999) as a simultaneous language model and <e1>parser</e1> for <e2>large-vocabulary speech recognition</e2>"
USAGE(e1,e2)

580	"The <e1>parser</e1> uses <e2>structural and lexical dependencies</e2> not considered by n-gram models, conditioning recognition on more linguistically-grounded relationships"
USAGE(e2,e1)

581	"Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional <e1>structural information</e1> useful for <e2>speech understanding</e2>.     "
USAGE(e1,e2)

582	"They are probability, <e1>rank</e1>, and <e2>entropy</e2>"
COMPARE(e1,e2)

583	"We evaluated the performance of the three <e1>pruning criteria</e1> in a real application of <e2>Chinese text input</e2> in terms of character error rate &amp;&amp;lpar&amp;&amp;CER&amp;&amp;rpar&amp;&amp;"
USAGE(e1,e2)

584	"Johnston 1998 presents an approach in which strategies for multimodal integration are stated declaratively using a <e1>unification-based grammar</e1> that is used by a <e2>multidimensional chart parser</e2> to compose inputs"
USAGE(e1,e2)

585	"In this paper, we present an alternative approach in which <e1>multimodal parsing and understanding</e1> are achieved using a <e2>weighted finite-state device</e2> which takes speech and gesture streams as inputs and outputs their joint interpretation"
USAGE(e2,e1)

586	" This paper proposes to use a <e1>convolution kernel</e1> over parse trees to model <e2>syntactic structure information</e2> for relation extraction"
MODEL-FEATURE(e1,e2)

587	"Our study reveals that the <e1>syntactic structure features</e1> embedded in a <e2>parse tree</e2> are very effective for relation extraction and these features can be well captured by the convolution tree kernel"
PART_WHOLE(e1,e2)

588	" This paper describes a particular approach to <e1>parsing</e1> that utilizes recent advances in <e2>unification-based parsing</e2> and in classification-based knowledge representation"
USAGE(e2,e1)

589	"As <e1>unification-based grammatical frameworks</e1> are extended to handle richer descriptions of linguistic information, they begin to share many of the properties that have been developed in <e2>KL-ONE-like knowledge representation systems</e2>"
COMPARE(e1,e2)

590	"This commonality suggests that some of the <e1>classification-based representation techniques</e1> can be applied to <e2>unification-based linguistic descriptions</e2>"
USAGE(e1,e2)

591	"The use of a <e1>KL-ONE style representation</e1> for <e2>parsing</e2> and semantic interpretation was first explored in the PSI-KLONE system [2], in which parsing is characterized as an inference process called incremental description refinement.     "
USAGE(e1,e2)

592	"The use of a KL-ONE style representation for parsing and semantic interpretation was first explored in the PSI-KLONE system [2], in which <e1>parsing</e1> is characterized as an inference process called <e2>incremental description refinement</e2>.     "
USAGE(e2,e1)

593	" "To explain complex phenomena, an <e1>explanation system</e1> must be able to select information from a formal representation of <e2>domain knowledge</e2>, organize the selected information into multisentential discourse plans, and realize the discourse plans in text"
USAGE(e2,e1)

594	"This paper reports on a seven-year effort to empirically study <e1>explanation generation</e1> from <e2>semantically rich, large-scale knowledge bases</e2>"
USAGE(e2,e1)

595	"In particular, it describes a <e1>robust explanation system</e1> that constructs multisentential and multi-paragraph explanations from the a <e2>large-scale knowledge base</e2> in the domain of botanical anatomy, physiology, and development"
USAGE(e2,e1)

596	"Given a particular concept, or word sense, a <e1>topic signature</e1> is a set of <e2>words</e2> that tend to co-occur with it"
PART_WHOLE(e2,e1)

597	"<e1>Topic signatures</e1> can be useful in a number of <e2>Natural Language Processing &amp;&amp;lpar&amp;&amp;NLP&amp;&amp;rpar&amp;&amp; applications</e2>, such as Word Sense Disambiguation &amp;&amp;lpar&amp;&amp;WSD&amp;&amp;rpar&amp;&amp; and Text Summarisation"
USAGE(e1,e2)

598	"Our method takes advantage of the different way in which <e1>word senses</e1> are lexicalised in <e2>English</e2> and Chinese, and also exploits the large amount of Chinese text available in corpora and on the Web"
PART_WHOLE(e1,e2)

599	"Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese, and also exploits the large amount of <e1>Chinese text</e1> available in <e2>corpora</e2> and on the Web"
PART_WHOLE(e1,e2)

600	"We evaluated the <e1>topic signatures</e1> on a <e2>WSD task</e2>, where we trained a second-order vector cooccurrence algorithm on standard WSD datasets, with promising results.     "
USAGE(e1,e2)

601	" This paper presents a novel <e1>ensemble learning approach</e1> to resolving <e2>German pronouns</e2>"
USAGE(e1,e2)

602	"Furthermore, we present a <e1>standalone system</e1> that resolves pronouns in <e2>unannotated text</e2> by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process"
USAGE(e1,e2)

603	"Although the system performs well within a limited <e1>textual domain</e1>, further research is needed to make it effective for <e2>open-domain question answering</e2> and text summarisation.     "
COMPARE(e1,e2)

604	" This paper presents a <e1>machine learning approach</e1> to <e2>bare slice disambiguation</e2> in dialogue"
USAGE(e1,e2)

605	"We extract a set of <e1>heuristic principles</e1> from a corpus-based sample and formulate them as <e2>probabilistic Horn clauses</e2>"
MODEL-FEATURE(e2,e1)

606	"We then use the predicates of such clauses to create a set of <e1>domain independent features</e1> to annotate an <e2>input dataset</e2>, and run two different machine learning algorithms : SLIPPER, a rule-based learning algorithm, and TiMBL, a memory-based system"
MODEL-FEATURE(e1,e2)

607	"We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset, and run two different machine learning algorithms : SLIPPER, a <e1>rule-based learning algorithm</e1>, and TiMBL, a <e2>memory-based system</e2>"
COMPARE(e1,e2)

608	"The results show that the <e1>features</e1> in terms of which we formulate our <e2>heuristic principles</e2> have significant predictive power, and that rules that closely resemble our Horn clauses can be learnt automatically from these features.     "
MODEL-FEATURE(e1,e2)

609	"The results show that the features in terms of which we formulate our heuristic principles have significant predictive power, and that <e1>rules</e1> that closely resemble our <e2>Horn clauses</e2> can be learnt automatically from these features.     "
COMPARE(e1,e2)

610	"The new criterion &#8211; <e1>meaning-entailing substitutability</e1> &#8211; fits the needs of <e2>semantic-oriented NLP applications</e2> and can be evaluated directly (independent of an application) at a good level of human agreement"
USAGE(e1,e2)

611	"Motivated by this <e1>semantic criterion</e1> we analyze the empirical quality of <e2>distributional word feature vectors</e2> and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality"
MODEL-FEATURE(e1,e2)

612	"Finally, a novel <e1>feature weighting and selection function</e1> is presented, which yields superior <e2>feature vectors</e2> and better word similarity performance.    "
RESULT(e1,e2)

613	"In this paper, we identify <e1>features</e1> of <e2>electronic discussions</e2> that influence the clustering process, and offer a filtering mechanism that removes undesirable influences"
MODEL-FEATURE(e1,e2)

614	"We tested the <e1>clustering and filtering processes</e1> on <e2>electronic newsgroup discussions</e2>, and evaluated their performance by means of two experiments : coarse-level clustering simple information retrieval.     "
USAGE(e1,e2)

615	"We tested the clustering and filtering processes on electronic newsgroup discussions, and evaluated their <e1>performance</e1> by means of two experiments : <e2>coarse-level clustering</e2> simple information retrieval.     "
RESULT(e2,e1)

616	" We present a new HMM tagger that exploits <e1>context</e1> on both sides of a <e2>word</e2> to be tagged, and evaluate it in both the unsupervised and supervised case"
MODEL-FEATURE(e1,e2)

617	"Observing that the <e1>quality</e1> of the lexicon greatly impacts the <e2>accuracy</e2> that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable"
RESULT(e1,e2)

618	"Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of <e1>HMM training</e1> that improves <e2>accuracy</e2> when training of lexical probabilities is unstable"
RESULT(e1,e2)

619	" Past work of generating <e1>referring expressions</e1> mainly utilized attributes of <e2>objects</e2> and binary relations between objects"
USAGE(e1,e2)

620	" Past work of generating referring expressions mainly utilized attributes of objects and <e1>binary relations</e1> between <e2>objects</e2>"
MODEL-FEATURE(e1,e2)

621	"To overcome this limitation, this paper proposes a method utilizing the perceptual groups of <e1>objects</e1> and <e2>n-ary relations</e2> among them"
MODEL-FEATURE(e2,e1)

622	" <e1>Machine transliteration/back-transliteration</e1> plays an important role in many <e2>multilingual speech and language applications</e2>"
PART_WHOLE(e1,e2)

623	"In this paper, a novel framework for <e1>machine transliteration/backtransliteration</e1> that allows us to carry out <e2>direct orthographical mapping &amp;&amp;lpar&amp;&amp;DOM&amp;&amp;rpar&amp;&amp;</e2> between two different languages is presented"
USAGE(e1,e2)

624	"Under this framework, a joint source-channel transliteration model, also called <e1>n-gram transliteration model &amp;&amp;lpar&amp;&amp;n-gram TM&amp;&amp;rpar&amp;&amp;</e1>, is further proposed to model the <e2>transliteration process</e2>"
MODEL-FEATURE(e1,e2)

625	"We evaluate the proposed methods through several <e1>transliteration/backtransliteration experiments</e1> for <e2>English/Chinese and English/Japanese language pairs</e2>"
USAGE(e1,e2)

626	" In this paper, we present a <e1>corpus-based supervised word sense disambiguation &amp;&amp;lpar&amp;&amp;WSD&amp;&amp;rpar&amp;&amp; system</e1> for <e2>Dutch</e2> which combines statistical classification (maximum entropy) with linguistic information"
USAGE(e1,e2)

627	"Instead of building individual <e1>classifiers</e1> per ambiguous wordform, we introduce a <e2>lemma-based approach</e2>"
COMPARE(e1,e2)

628	"The advantage of this novel method is that it clusters all <e1>inflected forms</e1> of an <e2>ambiguous word</e2> in one classifier, therefore augmenting the training material available to the algorithm"
MODEL-FEATURE(e1,e2)

629	"The advantage of this novel method is that it clusters all inflected forms of an ambiguous word in one classifier, therefore augmenting the <e1>training material</e1> available to the <e2>algorithm</e2>"
USAGE(e1,e2)

630	"Testing the <e1>lemma-based model</e1> on the <e2>Dutch Senseval-2 test data</e2>, we achieve a significant increase in accuracy over the wordform model"
USAGE(e1,e2)

631	" We present a <e1>text mining method</e1> for finding synonymous expressions based on the <e2>distributional hypothesis</e2> in a set of coherent corpora"
USAGE(e2,e1)

632	"This paper proposes a new methodology to improve the <e1>accuracy</e1> of a <e2>term aggregation system</e2> using each author's text as a coherent corpus"
RESULT(e2,e1)

633	"This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author's <e1>text</e1> as a coherent <e2>corpus</e2>"
USAGE(e1,e2)

634	"Our approach is based on the idea that one person tends to use one <e1>expression</e1> for one <e2>meaning</e2>"
MODEL-FEATURE(e2,e1)

635	"According to our assumption, most of the <e1>words</e1> with <e2>similar context features</e2> in each author's corpus tend not to be synonymous expressions"
MODEL-FEATURE(e2,e1)

636	"Our proposed method improves the <e1>accuracy</e1> of our <e2>term aggregation system</e2>, showing that our approach is successful.     "
RESULT(e2,e1)

637	" While <e1>sentence extraction</e1> as an approach to <e2>summarization</e2> has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent"
USAGE(e1,e2)

638	" While sentence extraction as an approach to summarization has been shown to work in <e1>documents</e1> of certain <e2>genres</e2>, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent"
MODEL-FEATURE(e2,e1)

639	" While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of <e1>email communication</e1> where <e2>utterances</e2> are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent"
PART_WHOLE(e1,e2)

640	" While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary <e1>segments</e1> of <e2>dialogue</e2> that would make a summary coherent"
PART_WHOLE(e1,e2)

641	"In this paper, we present our work on the detection of <e1>question-answer pairs</e1> in an <e2>email conversation</e2> for the task of email summarization"
PART_WHOLE(e2,e1)

642	"We show that various <e1>features</e1> based on the structure of email-threads can be used to improve upon <e2>lexical similarity</e2> of discourse segments for question-answer pairing.     "
USAGE(e1,e2)

643	"The framework is composed of a novel algorithm to efficiently compute the <e1>co-occurrence distribution</e1> between pairs of <e2>terms</e2>, an independence model, and a parametric affinity model"
MODEL-FEATURE(e1,e2)

644	"In comparison with previous models, which either use arbitrary windows to compute <e1>similarity</e1> between <e2>words</e2> or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus"
MODEL-FEATURE(e1,e2)

645	"In comparison with previous models, which either use arbitrary windows to compute similarity between words or use <e1>lexical affinity</e1> to create <e2>sequential models</e2>, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus"
USAGE(e1,e2)

646	"In comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the <e1>co-occurrence patterns</e1> of any pair of <e2>words</e2> or phrases at any distance in the corpus"
MODEL-FEATURE(e1,e2)

647	" The paper presents a method for <e1>word sense disambiguation</e1> based on <e2>parallel corpora</e2>"
USAGE(e2,e1)

648	"The method exploits recent advances in word alignment and <e1>word clustering</e1> based on <e2>automatic extraction</e2> of translation equivalents and being supported by available aligned wordnets for the languages in the corpus"
USAGE(e2,e1)

649	"The same system used in a validation mode, can be used to check and spot <e1>alignment errors</e1> in <e2>multilingually aligned wordnets</e2> as BalkaNet and EuroWordNet.     "
PART_WHOLE(e1,e2)

650	" We present <e1>Minimum Bayes-Risk &amp;&amp;lpar&amp;&amp;MBR&amp;&amp;rpar&amp;&amp; decoding</e1> for <e2>statistical machine translation</e2>"
USAGE(e1,e2)

651	"This statistical approach aims to minimize expected loss of translation errors under <e1>loss functions</e1> that measure <e2>translation performance</e2>"
MODEL-FEATURE(e1,e2)

652	"We describe a hierarchy of <e1>loss functions</e1> that incorporate different levels of <e2>linguistic information</e2> from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences"
MODEL-FEATURE(e1,e2)

653	"We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, <e1>word-to-word alignments</e1> from an <e2>MT system</e2>, and syntactic structure from parse-trees of source and target language sentences"
PART_WHOLE(e2,e1)

654	"We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from <e1>parse-trees</e1> of <e2>source and target language sentences</e2>"
MODEL-FEATURE(e1,e2)

655	"We report the <e1>performance</e1> of the <e2>MBR decoders</e2> on a Chinese-to-English translation task"
RESULT(e1,e2)

656	"Our results show that <e1>MBR decoding</e1> can be used to tune <e2>statistical MT performance</e2> for specific loss functions.     "
USAGE(e1,e2)

657	" <e1>Information extraction techniques</e1> automatically create <e2>structured databases</e2> from unstructured data sources, such as the Web or newswire documents"
RESULT(e1,e2)

658	"For many reasons, it is highly desirable to accurately estimate the <e1>confidence</e1> the system has in the correctness of each <e2>extracted field</e2>"
MODEL-FEATURE(e1,e2)

659	"The <e1>information extraction system</e1> we evaluate is based on a <e2>linear-chain conditional random field &amp;&amp;lpar&amp;&amp;CRF&amp;&amp;rpar&amp;&amp;</e2>, a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a  Markov model"
USAGE(e2,e1)

660	"The information extraction system we evaluate is based on a linear-chain conditional random field &amp;&amp;lpar&amp;&amp;CRF&amp;&amp;rpar&amp;&amp;, a <e1>probabilistic model</e1> which has performed well on <e2>information extraction tasks</e2> because of its ability to capture arbitrary, overlapping features of the input in a  Markov model"
USAGE(e1,e2)

661	"The information extraction system we evaluate is based on a linear-chain conditional random field &amp;&amp;lpar&amp;&amp;CRF&amp;&amp;rpar&amp;&amp;, a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping <e1>features</e1> of the input in a <e2> Markov model</e2>"
MODEL-FEATURE(e2,e1)

662	"We implement several techniques to estimate the <e1>confidence</e1> of both <e2>extracted fields</e2> and entire multi-field records, obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records.     "
MODEL-FEATURE(e1,e2)

663	"It then presents an implemented <e1>graphic interpretation system</e1> that takes into account a variety of <e2>communicative signals</e2>, and an evaluation study showing that evidence obtained from shallow processing of the graphic's caption has a significant impact on the system's success"
USAGE(e2,e1)

664	" We present a framework for <e1>word alignment</e1> based on <e2>log-linear models</e2>"
USAGE(e2,e1)

665	"All <e1>knowledge sources</e1> are treated as <e2>feature functions</e2>, which depend on the source langauge sentence, the target language sentence and possible additional variables"
USAGE(e1,e2)

666	"<e1>Log-linear models</e1> allow <e2>statistical alignment models</e2> to be easily extended by incorporating syntactic information"
USAGE(e1,e2)

667	"In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and <e1>bilingual dictionary coverage</e1> as <e2>features</e2>"
USAGE(e1,e2)

668	"Our experiments show that <e1>log-linear models</e1> significantly outperform <e2>IBM translation models</e2>.     "
COMPARE(e1,e2)

669	" This paper presents the results of automatically inducing a <e1>Combinatory Categorial Grammar &amp;&amp;lpar&amp;&amp;CCG&amp;&amp;rpar&amp;&amp; lexicon</e1> from a <e2>Turkish dependency treebank</e2>"
USAGE(e2,e1)

670	"The fact that <e1>Turkish</e1> is an <e2>agglutinating free word order language</e2> presents a challenge for language theories"
MODEL-FEATURE(e2,e1)

671	"We explored possible ways to obtain a compact lexicon, consistent with CCG principles, from a <e1>treebank</e1> which is an order of magnitude smaller than <e2>Penn WSJ</e2>.     "
COMPARE(e1,e2)

672	" In the <e1>Chinese language</e1>, a <e2>verb</e2> may have its dependents on its left, right or on both sides"
PART_WHOLE(e2,e1)

673	"The <e1>ambiguity resolution</e1> of right-side dependencies is essential for <e2>dependency parsing</e2> of sentences with two or more verbs"
PART_WHOLE(e1,e2)

674	"The ambiguity resolution of right-side dependencies is essential for dependency parsing of <e1>sentences</e1> with two or more <e2>verbs</e2>"
PART_WHOLE(e2,e1)

675	"Previous works on shift-reduce dependency parsers may not guarantee the <e1>connectivity</e1> of a <e2>dependency tree</e2> due to their weakness at resolving the right-side dependencies"
MODEL-FEATURE(e1,e2)

676	"This paper proposes a <e1>two-phase shift-reduce dependency parser</e1> based on <e2>SVM learning</e2>"
USAGE(e2,e1)

677	"In experimental evaluation, our proposed method outperforms previous <e1>shift-reduce dependency parsers</e1> for the <e2>Chine language</e2>, showing improvement of dependency accuracy by 10.08%.     "
USAGE(e1,e2)

678	" We present an operable definition of <e1>focus</e1> which is argued to be of a cognito-pragmatic nature and explore how it is determined in <e2>discourse</e2> in a formalized manner"
PART_WHOLE(e1,e2)

679	"Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of <e1>focus</e1> via FDA as a discourse-level construct into <e2>speech synthesis systems</e2>, in particular, concept-to-speech systems, is also briefly discussed.     "
USAGE(e1,e2)

680	" Currently several <e1>grammatical formalisms</e1> converge towards being declarative and towards utilizing <e2>context-free phrase-structure grammar</e2> as a backbone, e.g. LFG and PATR-II"
USAGE(e2,e1)

681	"The aim of this paper is to provide a survey and a practical comparison of fundamental <e1>rule-invocation strategies</e1> within <e2>context-free chart parsing</e2>.     "
PART_WHOLE(e1,e2)

682	" In this paper I will argue for a <e1>model of grammatical processing</e1> that is based on <e2>uniform processing</e2> and knowledge sources"
USAGE(e2,e1)

683	"The main feature of this model is to view <e1>parsing</e1> and <e2>generation</e2> as two strongly interleaved tasks performed by a single parametrized deduction process"
COMPARE(e1,e2)

684	" One of the major problems one is faced with when decomposing <e1>words</e1> into their <e2>constituent parts</e2> is ambiguity: the generation of multiple analyses for one input word, many of which are implausible"
PART_WHOLE(e2,e1)

685	" One of the major problems one is faced with when decomposing words into their constituent parts is ambiguity: the generation of multiple <e1>analyses</e1> for one <e2>input word</e2>, many of which are implausible"
MODEL-FEATURE(e1,e2)

686	"In order to deal with ambiguity, the <e1>MORphological PArser MORPA</e1> is provided with a <e2>probabilistic context-free grammar &amp;&amp;lpar&amp;&amp;PCFG&amp;&amp;rpar&amp;&amp;</e2>, i.e. it combines a "conventional" context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse"
PART_WHOLE(e2,e1)

687	"In order to deal with ambiguity, the MORphological PArser MORPA is provided with a probabilistic context-free grammar &amp;&amp;lpar&amp;&amp;PCFG&amp;&amp;rpar&amp;&amp;, i.e. it combines a <e1>"conventional" context-free morphological grammar</e1> to filter out ungrammatical segmentations with a <e2>probability-based scoring function</e2> which determines the likelihood of each successful parse"
USAGE(e2,e1)

688	"MORPA is a fully implemented <e1>parser</e1> developed for use in a <e2>text-to-speech conversion system</e2>.     "
USAGE(e1,e2)

689	"The <e1>output</e1> can be customized to meet different <e2>segmentation standards</e2> through the application of an ordered list of transformation"
MODEL-FEATURE(e2,e1)

690	"The <e1>system</e1> participated in all the tracks of the segmentation bakeoff -- PK-open, PK-closed, AS-open, AS-closed, HK-open, HK-closed, MSR-open and MSR- closed -- and achieved the <e2>state-of-the-art performance</e2> in MSR-open, MSR-close and PK-open tracks"
RESULT(e1,e2)

691	" In this paper a <e1>morphological component</e1> with a limited capability to automatically interpret (and generate) <e2>derived words</e2> is presented"
USAGE(e1,e2)

692	"The system combines an extended two-level morphology [Trost, 1991a; Trost, 1991b] with a <e1>feature-based word grammar</e1> building on a <e2>hierarchical lexicon</e2>"
USAGE(e2,e1)

693	"<e1>Polymorphemic stems</e1> not explicitly stored in the lexicon are given a <e2>compositional interpretation</e2>"
MODEL-FEATURE(e2,e1)

694	" This paper proposes an approach to <e1>full parsing</e1> suitable for <e2>Information Extraction</e2> from texts"
USAGE(e1,e2)

695	"Sequences of cascades of rules deterministically analyze the <e1>text</e1>, building <e2>unambiguous structures</e2>"
MODEL-FEATURE(e2,e1)

696	"It was implemented in the <e1>IE module</e1> of <e2>FACILE, a EU project for multilingual text classification and IE</e2>.     "
PART_WHOLE(e1,e2)

697	"A very simple improved <e1>duration model</e1> has reduced the <e2>error rate</e2> by about 10% in both triphone and semiphone systems"
RESULT(e1,e2)

698	"Finally, the <e1>recognizer</e1> has been modified to use <e2>bigram back-off language models</e2>"
USAGE(e2,e1)

699	"There are four <e1>language pairs</e1> currently supported by <e2>GLOSSER</e2>: English-Bulgarian, English-Estonian, English-Hungarian and French-Dutch"
MODEL-FEATURE(e1,e2)

700	"A demonstration (in UNIX) for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis &amp;&amp;lpar&amp;&amp;ICALL&amp;&amp;rpar&amp;&amp;, including disambiguated morphological analysis and lemmatized indexing for an <e1>aligned bilingual corpus</e1> of <e2>word examples</e2>"
PART_WHOLE(e2,e1)

701	" This paper addresses the problem of identifying likely <e1>topics</e1> of <e2>texts</e2> by their position in the text"
MODEL-FEATURE(e1,e2)

702	"It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on <e1>genre-specific regularities</e1> of <e2>discourse structure</e2>"
MODEL-FEATURE(e1,e2)

703	"We make use of a <e1>conditional log-linear model</e1>, with <e2>hidden variables</e2> representing the assignment of lexical items to word clusters or word senses"
MODEL-FEATURE(e2,e1)

704	"We make use of a conditional log-linear model, with hidden variables representing the assignment of <e1>lexical items</e1> to <e2>word clusters</e2> or word senses"
MODEL-FEATURE(e2,e1)

705	"The model learns to automatically make these <e1>assignments</e1> based on a <e2>discriminative training criterion</e2>"
USAGE(e2,e1)

706	"Training and <e1>decoding</e1> with the model requires summing over an exponential number of hidden-variable assignments: the required summations can be computed efficiently and exactly using <e2>dynamic programming</e2>"
USAGE(e2,e1)

707	"The model gives an F-measure improvement of ~1.25% beyond the <e1>base parser</e1>, and an ~0.25% improvement beyond <e2>Collins &amp;&amp;lpar&amp;&amp;2000&amp;&amp;rpar&amp;&amp; reranker</e2>"
COMPARE(e1,e2)

708	" <e1>Taiwan Child Language Corpus</e1> contains <e2>scripts</e2> transcribed from about 330 hours of recordings of fourteen young children from Southern Min Chinese speaking families in Taiwan"
PART_WHOLE(e2,e1)

709	" Taiwan Child Language Corpus contains scripts transcribed from about 330 hours of <e1>recordings</e1> of fourteen young children from <e2>Southern Min Chinese</e2> speaking families in Taiwan"
MODEL-FEATURE(e2,e1)

710	"The format of the <e1>corpus</e1> adopts the <e2>Child Language Data Exchange System &amp;&amp;lpar&amp;&amp;CHILDES&amp;&amp;rpar&amp;&amp;</e2>"
MODEL-FEATURE(e2,e1)

711	"The size of the <e1>corpus</e1> is about 1.6 million <e2>words</e2>"
PART_WHOLE(e2,e1)

712	"In this paper, we describe data collection, transcription, word segmentation, and <e1>part-of-speech annotation</e1> of this <e2>corpus</e2>"
USAGE(e1,e2)

713	" Robust <e1>natural language interpretation</e1> requires strong <e2>semantic domain models</e2>, fail-soft recovery heuristics, and very flexible control structures"
USAGE(e2,e1)

714	"Although <e1>single-strategy parsers</e1> have met with a measure of success, a <e2>multi-strategy approach</e2> is shown to provide a much higher degree of flexibility, redundancy, and ability to bring task-specific domain knowledge (in addition to general linguistic knowledge) to bear on both grammatical and ungrammatical input"
COMPARE(e1,e2)

715	"Although single-strategy parsers have met with a measure of success, a multi-strategy approach is shown to provide a much higher degree of flexibility, redundancy, and ability to bring <e1>task-specific domain knowledge</e1> (in addition to <e2>general linguistic knowledge</e2>) to bear on both grammatical and ungrammatical input"
COMPARE(e1,e2)

716	"A <e1>parsing algorithm</e1> is presented that integrates several different <e2>parsing strategies</e2>, with case-frame instantiation dominating"
USAGE(e2,e1)

717	"Each of these <e1>parsing strategies</e1> exploits different <e2>types of knowledge</e2>; and their combination provides a strong framework in which to process conjunctions, fragmentary input, and ungrammatical structures, as well as less exotic, grammatically correct input"
USAGE(e2,e1)

718	"Each of these parsing strategies exploits different types of knowledge; and their combination provides a strong framework in which to process conjunctions, fragmentary input, and <e1>ungrammatical structures</e1>, as well as less exotic, <e2>grammatically correct input</e2>"
COMPARE(e1,e2)

719	"Several <e1>specific heuristics</e1> for handling ungrammatical input are presented within this <e2>multi-strategy framework</e2>"
PART_WHOLE(e1,e2)

720	" By generalizing the notion of location of a constituent to allow discontinuous locations, one can describe the <e1>discontinuous constituents</e1> of <e2>non-configurational languages</e2>"
PART_WHOLE(e1,e2)

721	"These <e1>discontinuous constituents</e1> can be described by a variant of <e2>definite clause grammars</e2>, and these grammars can be used in conjunction with a proof procedure to create a parser for non-configurational languages"
MODEL-FEATURE(e2,e1)

722	"These discontinuous constituents can be described by a variant of definite clause grammars, and these <e1>grammars</e1> can be used in conjunction with a proof procedure to create a <e2>parser for non-configurational languages</e2>"
USAGE(e1,e2)

723	"  A system is described for acquiring a <e1>context-sensitive, phrase structure grammar</e1> which is applied by a <e2>best-path, bottom-up, deterministic parser</e2>"
USAGE(e1,e2)

724	"Overall, this research concludes that <e1>CSG</e1> is a computationally and conceptually tractable approach to the construction of <e2>phrase structure grammar</e2> for news story text"
USAGE(e1,e2)

725	" We present a corpus-based study of methods that have been proposed in the linguistics literature for selecting the <e1>semantically unmarked term</e1> out of a pair of <e2>antonymous adjectives</e2>"
PART_WHOLE(e1,e2)

726	"In the paper we propose a black-box method for comparing the <e1>lexical coverage</e1> of <e2>MT systems</e2>"
MODEL-FEATURE(e1,e2)

727	"The method is based on lists of <e1>words</e1> from different <e2>frequency classes</e2>"
MODEL-FEATURE(e2,e1)

728	" We describe a method for interpreting <e1>abstract flat syntactic representations, LFG f-structures</e1>, as <e2>underspecified semantic representations, here Underspecified Discourse Representation Structures &amp;&amp;lpar&amp;&amp;UDRSs&amp;&amp;rpar&amp;&amp;</e2>"
MODEL-FEATURE(e2,e1)

729	"It provides a <e1>model theoretic interpretation</e1> and an inferential component which operates directly on underspecified representations for <e2>f-structures</e2> through the translation images of f-structures as UDRSs"
MODEL-FEATURE(e1,e2)

730	"It provides a model theoretic interpretation and an inferential component which operates directly on underspecified representations for f-structures through the <e1>translation images</e1> of <e2>f-structures</e2> as UDRSs"
MODEL-FEATURE(e1,e2)

731	" In this paper we describe a systematic approach for creating a <e1>dialog management system</e1> based on a <e2>Construct Algebra</e2>, a collection of relations and operations on a task representation"
USAGE(e2,e1)

732	" In this paper we describe a systematic approach for creating a dialog management system based on a Construct Algebra, a <e1>collection of relations and operations</e1> on a <e2>task representation</e2>"
MODEL-FEATURE(e1,e2)

733	"These relations and operations are <e1>analytical components</e1> for building higher level abstractions called <e2>dialog motivators</e2>"
PART_WHOLE(e1,e2)

734	"The <e1>dialog manager</e1>, consisting of a <e2>collection of dialog motivators</e2>, is entirely built using the Construct Algebra"
PART_WHOLE(e2,e1)

735	" STRAND (Resnik, 1998) is a <e1>language-independent system</e1> for <e2>automatic discovery of text</e2> in parallel translation on the World Wide Web"
USAGE(e1,e2)

736	"This paper extends the preliminary <e1>STRAND</e1> results by adding <e2>automatic language identification</e2>, scaling up by orders of magnitude, and formally evaluating performance"
PART_WHOLE(e2,e1)

737	"The most recent end-product is an <e1>automatically acquired parallel corpus</e1> comprising 2491 <e2>English-French document pairs</e2>, approximately 1.5 million words per language"
PART_WHOLE(e2,e1)

738	"The main of this project is <e1>computer-assisted acquisition and morpho-syntactic description of verb-noun collocations</e1> in <e2>Polish</e2>"
USAGE(e1,e2)

739	"The presented here <e1>corpus-based approach</e1> permitted us to triple the size the <e2>verb-noun collocation dictionary for Polish</e2>"
USAGE(e1,e2)

740	" In this paper we deal with a recently developed <e1>large Czech MWE database</e1> containing at the moment 160 000 <e2>MWEs</e2> (treated as lexical units)"
PART_WHOLE(e2,e1)

741	"It was compiled from various resources such as encyclopedias and dictionaries, public <e1>databases</e1> of <e2>proper names</e2> and toponyms, collocations obtained from Czech WordNet, lists of botanical and zoological terms and others"
PART_WHOLE(e2,e1)

742	"It was compiled from various resources such as encyclopedias and dictionaries, public databases of proper names and toponyms, <e1>collocations</e1> obtained from <e2>Czech WordNet</e2>, lists of botanical and zoological terms and others"
PART_WHOLE(e1,e2)

743	"We compare the built <e1>MWEs database</e1> with the <e2>corpus data</e2> from Czech National Corpus (approx"
COMPARE(e1,e2)

744	"To obtain a more complete list of MWEs we propose and use a technique exploiting the <e1>Word Sketch Engine</e1>, which allows us to work with <e2>statistical parameters</e2> such as frequency of MWEs and their components as well as with the salience for the whole MWEs"
MODEL-FEATURE(e2,e1)

745	"To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine, which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the <e1>salience</e1> for the whole <e2>MWEs</e2>"
MODEL-FEATURE(e1,e2)

746	"We also discuss exploitation of the <e1>database</e1> for working out a more adequate <e2>tagging</e2> and lemmatization"
USAGE(e1,e2)

747	"The final goal is to be able to recognize <e1>MWEs</e1> in <e2>corpus text</e2> and lemmatize them as complete lexical units, i"
PART_WHOLE(e1,e2)

748	" We describe a set of experiments to explore <e1>statistical techniques</e1> for ranking and selecting the best <e2>translations</e2> in a graph of translation hypotheses"
USAGE(e1,e2)

749	" We describe a set of experiments to explore statistical techniques for ranking and selecting the best translations in a <e1>graph</e1> of <e2>translation hypotheses</e2>"
PART_WHOLE(e2,e1)

750	"In a previous paper (Carl, 2007) we have described how the <e1>hypotheses graph</e1> is generated through <e2>shallow mapping</e2> and permutation rules"
USAGE(e2,e1)

751	"We have given examples of its <e1>nodes</e1> consisting of <e2>vectors representing morpho-syntactic properties</e2> of words and phrases"
PART_WHOLE(e2,e1)

752	"The feature functions are trained off-line on different types of text and their <e1>log-linear combination</e1> is then used to retrieve the best M <e2>translation paths</e2> in the graph"
USAGE(e1,e2)

753	"We compare two language modelling toolkits, the <e1>CMU</e1> and the <e2>SRI toolkit</e2> and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.    "
COMPARE(e1,e2)

754	"We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) <e1>word-lemma based feature function models</e1> produce better results than <e2>token-based models</e2>, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.    "
COMPARE(e1,e2)

755	"We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a <e1>PoS-tag feature function</e1> to the <e2>word-lemma model</e2> improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.    "
PART_WHOLE(e1,e2)

756	"We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) <e1>weights</e1> for <e2>lexical translations</e2> are suitable if the training material is similar to the texts to be translated.    "
MODEL-FEATURE(e1,e2)

757	"We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the <e1>training material</e1> is similar to the <e2>texts</e2> to be translated.    "
COMPARE(e1,e2)

758	" Existing techniques extract term candidates by looking for <e1>internal and contextual information</e1> associated with <e2>domain specific terms</e2>"
MODEL-FEATURE(e1,e2)

759	"The algorithms always face the dilemma that fewer <e1>features</e1> are not enough to distinguish <e2>terms</e2> from non-terms whereas more features lead to more conflicts among selected features"
MODEL-FEATURE(e1,e2)

760	"This paper presents a novel approach for <e1>term extraction</e1> based on <e2>delimiters</e2> which are much more stable and domain independent"
USAGE(e2,e1)

761	" The present paper reports on a preparatory research for building a language corpus annotation scenario capturing the <e1>discourse relations</e1> in <e2>Czech</e2>"
PART_WHOLE(e1,e2)

762	"We primarily focus on the description of the <e1>syntactically motivated relations</e1> in <e2>discourse</e2>, basing our findings on the theoretical background of the Prague Dependency Treebank 2.0 and the Penn Discourse Treebank 2"
PART_WHOLE(e1,e2)

763	"Our aim is to revisit the present-day <e1>syntactico-semantic &amp;&amp;lpar&amp;&amp;tectogrammatical&amp;&amp;rpar&amp;&amp; annotation</e1> in the <e2>Prague Dependency Treebank</e2>, extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new, discourse level of annotation"
PART_WHOLE(e1,e2)

764	"Our aim is to revisit the present-day syntactico-semantic &amp;&amp;lpar&amp;&amp;tectogrammatical&amp;&amp;rpar&amp;&amp; annotation in the Prague Dependency Treebank, extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new, <e1>discourse level</e1> of <e2>annotation</e2>"
MODEL-FEATURE(e1,e2)

765	"In this paper, we propose a feasible process of such a transfer, comparing the possibilities the <e1>Praguian dependency-based approach</e1> offers with the <e2>Penn discourse annotation</e2> based primarily on the analysis and classification of discourse connectives.     "
COMPARE(e1,e2)

766	" In this paper, we reported experiments of <e1>unsupervised automatic acquisition</e1> of Italian and English verb subcategorization frames &amp;&amp;lpar&amp;&amp;SCFs&amp;&amp;rpar&amp;&amp; from <e2>general and domain corpora</e2>"
USAGE(e2,e1)

767	"The proposed technique operates on syntactically shallow-parsed corpora on the basis of a limited number of search heuristics not relying on any previous <e1>lexico-syntactic knowledge</e1> about <e2>SCFs</e2>"
MODEL-FEATURE(e1,e2)

768	"The issue of whether <e1>verbs</e1> sharing similar <e2>SCFs distributions</e2> happen to share similar semantic properties as well was also explored by clustering verbs that share frames with the same distribution using the Minimum Description Length Principle &amp;&amp;lpar&amp;&amp;MDL&amp;&amp;rpar&amp;&amp;"
MODEL-FEATURE(e2,e1)

769	"The issue of whether verbs sharing similar SCFs distributions happen to share similar semantic properties as well was also explored by clustering <e1>verbs</e1> that share <e2>frames</e2> with the same distribution using the Minimum Description Length Principle &amp;&amp;lpar&amp;&amp;MDL&amp;&amp;rpar&amp;&amp;"
MODEL-FEATURE(e2,e1)

770	" The <e1>translation</e1> of English text into American Sign Language &amp;&amp;lpar&amp;&amp;ASL&amp;&amp;rpar&amp;&amp; animation tests the limits of <e2>traditional MT architectural designs</e2>"
USAGE(e2,e1)

771	"A new <e1>semantic representation</e1> is proposed that uses <e2>virtual reality 3D scene modeling software</e2> to produce spatially complex ASL phenomena called "classifier predicates." The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates transfer and direct approaches into a single system.     "
USAGE(e1,e2)

772	"A new semantic representation is proposed that uses virtual reality 3D scene modeling software to produce spatially complex ASL phenomena called "classifier predicates." The model acts as an <e1>interlingua</e1> within a new <e2>multi-pathway MT architecture design</e2> that also incorporates transfer and direct approaches into a single system.     "
PART_WHOLE(e1,e2)

773	" In our current research into the design of <e1>cognitively well-motivated interfaces</e1> relying primarily on the <e2>display of graphical information</e2>, we have observed that graphical information alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users' expectations"
USAGE(e2,e1)

774	"To solve this problem, we are working towards the integration of <e1>natural language generation</e1> to augment the <e2>interaction</e2>    "
USAGE(e1,e2)

775	"For both corpora word recognition experiments were carried out with <e1>vocabularies</e1> containing up to 20k <e2>words</e2>"
PART_WHOLE(e2,e1)

776	"The recognizer makes use of <e1>continuous density HMM</e1> with <e2>Gaussian mixture</e2> for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling"
MODEL-FEATURE(e2,e1)

777	"The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and <e1>n-gram statistics</e1> estimated on the newspaper texts for <e2>language modeling</e2>"
USAGE(e1,e2)

778	"A second <e1>forward pass</e1>, which makes use of a <e2>word graph</e2> generated with the bigram, incorporates a trigram language model"
USAGE(e2,e1)

779	"<e1>Acoustic modeling</e1> uses <e2>cepstrum-based features</e2>, context-dependent phone models &amp;&amp;lpar&amp;&amp;intra and interword&amp;&amp;rpar&amp;&amp;, phone duration models, and sex-dependent models.     "
USAGE(e2,e1)

780	"The <e1>system</e1> is based on a <e2>multi-component architecture</e2> where each component is responsible for identifying one class of unknown words"
USAGE(e2,e1)

781	"The focus of this paper is the <e1>components</e1> that identify <e2>names</e2> and spelling errors"
USAGE(e1,e2)

782	"Each <e1>component</e1> uses a <e2>decision tree architecture</e2> to combine multiple types of evidence about the unknown word"
USAGE(e2,e1)

783	"Each component uses a decision tree architecture to combine multiple types of <e1>evidence</e1> about the <e2>unknown word</e2>"
MODEL-FEATURE(e1,e2)

784	"The system is evaluated using data from <e1>live closed captions</e1> - a genre replete with a wide variety of <e2>unknown words</e2>.     "
PART_WHOLE(e2,e1)

785	" <e1>Recognition of proper nouns</e1> in <e2>Japanese text</e2> has been studied as a part of the more general problem of morphological analysis in Japanese text processing ([1] [2])"
USAGE(e1,e2)

786	" Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of <e1>morphological analysis</e1> in <e2>Japanese text processing</e2> ([1] [2])"
PART_WHOLE(e1,e2)

787	"Our approach to the Multi-lingual Evaluation Task (MET) for Japanese text is to consider the given task as a <e1>morphological analysis problem</e1> in <e2>Japanese</e2>"
USAGE(e1,e2)

788	"Our <e1>morphological analyzer</e1> has done all the necessary work for the <e2>recognition and classification of proper names, numerical and temporal expressions, i.e. Named Entity &amp;&amp;lpar&amp;&amp;NE&amp;&amp;rpar&amp;&amp; items</e2> in the Japanese text"
USAGE(e1,e2)

789	"First, it uses several kinds of <e1>dictionaries</e1> to segment and tag <e2>Japanese character strings</e2>"
USAGE(e1,e2)

790	"Second, based on the information resulting from the dictionary lookup stage, a set of <e1>rules</e1> is applied to the <e2>segmented strings</e2> in order to identify NE items"
USAGE(e1,e2)

791	" We present a <e1>practically unsupervised learning method</e1> to produce <e2>single-snippet answers</e2> to definition questions in question answering systems that supplement Web search engines"
USAGE(e1,e2)

792	"The method exploits <e1>on-line encyclopedias and dictionaries</e1> to generate automatically an arbitrarily large number of <e2>positive and negative definition examples</e2>, which are then used to train an svm to separate the two classes"
USAGE(e1,e2)

793	" Terminology structuring has been the subject of much work in the context of <e1>terms</e1> extracted from <e2>corpora</e2>: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical &amp;&amp;lpar&amp;&amp;or other types of&amp;&amp;rpar&amp;&amp; relations between these terms"
PART_WHOLE(e1,e2)

794	" Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of <e1>terms</e1>, obtained from an existing resource or extracted from a <e2>corpus</e2>, identifying hierarchical &amp;&amp;lpar&amp;&amp;or other types of&amp;&amp;rpar&amp;&amp; relations between these terms"
PART_WHOLE(e1,e2)

795	" Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying <e1>hierarchical &amp;&amp;lpar&amp;&amp;or other types of&amp;&amp;rpar&amp;&amp; relations</e1> between these <e2>terms</e2>"
MODEL-FEATURE(e1,e2)

796	"The present paper focusses on <e1>terminology structuring</e1> by <e2>lexical methods</e2>, which match terms on the basis on their content words, taking morphological variants into account"
USAGE(e2,e1)

797	"The present paper focusses on terminology structuring by lexical methods, which match <e1>terms</e1> on the basis on their <e2>content words</e2>, taking morphological variants into account"
PART_WHOLE(e2,e1)

798	"Experiments are done on a 'flat' list of <e1>terms</e1> obtained from an originally <e2>hierarchically-structured terminology</e2>: the French version of the US National Library of Medicine MeSH thesaurus"
PART_WHOLE(e1,e2)

799	"We compare the <e1>lexically-induced relations</e1> with the original <e2>MeSH relations</e2>: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis ofthe 'new' relations not present in the MeSH"
COMPARE(e1,e2)

800	"We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis ofthe 'new' <e1>relations</e1> not present in the <e2>MeSH</e2>"
PART_WHOLE(e1,e2)

801	"On the other hand, it also reveals some specific structuring choices and <e1>naming conventions</e1> made by the <e2>MeSH</e2> designers, and emphasizes ontological commitments that cannot be left to automatic structuring.     "
MODEL-FEATURE(e1,e2)

802	" In this study, we propose a knowledge-independent method for aligning terms and thus extracting <e1>translations</e1> from a <e2>small, domain-specific corpus</e2> consisting of parallel English and Chinese court judgments from Hong Kong"
PART_WHOLE(e1,e2)

803	"With a sentence-aligned corpus, translation equivalences are suggested by analysing the <e1>frequency profiles</e1> of <e2>parallel concordances</e2>"
MODEL-FEATURE(e1,e2)

804	"The method overcomes the limitations of <e1>conventional statistical methods</e1> which require <e2>large corpora</e2> to be effective, and lexical approaches which depend on existing bilingual dictionaries"
USAGE(e2,e1)

805	"The method overcomes the limitations of conventional statistical methods which require large corpora to be effective, and <e1>lexical approaches</e1> which depend on existing <e2>bilingual dictionaries</e2>"
USAGE(e2,e1)

806	"Pilot testing on a <e1>parallel corpus</e1> of about 113K <e2>Chinese words</e2> and 120K English words gives an encouraging 85% precision and 45% recall"
PART_WHOLE(e2,e1)

807	"Future work includes fine-tuning the algorithm upon the analysis of the errors, and acquiring a <e1>translation lexicon</e1> for <e2>legal terminology</e2> by filtering out general terms.     "
PART_WHOLE(e2,e1)

808	" <e1>Coedition</e1> of a <e2>natural language text</e2> and its representation in some interlingual form seems the best and simplest way to share text revision across languages"
USAGE(e1,e2)

809	"We are developing a prototype where, in the simplest sharing scenario, naive users interact directly with the <e1>text</e1> in their <e2>language &amp;&amp;lpar&amp;&amp;L0&amp;&amp;rpar&amp;&amp;</e2>, and indirectly with the associated graph"
MODEL-FEATURE(e2,e1)

810	"Establishing a "best" correspondence between the "UNL-tree+L0" and the "MS-L0 structure", a lattice, may be done using the dictionary and trying to align the <e1>tree</e1> and the selected trajectory with as few <e2>crossing liaisons</e2> as possible"
MODEL-FEATURE(e2,e1)

811	"The improved method stops the <e1>EM algorithm</e1> at the <e2>optimum iteration number</e2>"
MODEL-FEATURE(e2,e1)

812	"In experiments, we solved 50 <e1>noun WSD problems</e1> in the <e2>Japanese Dictionary Task in SENSEVAL2</e2>"
PART_WHOLE(e1,e2)

813	" In this paper we discuss a proposed <e1>user knowledge modeling architecture</e1> for the <e2>ICICLE system</e2>, a language tutoring application for deaf learners of written English"
PART_WHOLE(e1,e2)

814	" In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system, a <e1>language tutoring application</e1> for deaf learners of <e2>written English</e2>"
USAGE(e1,e2)

815	"We motivate our <e1>model design</e1> by citing relevant research on <e2>second language and cognitive skill acquisition</e2>, and briefly discuss preliminary empirical evidence supporting the design"
USAGE(e2,e1)

816	" This paper describes novel and practical <e1>Japanese parsers</e1> that uses <e2>decision trees</e2>"
USAGE(e2,e1)

817	"First, we construct a single <e1>decision tree</e1> to estimate <e2>modification probabilities</e2>; how one phrase tends to modify another"
MODEL-FEATURE(e1,e2)

818	"Next, we introduce a boosting algorithm in which several <e1>decision trees</e1> are constructed and then combined for <e2>probability estimation</e2>"
USAGE(e1,e2)

819	" Automatic estimation of <e1>word significance</e1> oriented for <e2>speech-based Information Retrieval &amp;&amp;lpar&amp;&amp;IR&amp;&amp;rpar&amp;&amp;</e2> is addressed"
USAGE(e1,e2)

820	"Since the <e1>significance</e1> of <e2>words</e2> differs in IR, automatic speech recognition &amp;&amp;lpar&amp;&amp;ASR&amp;&amp;rpar&amp;&amp; performance has been evaluated based on weighted word error rate &amp;&amp;lpar&amp;&amp;WWER&amp;&amp;rpar&amp;&amp;, which gives a weight on errors from the viewpoint of IR, instead of word error rate &amp;&amp;lpar&amp;&amp;WER&amp;&amp;rpar&amp;&amp;, which treats all words uniformly"
MODEL-FEATURE(e1,e2)

821	"Since the significance of words differs in IR, automatic speech recognition &amp;&amp;lpar&amp;&amp;ASR&amp;&amp;rpar&amp;&amp; performance has been evaluated based on weighted word error rate &amp;&amp;lpar&amp;&amp;WWER&amp;&amp;rpar&amp;&amp;, which gives a <e1>weight</e1> on errors from the viewpoint of IR, instead of <e2>word error rate &amp;&amp;lpar&amp;&amp;WER&amp;&amp;rpar&amp;&amp;</e2>, which treats all words uniformly"
COMPARE(e1,e2)

822	"A <e1>decoding strategy</e1> that minimizes WWER based on a <e2>Minimum Bayes-Risk framework</e2> has been shown, and the reduction of errors on both ASR and IR has been reported"
USAGE(e2,e1)

823	"A decoding strategy that minimizes WWER based on a Minimum Bayes-Risk framework has been shown, and the reduction of errors on both <e1>ASR</e1> and <e2>IR</e2> has been reported"
COMPARE(e1,e2)

824	"In this paper, we propose an <e1>automatic estimation method</e1> for <e2>word significance &amp;&amp;lpar&amp;&amp;weights&amp;&amp;rpar&amp;&amp;</e2> based on its influence on IR"
USAGE(e1,e2)

825	" This study presents a <e1>method to automatically acquire paraphrases</e1> using <e2>bilingual corpora</e2>, which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques"
USAGE(e2,e1)

826	" This study presents a method to automatically acquire paraphrases using bilingual corpora, which utilizes the <e1>bilingual dependency relations</e1> obtained by projecting a monolingual dependency parse onto the other language sentence based on <e2>statistical alignment techniques</e2>"
USAGE(e2,e1)

827	"Since the paraphrasing method is capable of clearly disambiguating the <e1>sense</e1> of an original <e2>phrase</e2> using the bilingual context of dependency relation, it would be possible to obtain interchangeable paraphrases under a given context"
MODEL-FEATURE(e1,e2)

828	"Since the paraphrasing method is capable of clearly disambiguating the sense of an original phrase using the bilingual context of dependency relation, it would be possible to obtain interchangeable <e1>paraphrases</e1> under a given <e2>context</e2>"
MODEL-FEATURE(e2,e1)

829	"Also, we provide an advanced method to acquire <e1>generalized translation knowledge</e1> using the extracted <e2>paraphrases</e2>"
USAGE(e2,e1)

830	"We applied the method to acquire the <e1>generalized translation knowledge</e1> for <e2>Korean-English translation</e2>"
MODEL-FEATURE(e1,e2)

831	"Through experiments with <e1>parallel corpora</e1> of a <e2>Korean and English language pairs</e2>, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.     "
PART_WHOLE(e2,e1)

832	"Through experiments with parallel corpora of a Korean and English language pairs, we show that our <e1>paraphrasing method</e1> effectively extracts paraphrases with high <e2>precision</e2>, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.     "
RESULT(e1,e2)

833	"Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the <e1>translation knowledge</e1> extracted from the <e2>bilingual corpora</e2> could be generalized successfully using the paraphrases with the 12.5% compression ratio.     "
PART_WHOLE(e1,e2)

834	"Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the <e1>paraphrases</e1> with the 12.5% <e2>compression ratio</e2>.     "
MODEL-FEATURE(e2,e1)

835	" This paper describes a <e1>computational model</e1> of <e2>word segmentation</e2> and presents simulation results on realistic acquisition"
MODEL-FEATURE(e1,e2)

836	"In particular, we explore the capacity and limitations of <e1>statistical learning mechanisms</e1> that have recently gained prominence in <e2>cognitive psychology</e2> and linguistics.     "
USAGE(e1,e2)

837	"<e1>Dictionary construction</e1>, one of the most difficult tasks in developing a <e2>machine translation system</e2>, is expensive"
PART_WHOLE(e1,e2)

838	"To avoid this problem, we investigate how we build a <e1>dictionary</e1> using existing <e2>linguistic resources</e2>"
USAGE(e2,e1)

839	"Our <e1>algorithm</e1> can be applied to any <e2>language pairs</e2>, but for the present we focus on building a Korean-to-Japanese dictionary using English as a pivot"
USAGE(e1,e2)

840	"Our algorithm can be applied to any language pairs, but for the present we focus on building a <e1>Korean-to-Japanese dictionary</e1> using English as a <e2>pivot</e2>"
USAGE(e2,e1)

841	"We attempt three ways of automatic construction to corroborate the effect of the <e1>directionality</e1> of <e2>dictionaries</e2>"
MODEL-FEATURE(e1,e2)

842	"First, we introduce <e1>"one-time look up" method</e1> using a <e2>Korean-to-English and a Japanese-to-English dictionary</e2>"
USAGE(e2,e1)

843	"We present an approach to annotating a level of <e1>discourse structure</e1> that is based on identifying <e2>discourse connectives</e2> and their arguments"
USAGE(e2,e1)

844	" In this paper, we present a fully automated extraction system, named IntEx, to identify <e1>gene and protein interactions</e1> in <e2>biomedical text</e2>"
PART_WHOLE(e1,e2)

845	"Our approach is based on first splitting <e1>complex sentences</e1> into <e2>simple clausal structures</e2> made up of syntactic roles"
PART_WHOLE(e2,e1)

846	"Then, tagging <e1>biological entities</e1> with the help of <e2>biomedical and linguistic ontologies</e2>"
MODEL-FEATURE(e2,e1)

847	"Our <e1>extraction system</e1> handles <e2>complex sentences</e2> and extracts multiple and nested interactions specified in a sentence"
USAGE(e1,e2)

848	"Our extraction system handles complex sentences and extracts <e1>multiple and nested interactions</e1> specified in a <e2>sentence</e2>"
PART_WHOLE(e1,e2)

849	"Experimental evaluations with two other state of the art extraction systems indicate that the <e1>IntEx system</e1> achieves better <e2>performance</e2> without the labor intensive pattern engineering requirement.     "
RESULT(e1,e2)

850	" We propose a framework to derive the <e1>distance</e1> between <e2>concepts</e2> from distributional measures of word co-occurrences"
MODEL-FEATURE(e1,e2)

851	"We use the <e1>categories</e1> in a published <e2>thesaurus</e2> as coarse-grained concepts, allowing all possible distance values to be stored in a concept-concept matrix roughly.01% the size of that created by existing measures"
PART_WHOLE(e1,e2)

852	"We use the categories in a published thesaurus as <e1>coarse-grained concepts</e1>, allowing all possible distance values to be stored in a <e2>concept-concept matrix</e2> roughly.01% the size of that created by existing measures"
PART_WHOLE(e1,e2)

853	"We show that the newly proposed <e1>concept-distance measures</e1> outperform <e2>traditional distributional word-distance measures</e2> in the tasks of (1) ranking word pairs in order of semantic distance, and (2) correcting real-word spelling errors"
COMPARE(e1,e2)

854	"We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of (1) ranking <e1>word pairs</e1> in order of <e2>semantic distance</e2>, and (2) correcting real-word spelling errors"
MODEL-FEATURE(e2,e1)

855	"In the latter task, of all the <e1>WordNet-based measures</e1>, only that proposed by Jiang and Conrath outperforms the best <e2>distributional concept-distance measures</e2>.     "
COMPARE(e1,e2)

856	" We argue in favor of the the use of <e1>labeled directed graph</e1> to represent various types of <e2>linguistic structures</e2>, and illustrate how this allows one to view NLP tasks as graph transformations"
MODEL-FEATURE(e1,e2)

857	" We argue in favor of the the use of labeled directed graph to represent various types of linguistic structures, and illustrate how this allows one to view <e1>NLP tasks</e1> as <e2>graph transformations</e2>"
MODEL-FEATURE(e2,e1)

858	"We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: <e1>identification of non-local depenencies</e1> (using <e2>Penn Treebank data</e2>) and semantic role labeling (using Proposition Bank data).     "
MODEL-FEATURE(e1,e2)

859	"We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using Penn Treebank data) and <e1>semantic role labeling</e1> (using <e2>Proposition Bank data</e2>).     "
MODEL-FEATURE(e1,e2)

860	" Topical blog post retrieval is the task of ranking <e1>blog posts</e1> with respect to their <e2>relevance</e2> for a given topic"
MODEL-FEATURE(e2,e1)

861	"To improve <e1>topical blog post retrieval</e1> we incorporate <e2>textual credibility indicators</e2> in the retrieval process"
USAGE(e2,e1)

862	"We describe how to estimate these <e1>indicators</e1> and how to integrate them into a <e2>retrieval approach</e2> based on language models"
USAGE(e1,e2)

863	"Experiments on the TREC Blog track test set show that both groups of <e1>credibility indicators</e1> significantly improve <e2>retrieval effectiveness</e2>; the best performance is achieved when combining them.     "
RESULT(e1,e2)

864	"Four problems render vector space model &amp;&amp;lpar&amp;&amp;VSM&amp;&amp;rpar&amp;&amp;-based text classification approach ineffective: 1) Many <e1>words</e1> within <e2>song lyrics</e2> actually contribute little to sentiment; 2) Nouns and verbs used to express sentiment are ambiguous; 3) Negations and modifiers around the sentiment keywords make particular contributions to sentiment; 4) Song lyric is usually very short"
PART_WHOLE(e1,e2)

865	"To address these problems, the <e1>sentiment vector space model &amp;&amp;lpar&amp;&amp;s-VSM&amp;&amp;rpar&amp;&amp;</e1> is proposed to represent <e2>song lyric document</e2>"
MODEL-FEATURE(e1,e2)

866	"The preliminary experiments prove that the <e1>s-VSM model</e1> outperforms the <e2>VSM model</e2> in the lyric-based song sentiment classification task"
COMPARE(e1,e2)

867	" This paper shows that it is very often possible to identify the <e1>source language</e1> of medium-length speeches in the <e2>EUROPARL corpus</e2> on the basis of frequency counts of word n-grams (87.2%-96.7% accuracy depending on classification method)"
MODEL-FEATURE(e1,e2)

868	" This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of <e1>frequency counts</e1> of <e2>word n-grams</e2> (87.2%-96.7% accuracy depending on classification method)"
MODEL-FEATURE(e1,e2)

869	" This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams (87.2%-96.7% <e1>accuracy</e1> depending on <e2>classification method</e2>)"
RESULT(e2,e1)

870	" <e1>Words</e1> in <e2>Chinese text</e2> are not naturally separated by delimiters, which poses a challenge to standard machine translation &amp;&amp;lpar&amp;&amp;MT&amp;&amp;rpar&amp;&amp; systems"
PART_WHOLE(e1,e2)

871	"In <e1>MT</e1>, the widely used approach is to apply a <e2>Chinese word segmenter</e2> trained from manually annotated data, using a fixed lexicon"
USAGE(e2,e1)

872	"We propose a <e1>Bayesian semi-supervised Chinese word segmentation model</e1> which uses both <e2>monolingual and bilingual information</e2> to derive a segmentation suitable for MT"
USAGE(e2,e1)

873	"We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a <e1>segmentation</e1> suitable for <e2>MT</e2>"
USAGE(e1,e2)

874	"Indeed, <e1>automatic evaluations</e1> need <e2>high-quality data</e2> that allow the comparison of both automatic and human translations"
USAGE(e2,e1)

875	"This paper describes the impact of using <e1>different-quality references</e1> on <e2>evaluation</e2>"
USAGE(e1,e2)

876	"Thus, the limitations of the <e1>automatic metrics</e1> used within <e2>MT</e2> are also discussed in this regard.     "
USAGE(e1,e2)

877	"The tool supports <e1>queries</e1> with an arbitrary number of <e2>wildcards</e2>"
MODEL-FEATURE(e2,e1)

878	" This paper presents an approach to the <e1>unsupervised learning</e1> of parts of speech which uses both <e2>morphological and syntactic information</e2>"
USAGE(e2,e1)

879	"While the model is more complex than those which have been employed for <e1>unsupervised learning</e1> of POS tags in English, which use only <e2>syntactic information</e2>, the variety of languages in the world requires that we consider morphology as well"
USAGE(e2,e1)

880	"While the model is more complex than those which have been employed for unsupervised learning of POS tags in English, which use only syntactic information, the variety of <e1>languages</e1> in the world requires that we consider <e2>morphology</e2> as well"
PART_WHOLE(e2,e1)

881	"In many languages, <e1>morphology</e1> provides better clues to a word's category than <e2>word order</e2>"
COMPARE(e1,e2)

882	"We present the <e1>computational model</e1> for <e2>POS learning</e2>, and present results for applying it to Bulgarian, a Slavic language with relatively free word order and rich morphology.     "
USAGE(e1,e2)

883	"We present the computational model for POS learning, and present results for applying it to <e1>Bulgarian</e1>, a Slavic language with relatively <e2>free word order</e2> and rich morphology.     "
MODEL-FEATURE(e1,e2)

884	" We propose a solution to the challenge of the <e1>CoNLL 2008 shared task</e1> that uses a <e2>generative history-based latent variable model</e2> to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies"
USAGE(e2,e1)

885	" We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely <e1>derivation</e1> of a synchronous dependency parser for both <e2>syntactic and semantic dependencies</e2>"
MODEL-FEATURE(e1,e2)

886	"The submitted <e1>model</e1> yields 79.1% <e2>macro-average F1 performance</e2>, for the joint task, 86.9% syntactic dependencies LAS and 71.0% semantic dependencies F1"
RESULT(e1,e2)

887	"A larger <e1>model</e1> trained after the deadline achieves 80.5% <e2>macro-average F1</e2>, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1.     "
RESULT(e1,e2)

888	" Pipelined Natural Language Generation &amp;&amp;lpar&amp;&amp;NLG&amp;&amp;rpar&amp;&amp; systems have grown increasingly complex as <e1>architectural modules</e1> were added to support <e2>language functionalities</e2> such as referring expressions, lexical choice, and revision"
USAGE(e1,e2)

889	"This has given rise to discussions about the relative placement of these new <e1>modules</e1> in the overall <e2>architecture</e2>"
PART_WHOLE(e1,e2)

890	"We present examples which suggest that in a <e1>pipelined NLG architecture</e1>, the best approach is to strongly tie it to a <e2>revision component</e2>"
PART_WHOLE(e2,e1)

891	" With performance above 97% <e1>accuracy</e1> for newspaper text, <e2>part of speech &amp;&amp;lpar&amp;&amp;pos&amp;&amp;rpar&amp;&amp; tagging</e2> might be considered a solved problem"
RESULT(e2,e1)

892	"Previous studies have shown that allowing the <e1>parser</e1> to resolve <e2>pos tag ambiguity</e2> does not improve performance"
USAGE(e1,e2)

893	"However, for <e1>grammar formalisms</e1> which use more <e2>fine-grained grammatical categories</e2>, for example tag and ccg, tagging accuracy is much lower"
USAGE(e2,e1)

894	"We describe a <e1>multi-tagging approach</e1> which maintains a suitable level of lexical category ambiguity for accurate and efficient <e2>ccg parsing</e2>"
USAGE(e1,e2)

895	"Although pos tagging accuracy seems high, maintaining some <e1>pos tag ambiguity</e1> in the language processing pipeline results in more accurate <e2>ccg supertagging</e2>.     "
RESULT(e1,e2)

896	" Both rhetorical structure and <e1>punctuation</e1> have been helpful in <e2>discourse processing</e2>"
USAGE(e1,e2)

897	"Based on a corpus annotation project, this paper reports the <e1>discursive usage</e1> of 6 <e2>Chinese punctuation marks</e2> in news commentary texts: Colon, Dash, Ellipsis, Exclamation Mark, Question Mark, and Semicolon"
MODEL-FEATURE(e1,e2)

898	"The <e1>rhetorical patterns</e1> of these marks are compared against <e2>patterns</e2> around cue phrases in general"
COMPARE(e1,e2)

899	"Results show that these <e1>Chinese punctuation marks</e1>, though fewer in number than <e2>cue phrases</e2>, are easy to identify, have strong correlation with certain relations, and can be used as distinctive indicators of nuclearity in Chinese texts.     "
COMPARE(e1,e2)

900	"We show that the crucial operation of consistency checking for such descriptions is NP-complete, and therefore probably intractable, but proceed to develop <e1>algorithms</e1> which can sometimes alleviate the unpleasant consequences of this <e2>intractability</e2>.     "
USAGE(e1,e2)

901	" This paper presents an algorithm for selecting an appropriate <e1>classifier word</e1> for a <e2>noun</e2>"
MODEL-FEATURE(e1,e2)

902	"In Thai language, it frequently happens that there is fluctuation in the choice of <e1>classifier</e1> for a given <e2>concrete noun</e2>, both from the point of view of the whole speech community and individual speakers"
MODEL-FEATURE(e1,e2)

903	"As far as we can do in the rule-based approach is to give a default rule to pick up a corresponding <e1>classifier</e1> of each <e2>noun</e2>"
MODEL-FEATURE(e1,e2)

904	"Registration of <e1>classifier</e1> for each <e2>noun</e2> is limited to the type of unit classifier because other types are open due to the meaning of representation"
MODEL-FEATURE(e1,e2)

905	"We propose a <e1>corpus-based method</e1> (Biber,1993; Nagao,1993; Smadja,1993) which generates Noun Classifier Associations &amp;&amp;lpar&amp;&amp;NCA&amp;&amp;rpar&amp;&amp; to overcome the problems in <e2>classifier assignment</e2> and semantic construction of noun phrase"
USAGE(e1,e2)

906	" This paper describes an <e1>unsupervised learning method</e1> for <e2>associative relationships between verb phrases</e2>, which is important in developing reliable Q&amp;A systems"
USAGE(e1,e2)

907	"Our aim is to develop an <e1>unsupervised learning method</e1> that can obtain such an <e2>associative relationship</e2>, which we call scenario consistency"
USAGE(e1,e2)

908	"The method we are currently working on uses an <e1>expectation-maximization &amp;&amp;lpar&amp;&amp;EM&amp;&amp;rpar&amp;&amp; based word-clustering algorithm</e1>, and we have evaluated the effectiveness of this method using <e2>Japanese verb phrases</e2>"
USAGE(e1,e2)

909	"These <e1>models</e1> provide principled ways of including additional conditioning variables other than the <e2>preceding words</e2>, such as morphological or syntactic features"
COMPARE(e1,e2)

910	"This paper presents an <e1>entirely data-driven model selection procedure</e1> based on <e2>genetic search</e2>, which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks (Arabic and Turkish).     "
USAGE(e2,e1)

911	"This paper presents an entirely data-driven model selection procedure based on genetic search, which is shown to outperform both <e1>knowledge-based and random selection procedures</e1> on two different <e2>language modeling tasks</e2> (Arabic and Turkish).     "
USAGE(e1,e2)

912	"Landsbergen's advocacy of <e1>analytical inverses</e1> for <e2>compositional syntax rules</e2> encourages the application of Definite Clause Grammar techniques to the construction of a parser returning  Montague analysis trees"
MODEL-FEATURE(e1,e2)

913	"Landsbergen's advocacy of analytical inverses for compositional syntax rules encourages the application of <e1>Definite Clause Grammar techniques</e1> to the construction of a <e2>parser</e2> returning  Montague analysis trees"
USAGE(e1,e2)

914	"A <e1>parser MDCC</e1> is presented which implements an <e2>augmented Friedman - Warren algorithm</e2> permitting post referencing* and interfaces with a language of intenslonal logic translator LILT so as to display the derivational history of corresponding reduced IL formulae"
USAGE(e2,e1)

915	"A parser MDCC is presented which implements an augmented Friedman - Warren algorithm permitting post referencing* and interfaces with a language of intenslonal logic translator LILT so as to display the <e1>derivational history</e1> of corresponding <e2>reduced IL formulae</e2>"
MODEL-FEATURE(e1,e2)

916	" Theoretical research in the area of <e1>machine translation</e1> usually involves the search for and creation of an appropriate <e2>formalism</e2>"
USAGE(e2,e1)

917	"An important issue in this respect is the way in which the <e1>compositionality</e1> of <e2>translation</e2> is to be defined"
MODEL-FEATURE(e1,e2)

918	"In this paper, we will introduce the <e1>anaphoric component</e1> of the <e2>Mimo formalism</e2>"
PART_WHOLE(e1,e2)

919	"In <e1>Mimo</e1>, the <e2>translation</e2> of anaphoric relations is compositional"
USAGE(e1,e2)

920	"The <e1>anaphoric component</e1> is used to define <e2>linguistic phenomena</e2> such as wh-movement, the passive and the binding of reflexives and pronouns mono-lingually"
USAGE(e1,e2)

921	" A <e1>domain independent model</e1> is proposed for the <e2>automated interpretation</e2> of nominal compounds in English"
USAGE(e1,e2)

922	" A domain independent model is proposed for the automated interpretation of <e1>nominal compounds</e1> in <e2>English</e2>"
PART_WHOLE(e1,e2)

923	"This model is meant to account for productive rules of interpretation which are inferred from the <e1>morpho-syntactic and semantic characteristics</e1> of the <e2>nominal constituents</e2>"
MODEL-FEATURE(e1,e2)

924	"In particular, we make extensive use of Pustejovsky's principles concerning the <e1>predicative information</e1> associated with <e2>nominals</e2>"
MODEL-FEATURE(e1,e2)

925	"We argue that it is necessary to draw a line between <e1>generalizable semantic principles</e1> and <e2>domain-specific semantic information</e2>"
COMPARE(e1,e2)

926	"We explain this distinction and we show how this model may be applied to the <e1>interpretation</e1> of <e2>compounds</e2> in real texts, provided that complementary semantic information are retrieved.     "
MODEL-FEATURE(e1,e2)

927	"We present a novel <e1>entity-based representation</e1> of <e2>discourse</e2> which is inspired by Centering Theory and can be computed automatically from raw text"
MODEL-FEATURE(e1,e2)

928	"We view <e1>coherence assessment</e1> as a <e2>ranking learning problem</e2> and show that the proposed discourse representation supports the effective learning of a ranking function"
MODEL-FEATURE(e2,e1)

929	"Our experiments demonstrate that the <e1>induced model</e1> achieves significantly higher accuracy than a <e2>state-of-the-art coherence model</e2>.    "
COMPARE(e1,e2)

930	" <e1>Sentence boundary detection</e1> in speech is important for enriching <e2>speech recognition</e2> output, making it easier for humans to read and downstream modules to process"
USAGE(e1,e2)

931	"In previous work, we have developed <e1>hidden Markov model &amp;&amp;lpar&amp;&amp;HMM&amp;&amp;rpar&amp;&amp; and maximum entropy &amp;&amp;lpar&amp;&amp;Maxent&amp;&amp;rpar&amp;&amp; classifiers</e1> that integrate textual and prosodic <e2>knowledge sources</e2> for detecting sentence boundaries"
USAGE(e2,e1)

932	"We evaluate across two corpora (conversational telephone speech and broadcast news speech) on both <e1>human transcriptions</e1> and <e2>speech recognition</e2> output"
COMPARE(e1,e2)

933	"In general, our <e1>CRF</e1> model yields a lower error rate than the <e2>HMM and Max-ent models</e2> on the NIST sentence boundary detection task in speech, although it is interesting to note that the best results are achieved by three-way voting among the classifiers"
COMPARE(e1,e2)

934	"Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the <e1>training and test data</e1> with respect to <e2>topic</e2>"
MODEL-FEATURE(e2,e1)

935	"This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with <e1>training data</e1> labeled with <e2>emoticons</e2>, which has the potential of being independent of domain, topic and time.    "
MODEL-FEATURE(e2,e1)

936	" Using <e1>natural language processing</e1>, we carried out a trend survey on <e2>Japanese natural language processing studies</e2> that have been done over the last ten years"
USAGE(e1,e2)

937	" This paper explores the issue of using different <e1>co-occurrence similarities</e1> between <e2>terms</e2> for separating query terms that are useful for retrieval from those that are harmful"
MODEL-FEATURE(e1,e2)

938	" This paper explores the issue of using different co-occurrence similarities between terms for separating <e1>query terms</e1> that are useful for <e2>retrieval</e2> from those that are harmful"
USAGE(e1,e2)

939	"The hypothesis under examination is that <e1>useful terms</e1> tend to be more similar to each other than to other <e2>query terms</e2>"
COMPARE(e1,e2)

940	"<e1>Term similarities</e1> could then be used for determining which <e2>query terms</e2> are useful and best reflect the user's information need"
MODEL-FEATURE(e1,e2)

941	"A possible application would be to use this source of evidence for tuning the <e1>weights</e1> of the <e2>query terms</e2>.    "
MODEL-FEATURE(e1,e2)

942	" We propose a draft scheme of the model formalizing the <e1>structure of communicative context</e1> in <e2>dialogue interaction</e2>"
MODEL-FEATURE(e1,e2)

943	"<e1>Memo-functions</e1> also facilitate a simple way to construct a very compact representation of the <e2>parse forest</e2>"
USAGE(e1,e2)

944	"Extended CF grammars (<e1>grammars</e1> with <e2>regular expressions</e2> at the right hand side) can be parsed with a simple modification of the LR-parser for normal CF grammars.     "
PART_WHOLE(e2,e1)

945	"Extended CF grammars (grammars with regular expressions at the right hand side) can be parsed with a simple modification of the <e1>LR-parser</e1> for normal <e2>CF grammars</e2>.     "
USAGE(e1,e2)

946	" This paper defines a <e1>generative probabilistic model</e1> of <e2>parse trees</e2>, which we call PCFG-LA"
MODEL-FEATURE(e1,e2)

947	"Finegrained <e1>CFG rules</e1> are automatically induced from a <e2>parsed corpus</e2> by training a PCFG-LA model using an EM-algorithm"
USAGE(e2,e1)

948	"Finegrained CFG rules are automatically induced from a parsed corpus by <e1>training</e1> a PCFG-LA model using an <e2>EM-algorithm</e2>"
USAGE(e2,e1)

949	"Because exact <e1>parsing</e1> with a <e2>PCFG-LA</e2> is NP-hard, several approximations are described and empirically compared"
USAGE(e2,e1)

950	"In experiments using the Penn WSJ corpus, our automatically trained <e1>model</e1> gave a <e2>performance</e2> of 86.6% (F1, sentences &lt; 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.     "
RESULT(e1,e2)

951	"In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, <e1>sentences</e1> &lt; 40 <e2>words</e2>), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.     "
PART_WHOLE(e2,e1)

952	"In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences &lt; 40 words), which is comparable to that of an <e1>unlexicalized PCFG parser</e1> created using extensive <e2>manual feature selection</e2>.     "
USAGE(e2,e1)

953	"This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in <e1>feature-based relation extraction</e1> using <e2>SVM</e2>"
USAGE(e2,e1)

954	"Our study illustrates that the base <e1>phrase chunking</e1> information is very effective for relation extraction and contributes to most of the <e2>performance improvement</e2> from syntactic aspect while additional information from full parsing gives limited further enhancement"
RESULT(e1,e2)

955	"We also demonstrate how <e1>semantic information</e1> such as WordNet and Name List, can be used in feature-based relation extraction to further improve the <e2>performance</e2>"
RESULT(e1,e2)

956	"Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our <e1>system</e1> outperform previously best-reported <e2>systems</e2> on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.     "
COMPARE(e1,e2)

957	" This paper describes a novel <e1>system</e1> for <e2>acquiring adjectival subcategorization frames</e2> (scfs) and associated frequency information from English corpus data"
USAGE(e1,e2)

958	" This paper describes a novel system for acquiring adjectival subcategorization frames (scfs) and associated frequency information from <e1>English</e1> <e2>corpus data</e2>"
MODEL-FEATURE(e1,e2)

959	"The <e1>system</e1> incorporates a <e2>decision-tree classifier</e2> for 30 scf types which tests for the presence of grammatical relations (grs) in the output of a robust statistical parser"
PART_WHOLE(e2,e1)

960	"The system incorporates a decision-tree classifier for 30 scf types which tests for the presence of <e1>grammatical relations</e1> (grs) in the <e2>output</e2> of a robust statistical parser"
PART_WHOLE(e1,e2)

961	"It uses a powerful pattern-matching language to classify <e1>grs</e1> into <e2>frames</e2> hierarchically in a way that mirrors inheritance-based lexica"
MODEL-FEATURE(e2,e1)

962	"The experiments show that the <e1>system</e1> is able to detect scf types with <e2>70% precision</e2> and 66% recall rate"
RESULT(e1,e2)

963	"A new <e1>tool</e1> for <e2>linguistic annotation</e2> of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition.     "
USAGE(e1,e2)

964	"A new tool for linguistic annotation of <e1>scfs</e1> in <e2>corpus data</e2> is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition.     "
PART_WHOLE(e1,e2)

965	"A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining <e1>training and test data</e1> for <e2>subcategorization acquisition</e2>.     "
USAGE(e1,e2)

966	" We present a tool, called ILIMP, which takes as input a <e1>raw text</e1> in <e2>French</e2> and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag [ANA] for anaphoric or [IMP] for impersonal or expletive"
MODEL-FEATURE(e2,e1)

967	" We present a tool, called ILIMP, which takes as input a raw text in French and produces as output the same text in which every occurrence of the <e1>pronoun il</e1> is tagged either with tag <e2>[ANA]</e2> for anaphoric or [IMP] for impersonal or expletive"
MODEL-FEATURE(e2,e1)

968	"This tool is therefore designed to distinguish between the anaphoric occurrences of il, for which an anaphora resolution system has to look for an antecedent, and the <e1>expletive occurrences</e1> of this <e2>pronoun</e2>, for which it does not make sense to look for an antecedent"
MODEL-FEATURE(e1,e2)

969	"The <e1>precision rate</e1> for <e2>ILIMP</e2> is 97,5%"
RESULT(e2,e1)

970	"Other <e1>tasks</e1> using the <e2>method</e2> developed for ILIMP are described briefly, as well as the use of ILIMP in a modular syntactic analysis system"
USAGE(e2,e1)

971	"Other tasks using the method developed for ILIMP are described briefly, as well as the use of <e1>ILIMP</e1> in a modular <e2>syntactic analysis system</e2>"
USAGE(e1,e2)

972	" <e1>Systemic grammar</e1> has been used for <e2>AI text generation</e2> work in the past, but the implementations have tended be ad hoc or inefficient"
USAGE(e1,e2)

973	"This paper presents an approach to systemic text generation where <e1>AI problem solving techniques</e1> are applied directly to an unadulterated <e2>systemic grammar</e2>"
USAGE(e1,e2)

974	"The result is simple, efficient <e1>text generation</e1> firmly based in a <e2>linguistic theory</e2>"
USAGE(e2,e1)

975	" This paper presents a <e1>critical discussion</e1> of the various approaches that have been used in the <e2>evaluation of Natural Language systems</e2>"
TOPIC(e1,e2)

976	"We conclude that previous approaches have neglected to evaluate systems in the context of their use, e.g. solving a <e1>task</e1> requiring <e2>data retrieval</e2>"
USAGE(e2,e1)

977	"In the second half of the paper, we report a <e1>laboratory study</e1> using the <e2>Wizard of Oz technique</e2> to identify NL requirements for carrying out this task"
USAGE(e2,e1)

978	"We identify three important requirements which arose from the task that we gave our subjects: operators specific to the task of database access, complex contextual reference and reference to the <e1>structure</e1> of the <e2>information source</e2>"
MODEL-FEATURE(e1,e2)

979	" <e1>Semantic theories</e1> of <e2>natural language</e2> associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts"
TOPIC(e1,e2)

980	" Semantic theories of natural language associate <e1>meanings</e1> with <e2>utterances</e2> by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts"
MODEL-FEATURE(e1,e2)

981	" Semantic theories of natural language associate meanings with utterances by providing <e1>meanings</e1> for <e2>lexical items</e2> and rules for determining the meaning of larger units given the meanings of their parts"
MODEL-FEATURE(e1,e2)

982	" Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the <e1>meaning</e1> of larger <e2>units</e2> given the meanings of their parts"
MODEL-FEATURE(e1,e2)

983	"Traditionally, meanings are combined via function composition, which works well when <e1>constituent structure trees</e1> are used to guide <e2>semantic composition</e2>"
USAGE(e1,e2)

984	"More recently, the <e1>functional structure</e1> of <e2>LFG</e2> has been used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format"
PART_WHOLE(e1,e2)

985	"More recently, the functional structure of LFG has been used to provide the <e1>syntactic information</e1> necessary for constraining <e2>derivations</e2> of meaning in a cross-linguistically uniform format"
USAGE(e1,e2)

986	" In contrast to <e1>compositional approaches</e1>, we present a <e2>deductive approach</e2> to assembling meanings, based on reasoning with constraints, which meshes well with the unordered nature of information in the functional structure"
COMPARE(e1,e2)

987	" In contrast to compositional approaches, we present a deductive approach to assembling meanings, based on reasoning with constraints, which meshes well with the unordered nature of <e1>information</e1> in the <e2>functional structure</e2>"
PART_WHOLE(e1,e2)

988	"Our use of linear logic as a 'glue' for assembling meanings also allows for a coherent treatment of modification as well as of the <e1>LFG</e1> requirements of <e2>completeness</e2> and coherence.    "
MODEL-FEATURE(e2,e1)

989	" This paper presents an analysis of temporal anaphora in <e1>sentences</e1> which contain <e2>quantification over events</e2>, within the framework of Discourse Representation Theory"
PART_WHOLE(e2,e1)

990	"The analysis in (Partee, 1984) of <e1>quantified sentences</e1>, introduced by a <e2>temporal connective</e2>, gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after"
PART_WHOLE(e2,e1)

991	"The analysis in (Partee, 1984) of quantified sentences, introduced by a temporal connective, gives the wrong truth-conditions when the <e1>temporal connective</e1> in the <e2>subordinate clause</e2> is before or after"
PART_WHOLE(e1,e2)

992	"This problem has been previously analyzed in (de Swart, 1991) as an instance of the <e1>proportion problem</e1> and given a solution from a <e2>Generalized Quantifier approach</e2>"
TOPIC(e2,e1)

993	"By using a careful distinction between the different notions of reference time based on (Kamp and Reyle, 1993), we propose a solution to this <e1>problem</e1>, within the framework of <e2>DRT</e2>"
USAGE(e2,e1)

994	"We show some applications of this <e1>solution</e1> to additional <e2>temporal anaphora phenomena</e2> in quantified sentences.    "
USAGE(e1,e2)

995	" This paper proposes an automatic, essentially <e1>domain-independent means of evaluating Spoken Language Systems &amp;&amp;lpar&amp;&amp;SLS&amp;&amp;rpar&amp;&amp;</e1> which combines <e2>software</e2> we have developed for that purpose (the "Comparator") and a set of specifications for answer expressions (the "Common Answer Specification", or CAS)"
USAGE(e2,e1)

996	" This paper proposes an automatic, essentially domain-independent means of evaluating Spoken Language Systems &amp;&amp;lpar&amp;&amp;SLS&amp;&amp;rpar&amp;&amp; which combines software we have developed for that purpose (the "Comparator") and a set of <e1>specifications</e1> for <e2>answer expressions</e2> (the "Common Answer Specification", or CAS)"
MODEL-FEATURE(e1,e2)

997	"The Common Answer Specification determines the <e1>syntax</e1> of <e2>answer expressions</e2>, the minimal content that must be included in them, the data to be included in and excluded from test corpora, and the procedures used by the Comparator"
MODEL-FEATURE(e1,e2)

998	"The Common Answer Specification determines the syntax of answer expressions, the minimal content that must be included in them, the <e1>data</e1> to be included in and excluded from <e2>test corpora</e2>, and the procedures used by the Comparator"
PART_WHOLE(e1,e2)

999	"The Common Answer Specification determines the syntax of answer expressions, the minimal content that must be included in them, the data to be included in and excluded from test corpora, and the <e1>procedures</e1> used by the <e2>Comparator</e2>"
USAGE(e1,e2)

1000	"Though some details of the <e1>CAS</e1> are particular to individual <e2>domains</e2>, the Comparator software is domain-independent, as is the CAS approach.    "
MODEL-FEATURE(e2,e1)

1001	"Though some details of the CAS are particular to individual domains, the <e1>Comparator software</e1> is <e2>domain-independent</e2>, as is the CAS approach.    "
MODEL-FEATURE(e2,e1)

1002	" Two themes have evolved in <e1>speech and text image processing</e1> work at <e2>Xerox PARC</e2> that expand and redefine the role of recognition technology in document-oriented applications"
TOPIC(e2,e1)

1003	" Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of <e1>recognition technology</e1> in <e2>document-oriented applications</e2>"
PART_WHOLE(e1,e2)

1004	"One is the development of systems that provide functionality similar to that of <e1>text processors</e1> but operate directly on <e2>audio and scanned image data</e2>"
COMPARE(e1,e2)

1005	"A second, related theme is the use of <e1>speech and text-image recognition</e1> to retrieve arbitrary, user-specified information from <e2>documents with signal content</e2>"
USAGE(e1,e2)

1006	" In this paper we present a <e1>statistical profile</e1> of the <e2>Named Entity task</e2>, a specific information extraction task for which corpora in several languages are available"
MODEL-FEATURE(e1,e2)

1007	" In this paper we present a statistical profile of the Named Entity task, a specific information extraction task for which <e1>corpora</e1> in several <e2>languages</e2> are available"
MODEL-FEATURE(e2,e1)

1008	"Using the <e1>results</e1> of the <e2>statistical analysis</e2>, we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis.    "
RESULT(e2,e1)

1009	"Using the results of the statistical analysis, we propose an <e1>algorithm</e1> for <e2>lower bound estimation</e2> for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis.    "
USAGE(e1,e2)

1010	"Using the results of the statistical analysis, we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the <e1>cross-lingual comparisons</e1> provided by the <e2>analysis</e2>.    "
TOPIC(e2,e1)

1011	" We consider the problem of <e1>question-focused sentence retrieval</e1> from complex <e2>news articles</e2> describing multi-event stories published over time"
USAGE(e1,e2)

1012	"Annotators generated a list of questions central to understanding each <e1>story</e1> in our <e2>corpus</e2>"
PART_WHOLE(e1,e2)

1013	"Judges found <e1>sentences</e1> providing an <e2>answer</e2> to each question"
PART_WHOLE(e2,e1)

1014	"To address the <e1>sentence retrieval problem</e1>, we apply a <e2>stochastic, graph-based method</e2> for comparing the relative importance of the textual units, which was previously used successfully for generic summarization"
USAGE(e2,e1)

1015	"Currently, we present a topic-sensitive version of our <e1>method</e1> and hypothesize that it can outperform a competitive <e2>baseline</e2>, which compares the similarity of each sentence to the input question via IDF-weighted word overlap"
COMPARE(e1,e2)

1016	"Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the <e1>similarity</e1> of each <e2>sentence</e2> to the input question via IDF-weighted word overlap"
MODEL-FEATURE(e1,e2)

1017	"In our experiments, the <e1>method</e1> achieves a <e2>TRDR score</e2> that is significantly higher than that of the baseline.    "
RESULT(e1,e2)

1018	" A <e1>model</e1> is presented to characterize the <e2>class of languages</e2> obtained by adding reduplication to context-free languages"
MODEL-FEATURE(e1,e2)

1019	" A model is presented to characterize the class of languages obtained by adding <e1>reduplication</e1> to <e2>context-free languages</e2>"
PART_WHOLE(e1,e2)

1020	"The model is a <e1>pushdown automaton</e1> augmented with the ability to check reduplication by using the <e2>stack</e2> in a new way"
USAGE(e2,e1)

1021	"The model appears capable of accommodating the sort of <e1>reduplications</e1> that have been observed to occur in <e2>natural languages</e2>, but it excludes many of the unnatural constructions that other formal models have permitted.    "
PART_WHOLE(e1,e2)

1022	" This article is devoted to the problem of <e1>quantifying noun groups</e1> in <e2>German</e2>"
PART_WHOLE(e1,e2)

1023	"Moreover, some examples are given that underline the necessity of integrating some kind of information other than <e1>grammar sensu stricto</e1> into the <e2>treebank</e2>"
MODEL-FEATURE(e2,e1)

1024	"We argue that a more sophisticated and fine-grained <e1>annotation</e1> in the <e2>tree-bank</e2> would have very positve effects on stochastic parsers trained on the tree-bank and on grammars induced from the treebank, and it would make the treebank more valuable as a source of data for theoretical linguistic investigations"
PART_WHOLE(e1,e2)

1025	"We argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on stochastic parsers trained on the tree-bank and on grammars induced from the treebank, and it would make the <e1>treebank</e1> more valuable as a source of data for <e2>theoretical linguistic investigations</e2>"
USAGE(e1,e2)

1026	"The information gained from corpus research and the analyses that are proposed are realized in the framework of SILVA, a parsing and <e1>extraction tool</e1> for <e2>German text corpora</e2>.    "
USAGE(e1,e2)

1027	" <e1>Metagrammatical formalisms</e1> that combine <e2>context-free phrase structure rules</e2> and metarules &amp;&amp;lpar&amp;&amp;MPS grammars&amp;&amp;rpar&amp;&amp; allow concise statement of generalizations about the syntax of natural languages"
PART_WHOLE(e2,e1)

1028	" Metagrammatical formalisms that combine context-free phrase structure rules and metarules &amp;&amp;lpar&amp;&amp;MPS grammars&amp;&amp;rpar&amp;&amp; allow concise statement of generalizations about the <e1>syntax</e1> of <e2>natural languages</e2>"
PART_WHOLE(e1,e2)

1029	" In this paper we present a <e1>formalization</e1> of the centering approach to modeling <e2>attentional structure in discourse</e2> and use it as the basis for an algorithm to track discourse context and bind pronouns"
MODEL-FEATURE(e1,e2)

1030	" In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an <e1>algorithm</e1> to track <e2>discourse context</e2> and bind pronouns"
USAGE(e1,e2)

1031	"The algorithm has been implemented in an <e1>HPSG natural language system</e1> which serves as the interface to a <e2>database query application</e2>"
USAGE(e1,e2)

1032	"While <e1>HPSG</e1> has a more elaborated <e2>principle-based theory</e2> of possible phrase structures, TAG provides the means to represent lexicalized structures more explicitly"
PART_WHOLE(e2,e1)

1033	"While HPSG has a more elaborated principle-based theory of possible phrase structures, <e1>TAG</e1> provides the means to represent <e2>lexicalized structures</e2> more explicitly"
USAGE(e1,e2)

1034	"Our objectives are met by giving clear definitions that determine the <e1>projection of structures</e1> from the <e2>lexicon</e2>, and identify maximal projections, auxiliary trees and foot nodes"
PART_WHOLE(e1,e2)

1035	" Valiant showed that <e1>Boolean matrix multiplication &amp;&amp;lpar&amp;&amp;BMM&amp;&amp;rpar&amp;&amp;</e1> can be used for <e2>CFG parsing</e2>"
USAGE(e1,e2)

1036	"We prove a dual result: <e1>CFG parsers</e1> running in <e2>time O&amp;&amp;lpar&amp;&amp;|G||w|3-e&amp;&amp;rpar&amp;&amp;</e2> on a grammar G and a string w can be used to multiply m x m Boolean matrices in time O&amp;&amp;lpar&amp;&amp;m3-e/3&amp;&amp;rpar&amp;&amp;"
MODEL-FEATURE(e2,e1)

1037	"In the process we also provide a <e1>formal definition</e1> of <e2>parsing</e2> motivated by an informal notion due to Lang"
MODEL-FEATURE(e1,e2)

1038	"Our result establishes one of the first limitations on general CFG parsing: a fast, practical <e1>CFG parser</e1> would yield a fast, practical <e2>BMM algorithm</e2>, which is not believed to exist"
RESULT(e1,e2)

1039	" This paper introduces <e1>primitive Optimality Theory &amp;&amp;lpar&amp;&amp;OTP&amp;&amp;rpar&amp;&amp;</e1>, a linguistically motivated formalization of <e2>OT</e2>"
MODEL-FEATURE(e1,e2)

1040	"In contrast to less restricted <e1>theories</e1> using <e2>Generalized Alignment</e2>, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994)"
USAGE(e2,e1)

1041	"In contrast to less restricted theories using Generalized Alignment, OTP's optimal <e1>surface forms</e1> can be generated with <e2>finite-state methods</e2> adapted from (Ellison, 1994)"
USAGE(e2,e1)

1042	"Unfortunately these <e1>methods</e1> take <e2>time exponential on the size of the grammar</e2>"
MODEL-FEATURE(e2,e1)

1043	"Indeed the <e1>generation problem</e1> is shown <e2>NP-complete</e2> in this sense"
MODEL-FEATURE(e2,e1)

1044	"One avenue for future improvements is a new finite-state notion, factored automata, where <e1>regular languages</e1> are represented compactly via <e2>formal intersections of FSAs</e2>"
MODEL-FEATURE(e2,e1)

1045	"We report our analysis of a collection of 20 <e1>Wall Street Journal articles</e1> from the <e2>Penn Treebank Corpus</e2> and our experiments with WordNet to identify relations between bridging descriptions and their antecedents"
PART_WHOLE(e1,e2)

1046	" To verify <e1>hardware designs</e1> by <e2>model checking</e2>, circuit specifications are commonly expressed in the temporal logic CTL"
USAGE(e2,e1)

1047	" To verify hardware designs by model checking, <e1>circuit specifications</e1> are commonly expressed in the <e2>temporal logic CTL</e2>"
MODEL-FEATURE(e2,e1)

1048	"Automatic conversion of English to CTL requires the definition of an appropriately <e1>restricted subset</e1> of <e2>English</e2>"
PART_WHOLE(e1,e2)

1049	"We show how the limited <e1>semantic expressibility</e1> of <e2>CTL</e2> can be exploited to derive a hierarchy of subsets"
MODEL-FEATURE(e1,e2)

1050	"Our strategy avoids potential difficulties with approaches that take existing <e1>computational semantic analyses</e1> of <e2>English</e2> as their starting point--such as the need to ensure that all sentences in the subset possess a CTL translation"
TOPIC(e1,e2)

1051	"Our strategy avoids potential difficulties with approaches that take existing computational semantic analyses of English as their starting point--such as the need to ensure that all <e1>sentences</e1> in the <e2>subset</e2> possess a CTL translation"
PART_WHOLE(e1,e2)

1052	" In this paper, we present an <e1>unlexicalized parser</e1> for <e2>German</e2> which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2, higher than previously reported results on the NEGRA corpus"
USAGE(e1,e2)

1053	" In this paper, we present an unlexicalized parser for German which employs smoothing and <e1>suffix analysis</e1> to achieve a <e2>labelled bracket F-score</e2> of 76.2, higher than previously reported results on the NEGRA corpus"
RESULT(e1,e2)

1054	"In addition to the high accuracy of the model, the use of <e1>smoothing</e1> in an <e2>unlexicalized parser</e2> allows us to better examine the interplay between smoothing and parsing results.     "
USAGE(e1,e2)

1055	" This paper proposes an <e1>alignment adaptation approach</e1> to improve <e2>domain-specific &amp;&amp;lpar&amp;&amp;in-domain&amp;&amp;rpar&amp;&amp; word alignment</e2>"
USAGE(e1,e2)

1056	"The basic idea of alignment adaptation is to use <e1>out-of-domain corpus</e1> to improve <e2>in-domain word alignment</e2> results"
USAGE(e1,e2)

1057	"In this paper, we first train two statistical word alignment models with the large-scale <e1>out-of-domain corpus</e1> and the small-scale <e2>in-domain corpus</e2> respectively, and then interpolate these two models to improve the domain-specific word alignment"
COMPARE(e1,e2)

1058	"Experimental results show that our approach improves <e1>domain-specific word alignment</e1> in terms of both precision and recall, achieving a <e2>relative error rate reduction</e2> of 6.56% as compared with the state-of-the-art technologies.     "
RESULT(e1,e2)

1059	"Our contributions include a concise, <e1>modular architecture</e1> with reversible processes of <e2>understanding</e2> and generation, an information-state model of reference, and flexible links between semantics and collaborative problem solving.     "
PART_WHOLE(e2,e1)

1060	" This article deals with the <e1>interpretation</e1> of <e2>conceptual operations</e2> underlying the communicative use of natural language &amp;&amp;lpar&amp;&amp;NL&amp;&amp;rpar&amp;&amp; within the Structured Inheritance Network &amp;&amp;lpar&amp;&amp;SI-Nets&amp;&amp;rpar&amp;&amp; paradigm"
MODEL-FEATURE(e1,e2)

1061	"The operations are reduced to <e1>functions</e1> of a <e2>formal language</e2>, thus changing the level of abstraction of the operations to be performed on SI-Nets"
PART_WHOLE(e1,e2)

1062	"In this sense, operations on SI-Nets are not merely isomorphic to single epistemological objects, but can be viewed as a simulation of processes on a different level, that pertaining to the <e1>conceptual system</e1> of <e2>NL</e2>"
MODEL-FEATURE(e1,e2)

1063	"For this purpose, we have designed a version of <e1>KL-ONE</e1> which represents the <e2>epistemological level</e2>, while the new experimental language, KL-Conc, represents the conceptual level"
MODEL-FEATURE(e1,e2)

1064	"For this purpose, we have designed a version of KL-ONE which represents the epistemological level, while the new experimental language, <e1>KL-Conc</e1>, represents the <e2>conceptual level</e2>"
MODEL-FEATURE(e1,e2)

1065	" The <e1>verb forms</e1> are often claimed to convey two kinds of <e2>information</e2> : 1"
MODEL-FEATURE(e2,e1)

1066	"whether the <e1>event</e1> described in a <e2>sentence</e2> is present, past or future (= deictic information) 2"
PART_WHOLE(e1,e2)

1067	"whether the <e1>event</e1> described in a <e2>sentence</e2> is presented as completed, going on, just starting or being finished (= aspectual information)"
PART_WHOLE(e1,e2)

1068	"It will be demonstrated in this paper that one has to add a third component to the analysis of <e1>verb form meanings</e1>, namely whether or not they express <e2>habituality</e2>"
MODEL-FEATURE(e2,e1)

1069	" Unification is often the appropriate method for expressing <e1>relations</e1> between <e2>representations</e2> in the form of feature structures; however, there are circumstances in which a different approach is desirable"
MODEL-FEATURE(e1,e2)

1070	"A <e1>declarative formalism</e1> is presented which permits direct <e2>mappings</e2> of one feature structure into another, and illustrative examples are given of its application to areas of current interest.     "
USAGE(e1,e2)

1071	"  We give an analysis of <e1>ellipsis resolution</e1> in terms of a straightforward <e2>discourse copying algorithm</e2> that correctly predicts a wide range of phenomena"
MODEL-FEATURE(e2,e1)

1072	"[1991], the treatment directly encodes the intuitive distinction between <e1>full NPs</e1> and the <e2>referential elements</e2> that corefer with them through what we term role linking"
COMPARE(e1,e2)

1073	"The correct <e1>predictions</e1> for several problematic examples of <e2>ellipsis</e2> naturally result"
MODEL-FEATURE(e1,e2)

1074	" Dividing <e1>sentences</e1> in <e2>chunks of words</e2> is a useful preprocessing step for parsing, information extraction and information retrieval"
PART_WHOLE(e2,e1)

1075	"(Ramshaw and Marcus, 1995) have introduced a "convenient" <e1>data representation</e1> for <e2>chunking</e2> by converting it to a tagging task"
USAGE(e1,e2)

1076	"In this paper we will examine seven different <e1>data representations</e1> for the problem of recognizing <e2>noun phrase chunks</e2>"
MODEL-FEATURE(e1,e2)

1077	"We will show that the <e1>data representation choice</e1> has a minor influence on <e2>chunking performance</e2>"
RESULT(e1,e2)

1078	"However, equipped with the most suitabledata representation, our <e1>memory-based learning chunker</e1> was able to improve the best published <e2>chunking results</e2> for a standard data set.     "
RESULT(e1,e2)

1079	" In this paper we compare two competing approaches to <e1>part-of-speech tagging</e1>, <e2>statistical and constraint-based disambiguation</e2>, using French as our test language"
USAGE(e2,e1)

1080	" In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using <e1>French</e1> as our <e2>test language</e2>"
USAGE(e1,e2)

1081	"We imposed a time limit on our experiment: the amount of time spent on the design of our <e1>constraint system</e1> was about the same as the time we used to train and test the easy-to-implement <e2>statistical model</e2>"
COMPARE(e1,e2)

1082	"The accuracy of the <e1>statistical method</e1> is reasonably good, comparable to <e2>taggers</e2> for English"
COMPARE(e1,e2)

1083	" In order to build robust <e1>automatic abstracting systems</e1>, there is a need for better <e2>training resources</e2> than are currently available"
USAGE(e2,e1)

1084	"In this paper, we introduce an <e1>annotation scheme</e1> for scientific articles which can be used to build such a <e2>resource</e2> in a consistent way"
USAGE(e1,e2)

1085	" This paper describes an implemented program that takes a tagged text corpus and generates a partial list of the <e1>subcategorization frames</e1> in which each <e2>verb</e2> occurs"
MODEL-FEATURE(e1,e2)

1086	"The completeness of the output list increases monotonically with the total occurrences of each <e1>verb</e1> in the <e2>training corpus</e2>"
PART_WHOLE(e1,e2)

1087	" We focus on the problem of building large repositories of <e1>lexical conceptual structure &amp;&amp;lpar&amp;&amp;LCS&amp;&amp;rpar&amp;&amp; representations</e1> for <e2>verbs</e2> in multiple languages"
MODEL-FEATURE(e1,e2)

1088	"Our acquisition program - LEXICALL - takes, as input, the result of previous work on verb classification and <e1>thematic grid tagging</e1>, and outputs <e2>LCS representations</e2> for different languages"
USAGE(e1,e2)

1089	"These <e1>representations</e1> have been ported into <e2>English, Arabic and Spanish lexicons</e2>, each containing approximately 9000 verbs"
USAGE(e1,e2)

1090	"We are currently using these <e1>lexicons</e1> in an <e2>operational foreign language tutoring</e2> and machine translation"
USAGE(e1,e2)

1091	" We investigate the utility of an <e1>algorithm for translation lexicon acquisition &amp;&amp;lpar&amp;&amp;SABLE&amp;&amp;rpar&amp;&amp;</e1>, used previously on a very large corpus to acquire general <e2>translation lexicons</e2>, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.     "
USAGE(e1,e2)

1092	" We investigate the utility of an algorithm for translation lexicon acquisition &amp;&amp;lpar&amp;&amp;SABLE&amp;&amp;rpar&amp;&amp;, used previously on a very large corpus to acquire general translation lexicons, when that <e1>algorithm</e1> is applied to a much smaller corpus to produce candidates for <e2>domain-specific translation lexicons</e2>.     "
USAGE(e1,e2)

1093	" English is shown to be trans-context-free on the basis of <e1>coordinations</e1> of the respectively type that involve <e2>strictly syntactic cross-serial agreement</e2>"
MODEL-FEATURE(e1,e2)

1094	"The agreement in question involves <e1>number</e1> in <e2>nouns</e2> and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English, like grammatical gender in languages such as French, is partly arbitrary"
MODEL-FEATURE(e1,e2)

1095	"The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because <e1>grammatical number</e1> in <e2>English</e2>, like grammatical gender in languages such as French, is partly arbitrary"
PART_WHOLE(e1,e2)

1096	"The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English, like <e1>grammatical gender</e1> in languages such as <e2>French</e2>, is partly arbitrary"
PART_WHOLE(e1,e2)

1097	"The formal proof, which makes crucial use of the Interchange Lemma of Ogden et al., is so constructed as to be valid even if <e1>English</e1> is presumed to contain <e2>grammatical sentences</e2> in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively, whereas other arguments have foundered on this problem"
PART_WHOLE(e2,e1)

1098	"The formal proof, which makes crucial use of the Interchange Lemma of Ogden et al., is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of <e1>coordinate phrases</e1> one of whose members has fewer <e2>conjuncts</e2> than the other; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively, whereas other arguments have foundered on this problem"
PART_WHOLE(e2,e1)

1099	"The formal proof, which makes crucial use of the Interchange Lemma of Ogden et al., is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other; it thus goes through whatever the facts may be regarding <e1>constructions</e1> with unequal numbers of <e2>conjuncts</e2> in the scope of respectively, whereas other arguments have foundered on this problem"
PART_WHOLE(e2,e1)

1100	" Towards deep analysis of compositional classes of paraphrases, we have examined a <e1>class-oriented framework</e1> for collecting <e2>paraphrase examples</e2>, in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement"
USAGE(e1,e2)

1101	" Towards deep analysis of compositional classes of paraphrases, we have examined a class-oriented framework for collecting paraphrase examples, in which <e1>sentential paraphrases</e1> are collected for each <e2>paraphrase class</e2> separately by means of automatic candidate generation and manual judgement"
PART_WHOLE(e1,e2)

1102	" A <e1>flexible parser</e1> can deal with input that deviates from its <e2>grammar</e2>, in addition to input that conforms to it"
PART_WHOLE(e2,e1)

1103	"Focused interaction of this kind is facilitated by a <e1>construction-specific approach</e1> to <e2>flexible parsing</e2>, with specialized parsing techniques for each type of construction, and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to"
USAGE(e1,e2)

1104	"Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing, with <e1>specialized parsing techniques</e1> for each type of <e2>construction</e2>, and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to"
USAGE(e1,e2)

1105	"Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing, with specialized parsing techniques for each type of construction, and specialized <e1>ambiguity representations</e1> for each type of <e2>ambiguity</e2> that a particular construction can give rise to"
MODEL-FEATURE(e1,e2)

1106	"A <e1>construction-specific approach</e1> also aids in <e2>task-specific language development</e2> by allowing a language definition that is natural in terms of the task domain to be interpreted directly without compilation into a uniform grammar formalism, thus greatly speeding the testing of changes to the language definition"
USAGE(e1,e2)

1107	"Building on previous work at Carnegie-Mellon University e.g. [4, 5, 8], <e1>Plume's approach to parsing</e1> is based on <e2>semantic caseframe instantiation</e2>"
USAGE(e2,e1)

1108	"While <e1>Plume</e1> is well adapted to simple declarative and imperative utterances, it handles passives, relative clauses and interrogatives in an ad hoc manner leading to patchy <e2>syntactic coverage</e2>"
RESULT(e1,e2)

1109	" <e1>Languages</e1> differ in the concepts and real-world entities for which they have <e2>words</e2> and grammatical constructs"
PART_WHOLE(e2,e1)

1110	"Therefore translation must sometimes be a matter of approximating the <e1>meaning</e1> of a <e2>source language text</e2> rather than finding an exact counterpart in the target language"
MODEL-FEATURE(e1,e2)

1111	"We propose a <e1>translation framework</e1> based on <e2>Situation Theory</e2>"
USAGE(e2,e1)

1112	"The basic ingredients are an information lattice, a <e1>representation scheme</e1> for <e2>utterances</e2> embedded in contexts, and a mismatch resolution scheme defined in terms of information flow"
MODEL-FEATURE(e1,e2)

1113	"The basic ingredients are an information lattice, a representation scheme for utterances embedded in contexts, and a <e1>mismatch resolution scheme</e1> defined in terms of <e2>information flow</e2>"
MODEL-FEATURE(e2,e1)

1114	"We motivate our approach with examples of <e1>translation</e1> between <e2>English</e2> and Japanese"
USAGE(e1,e2)

1115	" <e1>Large-scale natural language generation</e1> requires the integration of vast amounts of <e2>knowledge</e2>: lexical, grammatical, and conceptual"
USAGE(e2,e1)

1116	"A <e1>robust generator</e1> must be able to operate well even when pieces of <e2>knowledge</e2> are missing"
USAGE(e2,e1)

1117	"To attack these problems, we have built a <e1>hybrid generator</e1>, in which gaps in symbolic knowledge are filled by <e2>statistical methods</e2>"
USAGE(e2,e1)

1118	"We also discuss how the hybrid generation model can be used to simplify current <e1>generators</e1> and enhance their <e2>portability</e2>, even when perfect knowledge is in principle obtainable.    "
MODEL-FEATURE(e2,e1)

1119	" It is challenging to translate names and technical terms across <e1>languages</e1> with different <e2>alphabets</e2> and sound inventories"
MODEL-FEATURE(e1,e2)

1120	"For example, computer in <e1>English</e1> comes out as ~ i/l:::'=--~-- (konpyuutaa) in <e2>Japanese</e2>"
COMPARE(e1,e2)

1121	"We describe and evaluate a method for performing <e1>backwards transliterations</e1> by <e2>machine</e2>"
USAGE(e2,e1)

1122	"This method uses a <e1>generative model</e1>, incorporating several distinct stages in the <e2>transliteration process</e2>"
USAGE(e1,e2)

1123	" Although adequate models of human language for syntactic analysis and semantic interpretation are of at least context-free complexity, for applications such as <e1>speech processing</e1> in which speed is important <e2>finite-state models</e2> are often preferred"
USAGE(e2,e1)

1124	"These requirements may be reconciled by using the more complex grammar to automatically derive a <e1>finite-state approximation</e1> which can then be used as a filter to guide <e2>speech recognition</e2> or to reject many hypotheses at an early stage of processing"
USAGE(e1,e2)

1125	" We present a <e1>statistical model</e1> of <e2>Japanese unknown words</e2> consisting of a set of length and spelling models classified by the character types that constitute a word"
MODEL-FEATURE(e1,e2)

1126	" We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the <e1>character types</e1> that constitute a <e2>word</e2>"
PART_WHOLE(e1,e2)

1127	"The point is quite simple: different character sets should be treated differently and the changes between character types are very important because <e1>Japanese script</e1> has both <e2>ideograms</e2> like Chinese (kanji) and phonograms like English (katakana)"
PART_WHOLE(e2,e1)

1128	" This paper discusses a <e1>decision-tree approach</e1> to the problem of assigning <e2>probabilities</e2> to words following a given text"
USAGE(e1,e2)

1129	" This paper discusses a decision-tree approach to the problem of assigning probabilities to <e1>words</e1> following a given <e2>text</e2>"
PART_WHOLE(e1,e2)

1130	" This poster paper describes a <e1>full scale two-level morphological description</e1> (Karttunen, 1983; Koskenniemi, 1983) of <e2>Turkish word structures</e2>"
MODEL-FEATURE(e1,e2)

1131	"The description has been implemented using the PC-KIMMO environment (Antworth, 1990) and is based on a <e1>root word lexicon</e1> of about 23,000 <e2>roots words</e2>"
PART_WHOLE(e2,e1)

1132	"<e1>Turkish</e1> is an <e2>agglutinative language</e2> with word structures formed by productive affixations of derivational and inflectional suffixes to root words"
MODEL-FEATURE(e1,e2)

1133	"The <e1>surface realizations</e1> of <e2>morphological constructions</e2> are constrained and modified by a number of phonetic rules such as vowel harmony.     "
MODEL-FEATURE(e1,e2)

1134	" The TIPSTER Architecture has been designed to enable a variety of different <e1>text applications</e1> to use a set of <e2>common text processing modules</e2>"
USAGE(e2,e1)

1135	"Since user interfaces work best when customized for particular applications , it is appropriator that no particular <e1>user interface styles or conventions</e1> are described in the <e2>TIPSTER Architecture specification</e2>"
TOPIC(e2,e1)

1136	"However, the Computing Research Laboratory &amp;&amp;lpar&amp;&amp;CRL&amp;&amp;rpar&amp;&amp; has constructed several <e1>TIPSTER applications</e1> that use a common set of configurable <e2>Graphical User Interface &amp;&amp;lpar&amp;&amp;GUI&amp;&amp;rpar&amp;&amp; functions</e2>"
USAGE(e2,e1)

1137	"These <e1>GUIs</e1> were constructed using <e2>CRL's TIPSTER User Interface Toolkit &amp;&amp;lpar&amp;&amp;TUIT&amp;&amp;rpar&amp;&amp;</e2>"
USAGE(e2,e1)

1138	"TUIT is a <e1>software library</e1> that can be used to construct <e2>multilingual TIPSTER user interfaces</e2> for a set of common user tasks"
USAGE(e1,e2)

1139	" This paper describes to what extent <e1>deep processing</e1> may benefit from <e2>shallow techniques</e2> and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad coverage unification based grammar of Spanish"
USAGE(e2,e1)

1140	" This paper describes to what extent deep processing may benefit from shallow techniques and it presents a <e1>NLP system</e1> which integrates a <e2>linguistic PoS tagger and chunker</e2> as a preprocessing module of a broad coverage unification based grammar of Spanish"
PART_WHOLE(e2,e1)

1141	"Experiments show that the efficiency of the overall analysis improves significantly and that our system also provides <e1>robustness</e1> to the <e2>linguistic processing</e2> while maintaining both the accuracy and the precision of the grammar.     "
MODEL-FEATURE(e1,e2)

1142	"Experiments show that the efficiency of the overall analysis improves significantly and that our system also provides robustness to the linguistic processing while maintaining both the accuracy and the <e1>precision</e1> of the <e2>grammar</e2>.     "
RESULT(e2,e1)

1143	" In this paper, we compare the performance of a state-of-the-art <e1>statistical parser</e1> (Bikel, 2004) in parsing <e2>written and spoken language</e2> and in generating sub-categorization cues from written and spoken language"
USAGE(e1,e2)

1144	"Although <e1>Bikel's parser</e1> achieves a higher <e2>accuracy</e2> for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language"
RESULT(e1,e2)

1145	"Our experiments also show that current technology for <e1>extracting subcategorization frames</e1> initially designed for <e2>written texts</e2> works equally well for spoken language"
USAGE(e1,e2)

1146	"Additionally, we explore the utility of <e1>punctuation</e1> in helping <e2>parsing</e2> and extraction of subcategorization cues"
USAGE(e1,e2)

1147	"Our experiments show that punctuation is of little help in parsing spoken language and extracting <e1>subcategorization cues</e1> from <e2>spoken language</e2>"
PART_WHOLE(e1,e2)

1148	"This indicates that there is no need to add <e1>punctuation</e1> in transcribing <e2>spoken corpora</e2> simply in order to help parsers.     "
PART_WHOLE(e1,e2)

1149	" This paper describes a <e1>characters-based Chinese collocation system</e1> and discusses the advantages of it over a traditional <e2>word-based system</e2>"
COMPARE(e1,e2)

1150	"Since <e1>wordbreaks</e1> are not conventionally marked in <e2>Chinese text corpora</e2>, a character-based collocation system has the dual advantages of avoiding pre-processing distortion and directly accessing sub-lexical information"
PART_WHOLE(e1,e2)

1151	"Furthermore, <e1>word-based collocational properties</e1> can be obtained through an auxiliary module of <e2>automatic segmentation</e2>.     "
USAGE(e2,e1)

1152	" An efficient <e1>bit-vector-based CKY-style parser</e1> for <e2>context-free parsing</e2> is presented"
USAGE(e1,e2)

1153	"The parser computes a compact <e1>parse forest representation</e1> of the complete set of possible <e2>analyses for large treebank grammars</e2> and long input sentences"
MODEL-FEATURE(e1,e2)

1154	"The <e1>parser</e1> uses <e2>bit-vector operations</e2> to parallelise the basic parsing operations"
USAGE(e2,e1)

1155	"We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a <e1>language model</e1> for <e2>answers</e2> and a transformation model for answer/question terms, trained on a corpus of 1 million question/answer pairs collected from the Web.     "
USAGE(e1,e2)

1156	"We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a <e1>transformation model</e1> for <e2>answer/question terms</e2>, trained on a corpus of 1 million question/answer pairs collected from the Web.     "
USAGE(e1,e2)

1157	"We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms, trained on a <e1>corpus</e1> of 1 million <e2>question/answer pairs</e2> collected from the Web.     "
PART_WHOLE(e2,e1)

1158	" The applicability of many current <e1>information extraction techniques</e1> is severely limited by the need for <e2>supervised training data</e2>"
USAGE(e2,e1)

1159	" We demonstrate that for certain <e1>field structured extraction tasks</e1>, such as classified advertisements and bibliographic citations, small amounts of <e2>prior knowledge</e2> can be used to learn effective models in a primarily unsupervised fashion"
USAGE(e2,e1)

1160	"Although hidden Markov models &amp;&amp;lpar&amp;&amp;HMMs&amp;&amp;rpar&amp;&amp; provide a suitable <e1>generative model</e1> for <e2>field structured text</e2>, general unsupervised HMM learning fails to learn useful structure in either of our domains"
MODEL-FEATURE(e1,e2)

1161	"In both domains, we found that <e1>unsupervised methods</e1> can attain <e2>accuracies</e2> with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.     "
RESULT(e1,e2)

1162	"In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that <e1>semi-supervised methods</e1> can make good use of small amounts of <e2>labeled data</e2>.     "
USAGE(e2,e1)

1163	" Despite much recent progress on accurate <e1>semantic role labeling</e1>, previous work has largely used <e2>independent classifiers</e2>, possibly combined with separate label sequence models via Viterbi decoding"
USAGE(e2,e1)

1164	"This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong <e1>dependencies</e1> between <e2>arguments</e2>"
MODEL-FEATURE(e1,e2)

1165	"We show how to build a <e1>joint model</e1> of <e2>argument frames</e2>, incorporating novel features that model these interactions into discriminative log-linear models"
MODEL-FEATURE(e1,e2)

1166	"We show how to build a joint model of argument frames, incorporating novel <e1>features</e1> that model these interactions into <e2>discriminative log-linear models</e2>"
PART_WHOLE(e1,e2)

1167	"This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for <e1>gold-standard parse trees</e1> on <e2>PropBank</e2>.     "
PART_WHOLE(e1,e2)

1168	"It enables us to select a concise set of reading <e1>texts</e1> (from a <e2>target corpus</e2>) that contains all the target vocabulary to be learned"
PART_WHOLE(e1,e2)

1169	"We used a specialized <e1>vocabulary</e1> for an English certification test as the <e2>target vocabulary</e2> and used English Wikipedia, a free-content encyclopedia, as the target corpus"
USAGE(e1,e2)

1170	"We used a specialized vocabulary for an English certification test as the target vocabulary and used <e1>English Wikipedia</e1>, a free-content encyclopedia, as the <e2>target corpus</e2>"
USAGE(e1,e2)

1171	"Specifically, the following components of the system are described: the <e1>syntactic analyzer</e1>, based on a <e2>Procedural Systemic Grammar</e2>, the semantic analyzer relying on the Conceptual Dependency Theory, and the dictionary"
USAGE(e2,e1)

1172	"Specifically, the following components of the system are described: the syntactic analyzer, based on a Procedural Systemic Grammar, the <e1>semantic analyzer</e1> relying on the <e2>Conceptual Dependency Theory</e2>, and the dictionary"
USAGE(e2,e1)

1173	" A proposal to deal with <e1>French tenses</e1> in the framework of <e2>Discourse Representation Theory</e2> is presented, as it has been implemented for a fragment at the IMS"
MODEL-FEATURE(e2,e1)

1174	"Instead of using operators to express the <e1>meaning</e1> of the <e2>tenses</e2> the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text"
MODEL-FEATURE(e1,e2)

1175	"Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the <e1>meaning</e1> of the <e2>text</e2> is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text"
MODEL-FEATURE(e1,e2)

1176	"Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the <e1>events</e1> of a <e2>sentence</e2> in the event structure of the preceeding text"
PART_WHOLE(e1,e2)

1177	"Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the <e1>event structure</e1> of the preceeding <e2>text</e2>"
MODEL-FEATURE(e1,e2)

1178	"Thereby a system of relevant times provided by the preceeding text and by the <e1>temporal adverbials</e1> of the <e2>sentence</e2> being processed is used"
PART_WHOLE(e1,e2)

1179	"In opposition to the approach of Kamp and Rohrer the exact <e1>meaning</e1> of the <e2>tenses</e2> is fixed by the resolution component and not in the process of syntactic analysis"
MODEL-FEATURE(e1,e2)

1180	"In opposition to the approach of Kamp and Rohrer the exact meaning of the tenses is fixed by the <e1>resolution component</e1> and not in the process of <e2>syntactic analysis</e2>"
COMPARE(e1,e2)

1181	"The motivation for introducing these <e1>languages</e1> is to provide tools for formalising <e2>grammatical frameworks</e2> perspicuously, and the paper illustrates this by showing how the leading ideas of GPSG can be captured in LT &amp;&amp;lpar&amp;&amp;LF&amp;&amp;rpar&amp;&amp;"
MODEL-FEATURE(e1,e2)

1182	"The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously, and the paper illustrates this by showing how the leading ideas of <e1>GPSG</e1> can be captured in <e2>LT &amp;&amp;lpar&amp;&amp;LF&amp;&amp;rpar&amp;&amp;</e2>"
COMPARE(e1,e2)

1183	" One of the claimed benefits of <e1>Tree Adjoining Grammars</e1> is that they have an <e2>extended domain of locality &amp;&amp;lpar&amp;&amp;EDOL&amp;&amp;rpar&amp;&amp;</e2>"
MODEL-FEATURE(e1,e2)

1184	"We consider how this can be exploited to limit the need for <e1>feature structure unification</e1> during <e2>parsing</e2>"
USAGE(e1,e2)

1185	"We compare two wide-coverage lexicalized grammars of English, <e1>LEXSYS</e1> and <e2>XTAG</e2>, finding that the two grammars exploit EDOL in different ways"
COMPARE(e1,e2)

1186	"We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two <e1>grammars</e1> exploit <e2>EDOL</e2> in different ways"
USAGE(e2,e1)

1187	" We provide a unified account of <e1>sentence-level and text-level anaphora</e1> within the framework of a <e2>dependency-based grammar model</e2>"
MODEL-FEATURE(e2,e1)

1188	"Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB's binding theory, while those for <e1>text-level anaphora</e1> incorporate an adapted version of a <e2>Grosz-Sidner-style focus model</e2>.     "
MODEL-FEATURE(e2,e1)

1189	"In contrast to many of the past efforts that make use of <e1>heuristic rules</e1> whose development requires intense <e2>knowledge engineering</e2>, our approach attempts to express the speech knowledge within a formal framework using well-defined mathematical tools"
USAGE(e2,e1)

1190	"In our system, features and <e1>decision strategies</e1> are discovered and trained automatically, using a large body of <e2>speech data</e2>"
USAGE(e2,e1)

1191	" A method of <e1>sense resolution</e1> is proposed that is based on <e2>WordNet</e2>, an on-line lexical database that incorporates semantic relations (synonymy, antonymy, hyponymy, meronymy, causal and troponymic entailment) as labeled pointers between word senses"
USAGE(e2,e1)

1192	" A method of sense resolution is proposed that is based on WordNet, an on-line <e1>lexical database</e1> that incorporates <e2>semantic relations</e2> (synonymy, antonymy, hyponymy, meronymy, causal and troponymic entailment) as labeled pointers between word senses"
PART_WHOLE(e2,e1)

1193	"With <e1>WordNet</e1>, it is easy to retrieve sets of <e2>semantically related words</e2>, a facility that will be used for sense resolution during text processing, as follows"
PART_WHOLE(e2,e1)

1194	"With WordNet, it is easy to retrieve sets of semantically related words, a facility that will be used for <e1>sense resolution</e1> during <e2>text processing</e2>, as follows"
PART_WHOLE(e1,e2)

1195	"When a <e1>word</e1> with multiple <e2>senses</e2> is encountered, one of two procedures will be followed"
MODEL-FEATURE(e2,e1)

1196	"Either, (1) words related in meaning to the alternative senses of the polysemous word will be retrieved; new strings will be derived by substituting these related words into the context of the polysemous word; a large <e1>textual corpus</e1> will then be searched for these <e2>derived strings</e2>; and that sense will be chosen that corresponds to the derived string that is found most often in the corpus"
PART_WHOLE(e2,e1)

1197	"Either, (1) words related in meaning to the alternative senses of the polysemous word will be retrieved; new strings will be derived by substituting these related words into the context of the polysemous word; a large textual corpus will then be searched for these derived strings; and that sense will be chosen that corresponds to the <e1>derived string</e1> that is found most often in the <e2>corpus</e2>"
PART_WHOLE(e1,e2)

1198	"Or, (2) the <e1>context</e1> of the <e2>polysemous word</e2> will be used as a key to search a large corpus; all words found to occur in that context will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.     "
MODEL-FEATURE(e1,e2)

1199	"Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all <e1>words</e1> found to occur in that <e2>context</e2> will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.     "
MODEL-FEATURE(e2,e1)

1200	"Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all words found to occur in that context will be noted; <e1>WordNet</e1> will then be used to estimate the <e2>semantic distance</e2> from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.     "
USAGE(e1,e2)

1201	"Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all words found to occur in that context will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other <e1>words</e1> occurring in the same <e2>context</e2> If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.     "
MODEL-FEATURE(e2,e1)

1202	" In this paper, we want to show how the <e1>morphological component</e1> of an existing <e2>NLP-system for Dutch &amp;&amp;lpar&amp;&amp;Dutch Medical Language Processor - DMLP&amp;&amp;rpar&amp;&amp;</e2> has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system &amp;&amp;lpar&amp;&amp;Linguistic String Project - Medical Language Processor&amp;&amp;rpar&amp;&amp; of the New York University"
PART_WHOLE(e1,e2)

1203	" In this paper, we want to show how the morphological component of an existing NLP-system for Dutch &amp;&amp;lpar&amp;&amp;Dutch Medical Language Processor - DMLP&amp;&amp;rpar&amp;&amp; has been extended in order to produce output that is compatible with the <e1>language independent modules</e1> of the <e2>LSP-MLP system &amp;&amp;lpar&amp;&amp;Linguistic String Project - Medical Language Processor&amp;&amp;rpar&amp;&amp;</e2> of the New York University"
PART_WHOLE(e1,e2)

1204	"The former can take advantage of the language independent developments of the latter, while focusing on <e1>idiosyncrasies</e1> for <e2>Dutch</e2>"
PART_WHOLE(e1,e2)

1205	" We describe a novel technique and implemented system for constructing a <e1>subcategorization dictionary</e1> from <e2>textual corpora</e2>"
USAGE(e2,e1)

1206	"Each dictionary entry encodes the <e1>relative frequency of occurrence</e1> of a comprehensive set of <e2>subcategorization classes</e2> for English"
MODEL-FEATURE(e1,e2)

1207	"An initial experiment, on a sample of 14 <e1>verbs</e1> which exhibit <e2>multiple complementation patterns</e2>, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes"
MODEL-FEATURE(e2,e1)

1208	"We also demonstrate that a <e1>subcategorization dictionary</e1> built with the system improves the <e2>accuracy</e2> of a parser by an appreciable amount    "
RESULT(e1,e2)

1209	" This paper shows how <e1>dictionary word sense definitions</e1> can be analysed by applying a hierarchy of <e2>phrasal patterns</e2>"
USAGE(e2,e1)

1210	"An experimental system embodying this mechanism has been implemented for processing <e1>definitions</e1> from the <e2>Longman Dictionary of Contemporary English</e2>"
PART_WHOLE(e1,e2)

1211	"A property of this dictionary, exploited by the system, is that it uses a <e1>restricted vocabulary</e1> in its <e2>word sense definitions</e2>"
USAGE(e1,e2)

1212	"The structures generated by the experimental system are intended to be used for the classification of new <e1>word senses</e1> in terms of the <e2>senses</e2> of words in the restricted vocabulary"
MODEL-FEATURE(e2,e1)

1213	"The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of <e1>words</e1> in the <e2>restricted vocabulary</e2>"
PART_WHOLE(e1,e2)

1214	"This ensures that reasonable incomplete analyses of the <e1>definitions</e1> are produced when more complete analyses are not possible, resulting in a relatively robust <e2>analysis mechanism</e2>"
TOPIC(e2,e1)

1215	"Thus the work reported addresses two <e1>robustness problems</e1> faced by current experimental <e2>natural language processing systems</e2>: coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions"
MODEL-FEATURE(e1,e2)

1216	" This paper presents an <e1>evaluation method</e1> employing a <e2>latent variable model</e2> for paraphrases with their contexts"
USAGE(e2,e1)

1217	" This paper presents an evaluation method employing a latent variable model for <e1>paraphrases</e1> with their <e2>contexts</e2>"
MODEL-FEATURE(e2,e1)

1218	"We assume that the <e1>context</e1> of a <e2>sentence</e2> is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred"
MODEL-FEATURE(e1,e2)

1219	"We assume that the context of a sentence is indicated by a <e1>latent variable</e1> of the <e2>model</e2> as a topic and that the likelihood of each variable can be inferred"
PART_WHOLE(e1,e2)

1220	"We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the <e1>likelihood</e1> of each <e2>variable</e2> can be inferred"
MODEL-FEATURE(e1,e2)

1221	"A paraphrase is evaluated for whether its <e1>sentences</e1> are used in the same <e2>context</e2>"
MODEL-FEATURE(e1,e2)

1222	"The results also revealed an upper bound of <e1>accuracy</e1> of 77% with the method when using only <e2>topic information</e2>.     "
RESULT(e2,e1)

1223	" An extension to the GPSG grammatical formalism is proposed, allowing <e1>non-terminals</e1> to consist of finite sequences of <e2>category labels</e2>, and allowing schematic variables to range over such sequences"
PART_WHOLE(e2,e1)

1224	"The extension is shown to be sufficient to provide a strongly adequate <e1>grammar</e1> for <e2>crossed serial dependencies</e2>, as found in e.g. Dutch subordinate clauses"
USAGE(e1,e2)

1225	"The extension is shown to be parseable by a simple extension to an existing <e1>parsing method</e1> for <e2>GPSG</e2>"
USAGE(e1,e2)
