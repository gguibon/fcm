# Valentin Mac√©
# valentin.mace@kedgebs.com
# Developed for my internship at the LIS lab during my 4th year in the Computer Science Engineer program at Polytech Marseille

# Feel free to use this code as you wish as long as you quote me as author
# There is a notebook version of this script in the folder 'notebook' wich is easier to read, but it is in french

#  ----------------------------
# |          PURPOSE           |
#  ----------------------------

# Pupose of this script is to convert a file in Semeval 2010 format to the format used by Mo Yu as input to FCM (Gormley, Yu 2015)
# FCM publication : https://www.cs.cmu.edu/~mgormley/papers/gormley+yu+dredze.emnlp.2015.pdf_

# Input format : 
# 
#     4  "A misty <e1>ridge</e1> uprises from the <e2>surge</e2>."
#     Other
#  
# Output format :
# 
#     Other	2	2	ridge	6	6	surge
#     A	DT	0	0	0	misty	JJ	B-adj.all	0	0	ridge	NN	B-noun.object	0	0	uprises	NNS	I-noun.object	0	1	from	IN	0	0	1	the	DT	0	0	0	surge	NN	B-noun.event	0	0	.	.	0	0	0
#     1	B-noun.object 0
#     1	B-noun.event 0

# Input format is pretty simple, first line is a sentence with 2 entities between quotes (not necessarily) and an index, second line is the relation type

# Ouput format contains 4 lines, first line is the relation type, index of begining for 1rst entity, index of ending for 1rst entity and same for 2nd entity
# second line is the sentence tagged with the SST (SuperSenseTagger), note that the last feature is not generated by the SST but by the present script
# third line is the size of the 1rst entity plus two of its tags
# fourth line is the same for 2nd entity

# Once this format is generated, it can be directly used by the FCM 
# FCM implementation in C++ : https://github.com/Gorov/FCM_nips_workshop

#  ----------------------------
# |            USAGE           |
#  ----------------------------

# Using python 3 (I have not tested with version 2)

# Open a terminal, go to 'raw_to_formated_script' folder and just use :
# python raw_to_formated.py <input file>
# Example : python raw_to_formated.py semeval2010_train

# Note: The input file HAS to be in the 'data/corpus/raw' folder, and the output will have the same name and be located in the 'formated' folder
# Of course the file has to be in the 2010 Semeval format for the script to work





# Setup
import sys
import copy
import numpy as np
import subprocess
import networkx as nx
import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_sm')


file_input = '../raw/'+str(sys.argv[1])
file_output = '../formated/'+str(sys.argv[1])

# Checking corpus funciton
# cheks if each sentence contains 2 entities
def check_corpus(file_path):
    file = open(file_path, "r")
    lines = file.readlines()
    corpus_ok = True
    for i in range(0, len(lines), 3):
        if('<e1>' not in lines[i] or '<e2>' not in lines[i] or '</e2>' not in lines[i] or '</e1>' not in lines[i]):
            print("There is a problem with the corpus at the line ",i+1)
            corpus_ok = False
    if(corpus_ok):
        print("The corpus seems ok")
print("\n\nChecking corpus integrity...")




# Shortest Dependency Path between 2 words function
# Input : sentence and the two words for which we want dep path
# Ouput : a tab containing words on the dep path
def shortest_dependency_path(doc, e1=None, e2=None):
    edges = []
    shortest_path = []
    for token in doc:
        for child in token.children:
            edges.append(('{0}'.format(token),
                          '{0}'.format(child)))
    graph = nx.Graph(edges)
    try:
        if(e1 in graph.nodes and e2 in graph.nodes):
            shortest_path = nx.shortest_path(graph, source=e1, target=e2)
    except nx.NetworkXNoPath:
        shortest_path = []
    return shortest_path


# Here starts the file processing
# Processing inconvenient words like 'a.m', '100,000' ..., spacing out entities and deleting sentence indexes

# Cheking corpus
check_corpus(file_input)

file = open(file_input, "r")
lines = file.readlines()

# Note: This preprocessing is necessary because the SST tagger is a little dumber than the one used by Mo Yu
# And I could not find it
print("Processing inconvenient words like 'a.m', 'U.S' ...")
for i in range(0, len(lines), 3):                        # All 3 lines ... so each sentence
    line_split = lines[i].split()                        
    for j in range(len(line_split)):
        if(line_split[j].find(',') > 0):                 # if there's a comma inside a word
            line_split[j] = line_split[j].replace(',','')# we delete it
        if(line_split[j].find('.') > 0):
            line_split[j] = line_split[j].replace('.','')
        if(line_split[j] == '.' and j != len(line_split)-2):
            line_split[j] = line_split[j].replace('.','')


    lines[i] = ' '.join(line_split) + '\n'               # rebuild the line from the split
    
# Spacing out entities for split to recognize them
# Deleting external quotes
print("Spacing out entities and deleting useless quotes")
i = 0
while i < len(lines):
    #lines[i] = lines[i].replace('"', '')                # Old version
    line_split = lines[i].split()
    for j in range(len(line_split)):
        if(j == 1 and '"' in line_split[j]):
            line_split[j] = line_split[j].replace('"','')
        if(j == len(line_split)-1 and '"' in line_split[j]):
            line_split[j] = line_split[j].replace('"','')
    lines[i] = ' '.join(line_split) + '\n'

    lines[i] = lines[i].replace('<e1>', '<e1> ')
    lines[i] = lines[i].replace('<e2>', '<e2> ')
    lines[i] = lines[i].replace('</e1>', ' </e1>')
    lines[i] = lines[i].replace('</e2>', ' </e2>')
    i+=1

# Deleting indexes
for i in range(0, len(lines), 3):
    line_split = lines[i].split()
    if(len(lines[i]) > 0):
        del line_split[0]
    lines[i] = ' '.join(line_split) + '\n'
    i+=1


# Extracting first line of Yu format and adding cleaned sentence
lines_res_temp_1 = copy.deepcopy(lines)               # will contain the first temporary result
i = 0
for i in range(0, len(lines), 3):                     # All 3 lines ... so each sentence
    line_split = lines[i].split()
    first_line = lines[i+1].replace('\n', '') + '	' # first we put the relation in first_line
    
    tabulation_fin = False                            # Bolean not to tabulate after e2
    j = 0
    while j < len(line_split):
        # Extracting indexes and deleting entity tags
        if('<e' in line_split[j]):
            tabulation_fin = not tabulation_fin
            del line_split[j]
            first_line += str(j) + '	'             # j = entity begining index
            k = j                                     # k is used to go over entity starting at j
            ent = ''
            while "</e" not in line_split[k]:
                ent += line_split[k]                  # adding the word in the entity to ent
                if("</e" not in line_split[k+1]):     # if the entity is not over we put a space
                    ent += ' '
                k+=1
            if(tabulation_fin == True):               # if it's the first entity we tabulate
                ent += '	'
            
        elif('</e' in line_split[j]):
            del line_split[j]
            j-=1
            first_line += str(j) + '	' + ent       # j = end entity index
        
        # Building res_temp_1 (containing blocks of 2 lines)
        lines_res_temp_1[i] = first_line + '\n'
        lines_res_temp_1[i+1] = ' '.join(line_split) + '\n'
        j+=1


# Processing file that will go through SST
file = open("sst/DATA/to_sst.txt", "w")
i = 0
file.write("	")     # tabulate once, otherwise the tagger will 'eat' the first word of the file

# Only the sentences will go through SST, the first line will be used later
for i in range(1, len(lines_res_temp_1)+1, 3):
        file.write(lines_res_temp_1[i])
        file.write('\n')
        file.write('\n')
file.close()

# Command to launch SST, see its README for more details
command = ['sst', 'multitag-line', './DATA/to_sst.txt', '0', '0', 'DATA/GAZ/gazlistall_minussemcor',
          './MODELS/WSJPOSc_base_20', 'DATA/WSJPOSc.TAGSET',
           './MODELS/SEM07_base_12', './DATA/WNSS_07.TAGSET',
           './MODELS/WSJc_base_20', './DATA/WSJ.TAGSET',
           './MODELS/CONLL03_base_15', './DATA/CONLL03.TAGSET',
           '>', './DATA/res_sst.tags', '&', 'clean.bat']
print('We call the SST tagger\n')
process = subprocess.Popen(command, cwd="sst", shell=True, stdout=subprocess.PIPE)
process.wait()   # Waiting for the end of SST tagging, results in res_sst.tags
print("End of SST tagging")


# Generating last tag for each word (whether it's on the dep path between entities or not)
# Deleting lemmas

# reading annotated sentences
res_sst = open("sst/DATA/res_sst.tags", "r")
lines_res_sst = res_sst.readlines()
res_sst.close()

# Generating a dependencies graph for each sentence, and finding the shortest path between e1 and e2
# Then checking for each word of the sentence if it's in this path
print("Generating dependency path feature")
for i in range(0, len(lines_res_sst), 3):
    line_split_res_sst = lines_res_sst[i].split()         # Line to modify (adding the tag for dep path)
    line_split_res_temp_1 = lines_res_temp_1[i].split()   # Sentence without annotation
    
    # finding e1, its size and starting index
    e1_size = int(line_split_res_temp_1[2]) - int(line_split_res_temp_1[1])+1
    e1_start = int(line_split_res_temp_1[1])
    e1 = lines_res_temp_1[i+1].split()[e1_start]
    # Same for e2
    e2_size = int(line_split_res_temp_1[4+e1_size]) - int(line_split_res_temp_1[3+e1_size])+1
    e2_start = int(line_split_res_temp_1[3+e1_size])
    e2 = lines_res_temp_1[i+1].split()[e2_start]
    
    # Loading the sentence to be processed with NetworkX
    doc = nlp(lines_res_temp_1[i+1])
    dep_path = shortest_dependency_path(doc, e1, e2)

    # For each word...
    for j in range(5, len(line_split_res_sst), 6):
        # if it's on the dep path between e1 and e2...
        if(line_split_res_sst[j-5] in dep_path
          and line_split_res_sst[j-5] != e1
          and line_split_res_sst[j-5] != e2):
            # putting it's last tag to 1
            line_split_res_sst[j] = '1'
        else:
            line_split_res_sst[j] = '0'
    lines_res_sst[i] = '	'.join(line_split_res_sst) + '\n'    
    
# Deleting lemmas
print("Deleting lemmas")
i = 0
for i in range(0, len(lines_res_sst), 3):                # All 3 lines ... so each sentence
    line_split = lines_res_sst[i].split()
    
    j = 2
    while j < len(line_split):
        del line_split[j]                                # Deleting lemma
        j+=5                                             # going to the next one
    lines_res_sst[i] = '	'.join(line_split) + '\n'



# Replacing the sentence by its annotated version in temporary result 1, and adding the last 2 lines

print("Finalizing...")
file = open(file_output, "w")
# In the final result
for i in range(len(lines_res_temp_1)):
    if(i%3 == 0):
        # Writing the first line with the relation, entities and their indexes
        file.write(lines_res_temp_1[i])
        # Adding the next line (annotated sentence)
        file.write(lines_res_sst[i])
        
        # Splitting them in order get informations for building the last 2 lines
        line_split = lines_res_temp_1[i].split()
        sst_split = lines_res_sst[i].split()
        
        # findind e1 size and its starting index
        e1_size = int(line_split[2]) - int(line_split[1])+1
        e1_start = int(line_split[1])
        append1 = ''                        # append1 contains the third line of Yu format (related to e1)
        for j in range(e1_size):
            if(j > 0):
                append1 +=' '
            # Getting tags for e1 (the 2nd and 3rd)
            append1 += sst_split[(e1_start+j)*5+2] + ' ' + sst_split[(e1_start+j)*5+3]
        file.write(str(e1_size) + '	' + append1 + '\n')
        
        # Same for e2
        e2_size = int(line_split[4+e1_size]) - int(line_split[3+e1_size])+1
        e2_start = int(line_split[3+e1_size])
        append2 = ''                        # append2 contains the fourth line of Yu format (related to e2)
        for j in range(e2_size):
            if(j > 0):
                append2 +=' '
            # Getting tags for e2 (the 2nd and 3rd)
            append2 += sst_split[(e2_start+j)*5+2] + ' ' + sst_split[(e2_start+j)*5+3]
        file.write(str(e2_size) + '	' + append2 + '\n')

file.close()
print("Done")