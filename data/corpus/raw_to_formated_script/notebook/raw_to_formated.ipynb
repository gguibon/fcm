{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Developpé par Valentin Macé dans le cadre du stage de fin de 4ème année du cycle Ingénieur Informatique (Polytech Marseille) au LIS\n",
    "\n",
    "![LIS](img/lis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "<br><br>\n",
    "<font color='orange'>Le but de ce script est de permettre à un fichier textuel au format Semeval 2010 d'être transformé vers le format utilisé par Mo Yu en entrée du FCM (Gormley, Yu 2015).</font>\n",
    "\n",
    "_Publication du FCM : https://www.cs.cmu.edu/~mgormley/papers/gormley+yu+dredze.emnlp.2015.pdf_\n",
    "<br><br><br>\n",
    "***\n",
    "Format d'entrée : \n",
    "\n",
    "    4  \"A misty <e1>ridge</e1> uprises from the <e2>surge</e2>.\"\n",
    "    Other\n",
    " On y distingue une première ligne avec un index et une phrase contenant deux entités e1 et e2, unde deuxième ligne avec le type de la relation entre les entités\n",
    " <br><br><br>\n",
    " \n",
    " Format de sortie :\n",
    "\n",
    "    Other\t2\t2\tridge\t6\t6\tsurge\n",
    "    A\tDT\t0\t0\t0\tmisty\tJJ\tB-adj.all\t0\t0\tridge\tNN\tB-noun.object\t0\t0\tuprises\tNNS\tI-noun.object\t0\t1\tfrom\tIN\t0\t0\t1\tthe\tDT\t0\t0\t0\tsurge\tNN\tB-noun.event\t0\t0\t.\t.\t0\t0\t0\n",
    "    1\tB-noun.object 0\n",
    "    1\tB-noun.event 0\n",
    "\n",
    "On y distingue une première ligne constituée de la relation entre e1 et e2, de l'indice de début de la première entité suivi de son indice de fin, de la première entité elle-même ('ridge'), le tout suivi des mêmes éléments pour la seconde entité\n",
    "\n",
    "Une deuxième ligne composée de la phrase (\"A misty...\") annotée grâce au SuperSense Tagger (SST), attention il se peut que cette ligne très longue apparaisse comme plusieurs lignes\n",
    "\n",
    "Enfin les deux dernières lignes représentent, pour chaque entité : sa taille suivie du deuxième tag généré par le SST, suivi du troisième tag\n",
    "<br><br><br>\n",
    "<font color='orange'>Remarque</font>, le dernier tag pour chaque mot n'est pas généré par le SST, c'est un boléen qui indique si le mot fait partie du dependency path entre e1 et e2\n",
    "\n",
    "Pour l'obtenir, on crée pour chaque phrase du corpus un graphe de dépendence non orienté, et on regarde pour chaque mot de la phrase si il se trouve sur le chemin le plus court entre e1 et e2\n",
    "***\n",
    "<br><br>\n",
    "Une fois ce format généré, il suffit de le donner en entrée du FCM\n",
    "\n",
    "*Implementation du FCM en C++ par Yu https://github.com/Gorov/FCM_nips_workshop*\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction de vérification de la structure du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus seems ok\n"
     ]
    }
   ],
   "source": [
    "# Vérifie toutes les lignes multiples de 3 (donc les phrases du corpus)\n",
    "# Si le corpus est décalé ce sera aussi détecté\n",
    "def check_corpus(file_path):\n",
    "    file = open(file_path, \"r\")\n",
    "    lines = file.readlines()\n",
    "    corpus_ok = True\n",
    "    for i in range(0, len(lines), 3):\n",
    "        if('<e1>' not in lines[i] or '<e2>' not in lines[i] or '</e2>' not in lines[i] or '</e1>' not in lines[i]):\n",
    "            print(\"There is a problem with the corpus at the line \",i+1)\n",
    "            corpus_ok = False\n",
    "    if(corpus_ok):\n",
    "        print(\"The corpus seems ok\")\n",
    "    \n",
    "check_corpus(\"notebook_input.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction qui retourne le chemin de dépendance le plus court entre deux mots d'une phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renvoie le chemin de dépendances le plus court\n",
    "# Input : la phrase, les deux mots à partir desquels on calcul le chemin\n",
    "# Ouput : un tableau contenant les mots présents sur le chemin de dépendance\n",
    "def shortest_dependency_path(doc, e1=None, e2=None):\n",
    "    edges = []\n",
    "    shortest_path = []\n",
    "    # On crée une arête entre chaque noeud (token) et ses enfants\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token),\n",
    "                          '{0}'.format(child)))\n",
    "    # Création du graphe avec ces arêtes\n",
    "    graph = nx.Graph(edges)\n",
    "    try:\n",
    "        if(e1 in graph.nodes and e2 in graph.nodes):\n",
    "            # Calcul du chemin le plus court\n",
    "            shortest_path = nx.shortest_path(graph, source=e1, target=e2)\n",
    "    except nx.NetworkXNoPath:\n",
    "        shortest_path = []\n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ici commence le traitement du fichier\n",
    "<br>\n",
    "#### Traitement des mots gênants, espacement des balises d'entités et suppression des indexes pour les phrases\n",
    "<br>\n",
    "Input:\n",
    "\n",
    "    699\t\"Now , <e2>administration</e2> <e1>officials</e1> in U.S. are \"good\" people\"\n",
    "Output:\n",
    "\n",
    "    Now , <e2> administration </e2> <e1> officials </e1> in U.S. are \"good\" people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge le fichier contenant le texte à traiter - Format Semeval 2010\n",
    "file = open(\"notebook_input.txt\", \"r\")\n",
    "# lines contiendra toutes les lignes du fichier original, nettoyées\n",
    "lines = file.readlines()\n",
    "\n",
    "# On supprime les caractères genants dans les mots : 'a.m.' ou 'U.S' -> 'am' et 'US'\n",
    "# pb -> reAce possède des 'U.S .' et pas 'U.S.' donc le dernier point n'est pas supprimé\n",
    "# possibilité d'amélioration, ainsi que 'a.m' qui devient le verbe 'am'\n",
    "for i in range(0, len(lines), 3):                        # Toutes les 3 lignes... donc chaque phrase\n",
    "    line_split = lines[i].split()                        # split sépare la phrase en un tableau de mots\n",
    "    for j in range(len(line_split)):\n",
    "        if(line_split[j].find(',') > 0):                 # Si la virgule fait partie d'un mot\n",
    "            line_split[j] = line_split[j].replace(',','')\n",
    "        if(line_split[j].find('.') > 0):\n",
    "            line_split[j] = line_split[j].replace('.','')\n",
    "        if(line_split[j] == '.' and j != len(line_split)-2):\n",
    "            line_split[j] = line_split[j].replace('.','')\n",
    "\n",
    "\n",
    "    lines[i] = ' '.join(line_split) + '\\n'               # join permet de reconstruire la ligne\n",
    "    \n",
    "# On espace les entités pour que le split à venir les détecte\n",
    "# On enlève les guillemets inutiles encadrant les phrases\n",
    "i = 0\n",
    "while i < len(lines):\n",
    "    #lines[i] = lines[i].replace('\"', '')                # Ancienne version on supprime tous les \"\n",
    "    line_split = lines[i].split()\n",
    "    for j in range(len(line_split)):\n",
    "        if(j == 1 and '\"' in line_split[j]):\n",
    "            line_split[j] = line_split[j].replace('\"','')\n",
    "        if(j == len(line_split)-1 and '\"' in line_split[j]):\n",
    "            line_split[j] = line_split[j].replace('\"','')\n",
    "    lines[i] = ' '.join(line_split) + '\\n'\n",
    "\n",
    "    lines[i] = lines[i].replace('<e1>', '<e1> ')\n",
    "    lines[i] = lines[i].replace('<e2>', '<e2> ')\n",
    "    lines[i] = lines[i].replace('</e1>', ' </e1>')\n",
    "    lines[i] = lines[i].replace('</e2>', ' </e2>')\n",
    "    i+=1\n",
    "\n",
    "# 0n supprime l'index de chaque phrase qui est inutile au FCM\n",
    "for i in range(0, len(lines), 3):        # Toutes les 3 lignes... donc chaque phrase\n",
    "    line_split = lines[i].split()\n",
    "    if(len(lines[i]) > 0):\n",
    "        del line_split[0]\n",
    "    lines[i] = ' '.join(line_split) + '\\n'\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction de la première ligne du format d'entrée du FCM et ajout de la phrase nettoyée\n",
    "<br>\n",
    "Input:\n",
    "\n",
    "    The <e1> child </e1> was carefully wrapped and bound into the <e2> cradle </e2> .\n",
    "    Other\n",
    "Output:\n",
    "\n",
    "    Other\t1\t1\tchild\t9\t9\tcradle\t\n",
    "    The child was carefully wrapped and bound into the cradle by means of a cord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On extrait la première ligne pour chaque phrase\n",
    "# elle contient : la relation, les entités et leurs indices de début et fin\n",
    "# On rajoute la phrase nettoyée en dessous, c'est le 1er resultat temporaire\n",
    "\n",
    "lines_res_temp_1 = copy.deepcopy(lines)               # contiendra le 1er resultat temporaire\n",
    "i = 0\n",
    "for i in range(0, len(lines), 3):                     # Toutes les 3 lignes... donc chaque phrase\n",
    "    line_split = lines[i].split()\n",
    "    first_line = lines[i+1].replace('\\n', '') + '\t' # On met d'abord la relation dans first_line\n",
    "    \n",
    "    '''rel_sens = ''                                     # On extrait le sens de la relation\n",
    "    j = 0                                                # (e1,e2) si e1 est avant e2 et vice-versa\n",
    "    for j in range(len(line_split)):                     # à décommenter si besoin\n",
    "        if(line_split[j] == '<e1>'):\n",
    "            rel_sens = '(e1,e2)'\n",
    "            break\n",
    "        elif(line_split[j] == '<e2>'):\n",
    "            rel_sens = '(e2,e1)'\n",
    "            break\n",
    "        j+=1\n",
    "    first_line += rel_sens + '\t'\n",
    "    '''\n",
    "    tabulation_fin = False                            # Boléen pour ne pas tabuler après e2\n",
    "    j = 0\n",
    "    while j < len(line_split):\n",
    "        # Extraction des indices et suppresion des balises\n",
    "        if('<e' in line_split[j]):\n",
    "            tabulation_fin = not tabulation_fin\n",
    "            del line_split[j]\n",
    "            first_line += str(j) + '\t'             # j = indice debut entité\n",
    "            k = j                                     # k sert à parcourir l'entité commençant à j\n",
    "            ent = ''\n",
    "            while \"</e\" not in line_split[k]:\n",
    "                ent += line_split[k]                  # On ajoute le mot de l'entité à ent\n",
    "                if(\"</e\" not in line_split[k+1]):     # Si entité non finie on espace\n",
    "                    ent += ' '\n",
    "                k+=1\n",
    "            if(tabulation_fin == True):               # Si c'est la première entité on tabule\n",
    "                ent += '\t'\n",
    "            \n",
    "        elif('</e' in line_split[j]):\n",
    "            del line_split[j]\n",
    "            j-=1\n",
    "            first_line += str(j) + '\t' + ent       # j = indice fin entité\n",
    "        \n",
    "        # On construit le résultat temporaire 1, constitué des blocs vus plus haut\n",
    "        lines_res_temp_1[i] = first_line + '\\n'\n",
    "        lines_res_temp_1[i+1] = ' '.join(line_split) + '\\n'\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Préparation du fichier à donner au SST\n",
    "#### Puis lancement du SST\n",
    "<br>\n",
    "Input:\n",
    "    \n",
    "    Other\t1\t1\tchild\t9\t9\tcradle\t\n",
    "    The child ...\n",
    "Output:\n",
    "\n",
    "    The\tDT\tthe\t0\t0\t0\tchild\tNN\tchild\tB-noun.person\tB-E:PER_DESC\t0\t..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"../sst/DATA/to_sst.txt\", \"w\")              # Toutes les 3 lignes... donc chaque phrase\n",
    "i = 0\n",
    "file.write(\"\t\")                                   # Ne pas supprimer cette tabulation\n",
    "# Préparation du fichier à donner au SST\n",
    "# Uniquement les phrases, la première ligne servira plus tard\n",
    "for i in range(1, len(lines_res_temp_1)+1, 3):\n",
    "        file.write(lines_res_temp_1[i])\n",
    "        file.write('\\n')\n",
    "        file.write('\\n')\n",
    "file.close()\n",
    "\n",
    "# Commande pour lancer le SST, voir son README pour son fonctionnement si besoin de la modifier\n",
    "command = ['sst', 'multitag-line', './DATA/to_sst.txt', '0', '0', 'DATA/GAZ/gazlistall_minussemcor',\n",
    "          './MODELS/WSJPOSc_base_20', 'DATA/WSJPOSc.TAGSET',\n",
    "           './MODELS/SEM07_base_12', './DATA/WNSS_07.TAGSET',\n",
    "           './MODELS/WSJc_base_20', './DATA/WSJ.TAGSET',\n",
    "           './MODELS/CONLL03_base_15', './DATA/CONLL03.TAGSET',\n",
    "           '>', './DATA/res_sst.tags', '&', 'clean.bat']\n",
    "process = subprocess.Popen(command, cwd=\"../sst\", shell=True, stdout=subprocess.PIPE)\n",
    "process.wait()   # On a attend la terminaison du SST, résultats dans res_sst.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul du dernier tag pour chque mot\n",
    "#### Suppression des lemmas\n",
    "<br>\n",
    "Input:\n",
    "    \n",
    "    The\tDT\tthe\t0\t0\t0\tchild\tNN\tchild\tB-noun.person\tB-E:PER_DESC\t0\t...\n",
    "Output:\n",
    "\n",
    "    The\tDT\t0\t0\t0\tchild\tNN\tB-noun.person\tB-E:PER_DESC\t1\t...\n",
    "Note : Ici j'ai arbitrairement mis à 1 le dernier tag du mot 'child' pour l'exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file = open(\"sst/DATA/res_sst.tags\", \"w\")\\nfor line in lines_res_sst:\\n    file.write(line)\\nfile.close()'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lecture du fichier contenant les résultats du SST\n",
    "# Toutes les 3 lignes, une phrase annotée\n",
    "res_sst = open(\"../sst/DATA/res_sst.tags\", \"r\")\n",
    "lines_res_sst = res_sst.readlines()\n",
    "res_sst.close()\n",
    "\n",
    "# Génération de la dernière valeur du format de Yu pour chaque mot\n",
    "# Pour chaque mot, on regarde si il se trouve sur le dependency path entre e1 et e2 dans sa phrase\n",
    "# -> Génération d'un graphe de dépendances non orienté pour la phrase et chercher le plus court chemin\n",
    "# si oui = '1' si non = '0'\n",
    "for i in range(0, len(lines_res_sst), 3):\n",
    "    line_split_res_sst = lines_res_sst[i].split()         # La ligne que l'on va modifier\n",
    "    line_split_res_temp_1 = lines_res_temp_1[i].split()   # La phrase sans annotation pour le graphe\n",
    "    \n",
    "    # On trouve e1, sa taille et son indice de début\n",
    "    e1_size = int(line_split_res_temp_1[2]) - int(line_split_res_temp_1[1])+1\n",
    "    e1_start = int(line_split_res_temp_1[1])\n",
    "    e1 = lines_res_temp_1[i+1].split()[e1_start]\n",
    "    # Idem pour e2\n",
    "    e2_size = int(line_split_res_temp_1[4+e1_size]) - int(line_split_res_temp_1[3+e1_size])+1\n",
    "    e2_start = int(line_split_res_temp_1[3+e1_size])\n",
    "    e2 = lines_res_temp_1[i+1].split()[e2_start]\n",
    "    \n",
    "    # On charge la phrase pour être traitée avec NetworkX\n",
    "    doc = nlp(lines_res_temp_1[i+1])\n",
    "    # On trouve le chemin de dépendance le plus court entre e1 et e2\n",
    "    dep_path = shortest_dependency_path(doc, e1, e2)\n",
    "\n",
    "    # Pour chaque mot..\n",
    "    for j in range(5, len(line_split_res_sst), 6):\n",
    "        # Si il est sur le chemin de dep et différent de e1 et e2\n",
    "        if(line_split_res_sst[j-5] in dep_path\n",
    "          and line_split_res_sst[j-5] != e1\n",
    "          and line_split_res_sst[j-5] != e2):\n",
    "            # Alors on met son dernier tag à 1\n",
    "            line_split_res_sst[j] = '1'\n",
    "        else:\n",
    "            line_split_res_sst[j] = '0'\n",
    "    lines_res_sst[i] = '\t'.join(line_split_res_sst) + '\\n'    \n",
    "    \n",
    "# Supression des lemmas\n",
    "i = 0\n",
    "for i in range(0, len(lines_res_sst), 3):                # Toutes les 3 lignes... donc chaque phrase\n",
    "    line_split = lines_res_sst[i].split()\n",
    "    \n",
    "    j = 2\n",
    "    while j < len(line_split):\n",
    "        del line_split[j]                                # On supprime le lemma\n",
    "        j+=5                                             # On avance au suivant\n",
    "    lines_res_sst[i] = '\t'.join(line_split) + '\\n'\n",
    "\n",
    "'''file = open(\"sst/DATA/res_sst.tags\", \"w\")\n",
    "for line in lines_res_sst:\n",
    "    file.write(line)\n",
    "file.close()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dans le résultat intermediaire 1 on remplace la phrase par sa version annotée et on prépare les deux lignes vides pour le res final\n",
    "<br>\n",
    "Input:\n",
    "    \n",
    "    Other\t1\t1\tchild\t9\t9\tcradle\t\n",
    "et\n",
    "\n",
    "    The\tDT\t0\t0\t0\tchild\tNN\tB-noun.person\tB-E:PER_DESC\t1\t...\n",
    "Output (résultat final):\n",
    "\n",
    "    Other\t1\t1\tchild\t9\t9\tcradle\n",
    "    The\tDT\t0\t0\t0\tchild\tNN\tB-noun.person\tB-E:PER_DESC\t1\n",
    "    1\tB-noun.person B-E:PER_DESC\n",
    "    1\tB-noun.artifact 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"notebook_output.txt\", \"w\")\n",
    "\n",
    "# Dans le résultat final...\n",
    "for i in range(len(lines_res_temp_1)):\n",
    "    if(i%3 == 0):\n",
    "        # On écrit la premiere ligne avec la relation, les entités et leurs indices\n",
    "        file.write(lines_res_temp_1[i])\n",
    "        # On ajoute sur la ligne suivante la phrase annotée avec le SST\n",
    "        file.write(lines_res_sst[i])\n",
    "        \n",
    "        # On split ces deux lignes pour en extraire des informations\n",
    "        # nécessaires à la construction des deux derniers blocs \n",
    "        line_split = lines_res_temp_1[i].split()\n",
    "        sst_split = lines_res_sst[i].split()\n",
    "        \n",
    "        # On trouve la taille de e1, son indice de départ\n",
    "        e1_size = int(line_split[2]) - int(line_split[1])+1\n",
    "        e1_start = int(line_split[1])\n",
    "        append1 = ''                        # append1 contiendra le 3ème bloc (celui décrivant e1)\n",
    "        for j in range(e1_size):\n",
    "            if(j > 0):\n",
    "                append1 +=' '\n",
    "            # On obtient les tags pour e1 (le 2ème et 3ème)\n",
    "            append1 += sst_split[(e1_start+j)*5+2] + ' ' + sst_split[(e1_start+j)*5+3]\n",
    "        file.write(str(e1_size) + '\t' + append1 + '\\n')\n",
    "        \n",
    "        # Idem pour e2\n",
    "        e2_size = int(line_split[4+e1_size]) - int(line_split[3+e1_size])+1\n",
    "        e2_start = int(line_split[3+e1_size])\n",
    "        append2 = ''                        # append2 contiendra le 4ème bloc (celui décrivant e2)\n",
    "        for j in range(e2_size):\n",
    "            if(j > 0):\n",
    "                append2 +=' '\n",
    "            # On obtient les tags pour e2 (le 2ème et 3ème)\n",
    "            append2 += sst_split[(e2_start+j)*5+2] + ' ' + sst_split[(e2_start+j)*5+3]\n",
    "        file.write(str(e2_size) + '\t' + append2 + '\\n')\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
